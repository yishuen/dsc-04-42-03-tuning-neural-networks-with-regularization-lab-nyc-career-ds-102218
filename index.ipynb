{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits.\n",
    "* Apply L1 and L2 regularization.\n",
    "* Aplly dropout regularization.\n",
    "* Observe and comment on the effect of using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "random_sample = df.sample(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 1.9005 - acc: 0.2262 - val_loss: 1.8265 - val_acc: 0.2930\n",
      "Epoch 2/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 1.7185 - acc: 0.3794 - val_loss: 1.5615 - val_acc: 0.4960\n",
      "Epoch 3/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 1.3821 - acc: 0.5822 - val_loss: 1.1865 - val_acc: 0.6460\n",
      "Epoch 4/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 1.0549 - acc: 0.6756 - val_loss: 0.9252 - val_acc: 0.7120\n",
      "Epoch 5/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.8646 - acc: 0.7139 - val_loss: 0.7863 - val_acc: 0.7380\n",
      "Epoch 6/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7637 - acc: 0.7373 - val_loss: 0.7072 - val_acc: 0.7640\n",
      "Epoch 7/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.7048 - acc: 0.7509 - val_loss: 0.6583 - val_acc: 0.7760\n",
      "Epoch 8/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.6657 - acc: 0.7618 - val_loss: 0.6241 - val_acc: 0.7830\n",
      "Epoch 9/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.6374 - acc: 0.7713 - val_loss: 0.5993 - val_acc: 0.7910\n",
      "Epoch 10/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.6155 - acc: 0.7789 - val_loss: 0.5797 - val_acc: 0.8020\n",
      "Epoch 11/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5977 - acc: 0.7849 - val_loss: 0.5669 - val_acc: 0.8040\n",
      "Epoch 12/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5823 - acc: 0.7902 - val_loss: 0.5545 - val_acc: 0.8050\n",
      "Epoch 13/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.5694 - acc: 0.7946 - val_loss: 0.5430 - val_acc: 0.8080\n",
      "Epoch 14/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.5578 - acc: 0.7983 - val_loss: 0.5389 - val_acc: 0.8100\n",
      "Epoch 15/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5476 - acc: 0.8021 - val_loss: 0.5317 - val_acc: 0.8080\n",
      "Epoch 16/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.5383 - acc: 0.8060 - val_loss: 0.5233 - val_acc: 0.8130\n",
      "Epoch 17/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.5298 - acc: 0.8089 - val_loss: 0.5176 - val_acc: 0.8100\n",
      "Epoch 18/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.5221 - acc: 0.8122 - val_loss: 0.5109 - val_acc: 0.8180\n",
      "Epoch 19/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.5151 - acc: 0.8138 - val_loss: 0.5071 - val_acc: 0.8170\n",
      "Epoch 20/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5083 - acc: 0.8163 - val_loss: 0.5015 - val_acc: 0.8210\n",
      "Epoch 21/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5022 - acc: 0.8184 - val_loss: 0.4994 - val_acc: 0.8210\n",
      "Epoch 22/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4965 - acc: 0.8215 - val_loss: 0.4962 - val_acc: 0.8180\n",
      "Epoch 23/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4911 - acc: 0.8226 - val_loss: 0.4925 - val_acc: 0.8220\n",
      "Epoch 24/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.4860 - acc: 0.8250 - val_loss: 0.4925 - val_acc: 0.8220\n",
      "Epoch 25/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4810 - acc: 0.8271 - val_loss: 0.4870 - val_acc: 0.8270\n",
      "Epoch 26/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4769 - acc: 0.8286 - val_loss: 0.4858 - val_acc: 0.8220\n",
      "Epoch 27/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4722 - acc: 0.8305 - val_loss: 0.4810 - val_acc: 0.8230\n",
      "Epoch 28/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4682 - acc: 0.8311 - val_loss: 0.4785 - val_acc: 0.8260\n",
      "Epoch 29/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4643 - acc: 0.8339 - val_loss: 0.4771 - val_acc: 0.8220\n",
      "Epoch 30/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.4606 - acc: 0.8345 - val_loss: 0.4759 - val_acc: 0.8260\n",
      "Epoch 31/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4570 - acc: 0.8355 - val_loss: 0.4753 - val_acc: 0.8270\n",
      "Epoch 32/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4535 - acc: 0.8369 - val_loss: 0.4779 - val_acc: 0.8250\n",
      "Epoch 33/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4504 - acc: 0.8389 - val_loss: 0.4724 - val_acc: 0.8280\n",
      "Epoch 34/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4473 - acc: 0.8399 - val_loss: 0.4734 - val_acc: 0.8260\n",
      "Epoch 35/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4441 - acc: 0.8407 - val_loss: 0.4735 - val_acc: 0.8270\n",
      "Epoch 36/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.4414 - acc: 0.8424 - val_loss: 0.4729 - val_acc: 0.8320\n",
      "Epoch 37/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4384 - acc: 0.8436 - val_loss: 0.4705 - val_acc: 0.8290\n",
      "Epoch 38/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4359 - acc: 0.8442 - val_loss: 0.4686 - val_acc: 0.8310\n",
      "Epoch 39/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.4332 - acc: 0.8449 - val_loss: 0.4687 - val_acc: 0.8260\n",
      "Epoch 40/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4305 - acc: 0.8469 - val_loss: 0.4716 - val_acc: 0.8290\n",
      "Epoch 41/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4284 - acc: 0.8474 - val_loss: 0.4680 - val_acc: 0.8300\n",
      "Epoch 42/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.4259 - acc: 0.8478 - val_loss: 0.4661 - val_acc: 0.8320\n",
      "Epoch 43/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.4237 - acc: 0.8495 - val_loss: 0.4668 - val_acc: 0.8270\n",
      "Epoch 44/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4215 - acc: 0.8504 - val_loss: 0.4679 - val_acc: 0.8320\n",
      "Epoch 45/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4192 - acc: 0.8509 - val_loss: 0.4733 - val_acc: 0.8260\n",
      "Epoch 46/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4175 - acc: 0.8518 - val_loss: 0.4669 - val_acc: 0.8270\n",
      "Epoch 47/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4151 - acc: 0.8531 - val_loss: 0.4684 - val_acc: 0.8290\n",
      "Epoch 48/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4133 - acc: 0.8537 - val_loss: 0.4706 - val_acc: 0.8280\n",
      "Epoch 49/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.4114 - acc: 0.8536 - val_loss: 0.4653 - val_acc: 0.8280\n",
      "Epoch 50/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4094 - acc: 0.8547 - val_loss: 0.4660 - val_acc: 0.8330\n",
      "Epoch 51/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4074 - acc: 0.8552 - val_loss: 0.4666 - val_acc: 0.8310\n",
      "Epoch 52/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4059 - acc: 0.8562 - val_loss: 0.4688 - val_acc: 0.8270\n",
      "Epoch 53/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4040 - acc: 0.8566 - val_loss: 0.4668 - val_acc: 0.8290\n",
      "Epoch 54/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4022 - acc: 0.8573 - val_loss: 0.4674 - val_acc: 0.8280\n",
      "Epoch 55/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.4009 - acc: 0.8579 - val_loss: 0.4670 - val_acc: 0.8310\n",
      "Epoch 56/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3990 - acc: 0.8588 - val_loss: 0.4689 - val_acc: 0.8310\n",
      "Epoch 57/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3973 - acc: 0.8595 - val_loss: 0.4668 - val_acc: 0.8270\n",
      "Epoch 58/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.3960 - acc: 0.8597 - val_loss: 0.4669 - val_acc: 0.8290\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3944 - acc: 0.8607 - val_loss: 0.4695 - val_acc: 0.8320\n",
      "Epoch 60/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3929 - acc: 0.8608 - val_loss: 0.4688 - val_acc: 0.8300\n",
      "Epoch 61/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.3916 - acc: 0.8613 - val_loss: 0.4672 - val_acc: 0.8310\n",
      "Epoch 62/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3901 - acc: 0.8618 - val_loss: 0.4677 - val_acc: 0.8320\n",
      "Epoch 63/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3887 - acc: 0.8632 - val_loss: 0.4697 - val_acc: 0.8300\n",
      "Epoch 64/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3874 - acc: 0.8626 - val_loss: 0.4691 - val_acc: 0.8290\n",
      "Epoch 65/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3857 - acc: 0.8632 - val_loss: 0.4728 - val_acc: 0.8300\n",
      "Epoch 66/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3847 - acc: 0.8632 - val_loss: 0.4695 - val_acc: 0.8310\n",
      "Epoch 67/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3833 - acc: 0.8647 - val_loss: 0.4789 - val_acc: 0.8250\n",
      "Epoch 68/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3821 - acc: 0.8649 - val_loss: 0.4725 - val_acc: 0.8300\n",
      "Epoch 69/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3809 - acc: 0.8655 - val_loss: 0.4729 - val_acc: 0.8300\n",
      "Epoch 70/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3797 - acc: 0.8663 - val_loss: 0.4722 - val_acc: 0.8240\n",
      "Epoch 71/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3784 - acc: 0.8665 - val_loss: 0.4740 - val_acc: 0.8290\n",
      "Epoch 72/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3774 - acc: 0.8665 - val_loss: 0.4770 - val_acc: 0.8280\n",
      "Epoch 73/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3761 - acc: 0.8674 - val_loss: 0.4759 - val_acc: 0.8250\n",
      "Epoch 74/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3748 - acc: 0.8678 - val_loss: 0.4748 - val_acc: 0.8270\n",
      "Epoch 75/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3738 - acc: 0.8675 - val_loss: 0.4728 - val_acc: 0.8220\n",
      "Epoch 76/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3728 - acc: 0.8680 - val_loss: 0.4759 - val_acc: 0.8270\n",
      "Epoch 77/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3715 - acc: 0.8685 - val_loss: 0.4745 - val_acc: 0.8260\n",
      "Epoch 78/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3702 - acc: 0.8689 - val_loss: 0.4793 - val_acc: 0.8250\n",
      "Epoch 79/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3694 - acc: 0.8696 - val_loss: 0.4747 - val_acc: 0.8270\n",
      "Epoch 80/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3685 - acc: 0.8697 - val_loss: 0.4786 - val_acc: 0.8290\n",
      "Epoch 81/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3673 - acc: 0.8700 - val_loss: 0.4794 - val_acc: 0.8250\n",
      "Epoch 82/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.3663 - acc: 0.8702 - val_loss: 0.4781 - val_acc: 0.8300\n",
      "Epoch 83/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3655 - acc: 0.8714 - val_loss: 0.4754 - val_acc: 0.8240\n",
      "Epoch 84/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3644 - acc: 0.8706 - val_loss: 0.4805 - val_acc: 0.8260\n",
      "Epoch 85/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3635 - acc: 0.8711 - val_loss: 0.4751 - val_acc: 0.8220\n",
      "Epoch 86/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3623 - acc: 0.8723 - val_loss: 0.4846 - val_acc: 0.8260\n",
      "Epoch 87/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3614 - acc: 0.8723 - val_loss: 0.4830 - val_acc: 0.8250\n",
      "Epoch 88/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3609 - acc: 0.8733 - val_loss: 0.4793 - val_acc: 0.8270\n",
      "Epoch 89/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3598 - acc: 0.8732 - val_loss: 0.4779 - val_acc: 0.8230\n",
      "Epoch 90/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3587 - acc: 0.8736 - val_loss: 0.4840 - val_acc: 0.8260\n",
      "Epoch 91/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.3579 - acc: 0.8734 - val_loss: 0.4830 - val_acc: 0.8260\n",
      "Epoch 92/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.3570 - acc: 0.8742 - val_loss: 0.4813 - val_acc: 0.8230\n",
      "Epoch 93/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3561 - acc: 0.8745 - val_loss: 0.4858 - val_acc: 0.8230\n",
      "Epoch 94/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3552 - acc: 0.8755 - val_loss: 0.4812 - val_acc: 0.8200\n",
      "Epoch 95/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3545 - acc: 0.8741 - val_loss: 0.4818 - val_acc: 0.8200\n",
      "Epoch 96/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3535 - acc: 0.8747 - val_loss: 0.4955 - val_acc: 0.8220\n",
      "Epoch 97/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3525 - acc: 0.8748 - val_loss: 0.4828 - val_acc: 0.8190\n",
      "Epoch 98/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3521 - acc: 0.8752 - val_loss: 0.4880 - val_acc: 0.8230\n",
      "Epoch 99/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3508 - acc: 0.8764 - val_loss: 0.4909 - val_acc: 0.8230\n",
      "Epoch 100/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3502 - acc: 0.8758 - val_loss: 0.4909 - val_acc: 0.8240\n",
      "Epoch 101/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.3493 - acc: 0.8771 - val_loss: 0.4980 - val_acc: 0.8250\n",
      "Epoch 102/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.3484 - acc: 0.8773 - val_loss: 0.4906 - val_acc: 0.8250\n",
      "Epoch 103/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3477 - acc: 0.8775 - val_loss: 0.4905 - val_acc: 0.8260\n",
      "Epoch 104/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3471 - acc: 0.8776 - val_loss: 0.4927 - val_acc: 0.8220\n",
      "Epoch 105/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3461 - acc: 0.8778 - val_loss: 0.4922 - val_acc: 0.8250\n",
      "Epoch 106/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3454 - acc: 0.8782 - val_loss: 0.4919 - val_acc: 0.8210\n",
      "Epoch 107/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3446 - acc: 0.8780 - val_loss: 0.4921 - val_acc: 0.8250\n",
      "Epoch 108/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3440 - acc: 0.8791 - val_loss: 0.4963 - val_acc: 0.8220\n",
      "Epoch 109/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3429 - acc: 0.8790 - val_loss: 0.4920 - val_acc: 0.8190\n",
      "Epoch 110/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.3424 - acc: 0.8786 - val_loss: 0.4913 - val_acc: 0.8200\n",
      "Epoch 111/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3414 - acc: 0.8796 - val_loss: 0.4898 - val_acc: 0.8220\n",
      "Epoch 112/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3408 - acc: 0.8797 - val_loss: 0.4959 - val_acc: 0.8220\n",
      "Epoch 113/120\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.3399 - acc: 0.8801 - val_loss: 0.4955 - val_acc: 0.8190\n",
      "Epoch 114/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3394 - acc: 0.8799 - val_loss: 0.4951 - val_acc: 0.8220\n",
      "Epoch 115/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3384 - acc: 0.8809 - val_loss: 0.4936 - val_acc: 0.8210\n",
      "Epoch 116/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.3377 - acc: 0.8803 - val_loss: 0.5010 - val_acc: 0.8200\n",
      "Epoch 117/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3372 - acc: 0.8810 - val_loss: 0.4954 - val_acc: 0.8230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3365 - acc: 0.8812 - val_loss: 0.4966 - val_acc: 0.8200\n",
      "Epoch 119/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3356 - acc: 0.8812 - val_loss: 0.5006 - val_acc: 0.8230\n",
      "Epoch 120/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3349 - acc: 0.8825 - val_loss: 0.4954 - val_acc: 0.8140\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 3s 44us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 57us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3303531802073769, 0.8823826086997986]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.582646284421285, 0.7979999996821086]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.8898 - acc: 0.1971 - val_loss: 1.8182 - val_acc: 0.2330\n",
      "Epoch 2/60\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 1.7148 - acc: 0.3108 - val_loss: 1.6008 - val_acc: 0.3750\n",
      "Epoch 3/60\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.4637 - acc: 0.4687 - val_loss: 1.3155 - val_acc: 0.5440\n",
      "Epoch 4/60\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1642 - acc: 0.6213 - val_loss: 1.0345 - val_acc: 0.6700\n",
      "Epoch 5/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.9385 - acc: 0.6985 - val_loss: 0.8563 - val_acc: 0.7270\n",
      "Epoch 6/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.8055 - acc: 0.7286 - val_loss: 0.7534 - val_acc: 0.7460\n",
      "Epoch 7/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.7301 - acc: 0.7440 - val_loss: 0.6913 - val_acc: 0.7560\n",
      "Epoch 8/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.6834 - acc: 0.7545 - val_loss: 0.6530 - val_acc: 0.7660\n",
      "Epoch 9/60\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6515 - acc: 0.7635 - val_loss: 0.6259 - val_acc: 0.7670\n",
      "Epoch 10/60\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6275 - acc: 0.7716 - val_loss: 0.6075 - val_acc: 0.7890\n",
      "Epoch 11/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.6083 - acc: 0.7784 - val_loss: 0.5892 - val_acc: 0.7730\n",
      "Epoch 12/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5922 - acc: 0.7831 - val_loss: 0.5769 - val_acc: 0.7840\n",
      "Epoch 13/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5783 - acc: 0.7904 - val_loss: 0.5676 - val_acc: 0.7830\n",
      "Epoch 14/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5661 - acc: 0.7940 - val_loss: 0.5564 - val_acc: 0.7830\n",
      "Epoch 15/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5551 - acc: 0.7985 - val_loss: 0.5500 - val_acc: 0.7970\n",
      "Epoch 16/60\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.5451 - acc: 0.8013 - val_loss: 0.5361 - val_acc: 0.8020\n",
      "Epoch 17/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5362 - acc: 0.8041 - val_loss: 0.5390 - val_acc: 0.8020\n",
      "Epoch 18/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5281 - acc: 0.8090 - val_loss: 0.5292 - val_acc: 0.7940\n",
      "Epoch 19/60\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.5203 - acc: 0.8114 - val_loss: 0.5236 - val_acc: 0.8060\n",
      "Epoch 20/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5133 - acc: 0.8144 - val_loss: 0.5166 - val_acc: 0.8070\n",
      "Epoch 21/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5064 - acc: 0.8168 - val_loss: 0.5132 - val_acc: 0.8090\n",
      "Epoch 22/60\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.5003 - acc: 0.8200 - val_loss: 0.5115 - val_acc: 0.8130\n",
      "Epoch 23/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4944 - acc: 0.8219 - val_loss: 0.5046 - val_acc: 0.8110\n",
      "Epoch 24/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4890 - acc: 0.8241 - val_loss: 0.5029 - val_acc: 0.8120\n",
      "Epoch 25/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4837 - acc: 0.8262 - val_loss: 0.5006 - val_acc: 0.8070\n",
      "Epoch 26/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4789 - acc: 0.8280 - val_loss: 0.4958 - val_acc: 0.8180\n",
      "Epoch 27/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4743 - acc: 0.8294 - val_loss: 0.4912 - val_acc: 0.8240\n",
      "Epoch 28/60\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.4698 - acc: 0.8312 - val_loss: 0.4928 - val_acc: 0.8240\n",
      "Epoch 29/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4654 - acc: 0.8337 - val_loss: 0.4920 - val_acc: 0.8180\n",
      "Epoch 30/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4616 - acc: 0.8346 - val_loss: 0.4927 - val_acc: 0.8110\n",
      "Epoch 31/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4577 - acc: 0.8366 - val_loss: 0.4888 - val_acc: 0.8230\n",
      "Epoch 32/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4541 - acc: 0.8380 - val_loss: 0.4873 - val_acc: 0.8300\n",
      "Epoch 33/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4508 - acc: 0.8390 - val_loss: 0.4846 - val_acc: 0.8280\n",
      "Epoch 34/60\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.4475 - acc: 0.8404 - val_loss: 0.4809 - val_acc: 0.8250\n",
      "Epoch 35/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4444 - acc: 0.8419 - val_loss: 0.4825 - val_acc: 0.8260\n",
      "Epoch 36/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4411 - acc: 0.8431 - val_loss: 0.4862 - val_acc: 0.8180\n",
      "Epoch 37/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4382 - acc: 0.8450 - val_loss: 0.4798 - val_acc: 0.8220\n",
      "Epoch 38/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4355 - acc: 0.8452 - val_loss: 0.4769 - val_acc: 0.8250\n",
      "Epoch 39/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4328 - acc: 0.8461 - val_loss: 0.4804 - val_acc: 0.8280\n",
      "Epoch 40/60\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.4302 - acc: 0.8479 - val_loss: 0.4793 - val_acc: 0.8150\n",
      "Epoch 41/60\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.4276 - acc: 0.8482 - val_loss: 0.4765 - val_acc: 0.8200\n",
      "Epoch 42/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4249 - acc: 0.8496 - val_loss: 0.4768 - val_acc: 0.8200\n",
      "Epoch 43/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4230 - acc: 0.8503 - val_loss: 0.4767 - val_acc: 0.8210\n",
      "Epoch 44/60\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4205 - acc: 0.8514 - val_loss: 0.4775 - val_acc: 0.8200\n",
      "Epoch 45/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4180 - acc: 0.8527 - val_loss: 0.4739 - val_acc: 0.8230\n",
      "Epoch 46/60\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.4158 - acc: 0.8533 - val_loss: 0.4764 - val_acc: 0.8280\n",
      "Epoch 47/60\n",
      "57500/57500 [==============================] - 2s 27us/step - loss: 0.4139 - acc: 0.8532 - val_loss: 0.4752 - val_acc: 0.8220\n",
      "Epoch 48/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4117 - acc: 0.8534 - val_loss: 0.4774 - val_acc: 0.8210\n",
      "Epoch 49/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4096 - acc: 0.8550 - val_loss: 0.4750 - val_acc: 0.8230\n",
      "Epoch 50/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4079 - acc: 0.8558 - val_loss: 0.4750 - val_acc: 0.8240\n",
      "Epoch 51/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4059 - acc: 0.8561 - val_loss: 0.4830 - val_acc: 0.8230\n",
      "Epoch 52/60\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.4042 - acc: 0.8561 - val_loss: 0.4794 - val_acc: 0.8200\n",
      "Epoch 53/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.4024 - acc: 0.8580 - val_loss: 0.4726 - val_acc: 0.8210\n",
      "Epoch 54/60\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.4006 - acc: 0.8591 - val_loss: 0.4818 - val_acc: 0.8210\n",
      "Epoch 55/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3987 - acc: 0.8589 - val_loss: 0.4791 - val_acc: 0.8240\n",
      "Epoch 56/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3969 - acc: 0.8595 - val_loss: 0.4776 - val_acc: 0.8230\n",
      "Epoch 57/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3953 - acc: 0.8607 - val_loss: 0.4714 - val_acc: 0.8220\n",
      "Epoch 58/60\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.3940 - acc: 0.8612 - val_loss: 0.4734 - val_acc: 0.8160\n",
      "Epoch 59/60\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3922 - acc: 0.8615 - val_loss: 0.4796 - val_acc: 0.8120\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.3908 - acc: 0.8620 - val_loss: 0.4780 - val_acc: 0.8220\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 3s 44us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3871865220899167, 0.8624869565258856]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5503462923367818, 0.7959999996821085]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 2.5194 - acc: 0.2338 - val_loss: 2.3886 - val_acc: 0.3350\n",
      "Epoch 2/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 2.1918 - acc: 0.4437 - val_loss: 2.0099 - val_acc: 0.5130\n",
      "Epoch 3/120\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 1.8422 - acc: 0.6024 - val_loss: 1.6983 - val_acc: 0.6540\n",
      "Epoch 4/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.5905 - acc: 0.6865 - val_loss: 1.4963 - val_acc: 0.7130\n",
      "Epoch 5/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.4338 - acc: 0.7194 - val_loss: 1.3720 - val_acc: 0.7310\n",
      "Epoch 6/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.3355 - acc: 0.7370 - val_loss: 1.2916 - val_acc: 0.7540\n",
      "Epoch 7/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.2679 - acc: 0.7507 - val_loss: 1.2335 - val_acc: 0.7630\n",
      "Epoch 8/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.2165 - acc: 0.7603 - val_loss: 1.1846 - val_acc: 0.7640\n",
      "Epoch 9/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 1.1743 - acc: 0.7688 - val_loss: 1.1478 - val_acc: 0.7740\n",
      "Epoch 10/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1380 - acc: 0.7757 - val_loss: 1.1148 - val_acc: 0.7800\n",
      "Epoch 11/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1060 - acc: 0.7822 - val_loss: 1.0837 - val_acc: 0.7900\n",
      "Epoch 12/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0771 - acc: 0.7877 - val_loss: 1.0571 - val_acc: 0.8000\n",
      "Epoch 13/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0507 - acc: 0.7921 - val_loss: 1.0326 - val_acc: 0.7920\n",
      "Epoch 14/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0262 - acc: 0.7959 - val_loss: 1.0114 - val_acc: 0.8030\n",
      "Epoch 15/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 1.0036 - acc: 0.8005 - val_loss: 0.9910 - val_acc: 0.7870\n",
      "Epoch 16/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9822 - acc: 0.8042 - val_loss: 0.9716 - val_acc: 0.7930\n",
      "Epoch 17/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9624 - acc: 0.8077 - val_loss: 0.9545 - val_acc: 0.7950\n",
      "Epoch 18/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9435 - acc: 0.8093 - val_loss: 0.9365 - val_acc: 0.8000\n",
      "Epoch 19/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9261 - acc: 0.8123 - val_loss: 0.9226 - val_acc: 0.8040\n",
      "Epoch 20/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.9094 - acc: 0.8147 - val_loss: 0.9060 - val_acc: 0.8040\n",
      "Epoch 21/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8935 - acc: 0.8172 - val_loss: 0.8925 - val_acc: 0.8120\n",
      "Epoch 22/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8786 - acc: 0.8184 - val_loss: 0.8778 - val_acc: 0.8210\n",
      "Epoch 23/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8643 - acc: 0.8199 - val_loss: 0.8631 - val_acc: 0.8200\n",
      "Epoch 24/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8507 - acc: 0.8229 - val_loss: 0.8516 - val_acc: 0.8240\n",
      "Epoch 25/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8379 - acc: 0.8240 - val_loss: 0.8396 - val_acc: 0.8280\n",
      "Epoch 26/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8257 - acc: 0.8252 - val_loss: 0.8304 - val_acc: 0.8260\n",
      "Epoch 27/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8143 - acc: 0.8268 - val_loss: 0.8173 - val_acc: 0.8230\n",
      "Epoch 28/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8029 - acc: 0.8282 - val_loss: 0.8089 - val_acc: 0.8240\n",
      "Epoch 29/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7924 - acc: 0.8300 - val_loss: 0.8006 - val_acc: 0.8220\n",
      "Epoch 30/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7825 - acc: 0.8312 - val_loss: 0.7904 - val_acc: 0.8270\n",
      "Epoch 31/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7728 - acc: 0.8318 - val_loss: 0.7821 - val_acc: 0.8250\n",
      "Epoch 32/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7636 - acc: 0.8338 - val_loss: 0.7725 - val_acc: 0.8260\n",
      "Epoch 33/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7550 - acc: 0.8343 - val_loss: 0.7645 - val_acc: 0.8270\n",
      "Epoch 34/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7467 - acc: 0.8354 - val_loss: 0.7587 - val_acc: 0.8260\n",
      "Epoch 35/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7386 - acc: 0.8367 - val_loss: 0.7530 - val_acc: 0.8240\n",
      "Epoch 36/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7309 - acc: 0.8380 - val_loss: 0.7455 - val_acc: 0.8230\n",
      "Epoch 37/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7235 - acc: 0.8384 - val_loss: 0.7416 - val_acc: 0.8280\n",
      "Epoch 38/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7164 - acc: 0.8402 - val_loss: 0.7322 - val_acc: 0.8280\n",
      "Epoch 39/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7098 - acc: 0.8405 - val_loss: 0.7268 - val_acc: 0.8250\n",
      "Epoch 40/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7033 - acc: 0.8415 - val_loss: 0.7207 - val_acc: 0.8340\n",
      "Epoch 41/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6972 - acc: 0.8421 - val_loss: 0.7165 - val_acc: 0.8290\n",
      "Epoch 42/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6910 - acc: 0.8430 - val_loss: 0.7129 - val_acc: 0.8230\n",
      "Epoch 43/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6856 - acc: 0.8431 - val_loss: 0.7074 - val_acc: 0.8260\n",
      "Epoch 44/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.6802 - acc: 0.8440 - val_loss: 0.7028 - val_acc: 0.8250\n",
      "Epoch 45/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6748 - acc: 0.8447 - val_loss: 0.6990 - val_acc: 0.8300\n",
      "Epoch 46/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6698 - acc: 0.8459 - val_loss: 0.6915 - val_acc: 0.8320\n",
      "Epoch 47/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6651 - acc: 0.8458 - val_loss: 0.6891 - val_acc: 0.8350\n",
      "Epoch 48/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6606 - acc: 0.8464 - val_loss: 0.6878 - val_acc: 0.8270\n",
      "Epoch 49/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6559 - acc: 0.8466 - val_loss: 0.6839 - val_acc: 0.8250\n",
      "Epoch 50/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.6518 - acc: 0.8478 - val_loss: 0.6785 - val_acc: 0.8330\n",
      "Epoch 51/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6478 - acc: 0.8481 - val_loss: 0.6741 - val_acc: 0.8330\n",
      "Epoch 52/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6439 - acc: 0.8484 - val_loss: 0.6713 - val_acc: 0.8280\n",
      "Epoch 53/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6400 - acc: 0.8485 - val_loss: 0.6711 - val_acc: 0.8290\n",
      "Epoch 54/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6365 - acc: 0.8494 - val_loss: 0.6640 - val_acc: 0.8330\n",
      "Epoch 55/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.6329 - acc: 0.8501 - val_loss: 0.6619 - val_acc: 0.8310\n",
      "Epoch 56/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6296 - acc: 0.8504 - val_loss: 0.6595 - val_acc: 0.8320\n",
      "Epoch 57/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6264 - acc: 0.8504 - val_loss: 0.6563 - val_acc: 0.8300\n",
      "Epoch 58/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6231 - acc: 0.8506 - val_loss: 0.6561 - val_acc: 0.8320\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6202 - acc: 0.8509 - val_loss: 0.6531 - val_acc: 0.8260\n",
      "Epoch 60/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6173 - acc: 0.8517 - val_loss: 0.6501 - val_acc: 0.8250\n",
      "Epoch 61/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.6145 - acc: 0.8517 - val_loss: 0.6503 - val_acc: 0.8240\n",
      "Epoch 62/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6117 - acc: 0.8522 - val_loss: 0.6445 - val_acc: 0.8290\n",
      "Epoch 63/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6093 - acc: 0.8526 - val_loss: 0.6428 - val_acc: 0.8270\n",
      "Epoch 64/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6069 - acc: 0.8522 - val_loss: 0.6416 - val_acc: 0.8270\n",
      "Epoch 65/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.6042 - acc: 0.8522 - val_loss: 0.6430 - val_acc: 0.8310\n",
      "Epoch 66/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.6021 - acc: 0.8538 - val_loss: 0.6384 - val_acc: 0.8250\n",
      "Epoch 67/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5999 - acc: 0.8539 - val_loss: 0.6397 - val_acc: 0.8300\n",
      "Epoch 68/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5979 - acc: 0.8530 - val_loss: 0.6327 - val_acc: 0.8280\n",
      "Epoch 69/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.5955 - acc: 0.8546 - val_loss: 0.6329 - val_acc: 0.8280\n",
      "Epoch 70/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5938 - acc: 0.8537 - val_loss: 0.6293 - val_acc: 0.8260\n",
      "Epoch 71/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5916 - acc: 0.8545 - val_loss: 0.6302 - val_acc: 0.8280\n",
      "Epoch 72/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5898 - acc: 0.8549 - val_loss: 0.6282 - val_acc: 0.8300\n",
      "Epoch 73/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5878 - acc: 0.8554 - val_loss: 0.6280 - val_acc: 0.8290\n",
      "Epoch 74/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5862 - acc: 0.8555 - val_loss: 0.6247 - val_acc: 0.8240\n",
      "Epoch 75/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5844 - acc: 0.8553 - val_loss: 0.6247 - val_acc: 0.8280\n",
      "Epoch 76/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5828 - acc: 0.8555 - val_loss: 0.6246 - val_acc: 0.8310\n",
      "Epoch 77/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5813 - acc: 0.8557 - val_loss: 0.6212 - val_acc: 0.8300\n",
      "Epoch 78/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5797 - acc: 0.8560 - val_loss: 0.6289 - val_acc: 0.8230\n",
      "Epoch 79/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5782 - acc: 0.8564 - val_loss: 0.6224 - val_acc: 0.8260\n",
      "Epoch 80/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.5768 - acc: 0.8567 - val_loss: 0.6231 - val_acc: 0.8190\n",
      "Epoch 81/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5754 - acc: 0.8563 - val_loss: 0.6175 - val_acc: 0.8240\n",
      "Epoch 82/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5738 - acc: 0.8562 - val_loss: 0.6185 - val_acc: 0.8230\n",
      "Epoch 83/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5726 - acc: 0.8570 - val_loss: 0.6149 - val_acc: 0.8270\n",
      "Epoch 84/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5712 - acc: 0.8565 - val_loss: 0.6158 - val_acc: 0.8290\n",
      "Epoch 85/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5698 - acc: 0.8569 - val_loss: 0.6153 - val_acc: 0.8290\n",
      "Epoch 86/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.5686 - acc: 0.8576 - val_loss: 0.6146 - val_acc: 0.8260\n",
      "Epoch 87/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5673 - acc: 0.8576 - val_loss: 0.6147 - val_acc: 0.8270\n",
      "Epoch 88/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5664 - acc: 0.8569 - val_loss: 0.6106 - val_acc: 0.8290\n",
      "Epoch 89/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5651 - acc: 0.8581 - val_loss: 0.6118 - val_acc: 0.8330\n",
      "Epoch 90/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5641 - acc: 0.8581 - val_loss: 0.6128 - val_acc: 0.8320\n",
      "Epoch 91/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5630 - acc: 0.8585 - val_loss: 0.6079 - val_acc: 0.8300\n",
      "Epoch 92/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5621 - acc: 0.8584 - val_loss: 0.6074 - val_acc: 0.8190\n",
      "Epoch 93/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.5610 - acc: 0.8583 - val_loss: 0.6087 - val_acc: 0.8240\n",
      "Epoch 94/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.5598 - acc: 0.8574 - val_loss: 0.6090 - val_acc: 0.8190\n",
      "Epoch 95/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5588 - acc: 0.8587 - val_loss: 0.6077 - val_acc: 0.8280\n",
      "Epoch 96/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5579 - acc: 0.8586 - val_loss: 0.6067 - val_acc: 0.8270\n",
      "Epoch 97/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5572 - acc: 0.8590 - val_loss: 0.6050 - val_acc: 0.8250\n",
      "Epoch 98/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5562 - acc: 0.8589 - val_loss: 0.6043 - val_acc: 0.8310\n",
      "Epoch 99/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5552 - acc: 0.8589 - val_loss: 0.6115 - val_acc: 0.8200\n",
      "Epoch 100/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5543 - acc: 0.8593 - val_loss: 0.6062 - val_acc: 0.8230\n",
      "Epoch 101/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5539 - acc: 0.8588 - val_loss: 0.6018 - val_acc: 0.8270\n",
      "Epoch 102/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5529 - acc: 0.8587 - val_loss: 0.6056 - val_acc: 0.8220\n",
      "Epoch 103/120\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.5518 - acc: 0.8594 - val_loss: 0.6094 - val_acc: 0.8220\n",
      "Epoch 104/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.5514 - acc: 0.8599 - val_loss: 0.5999 - val_acc: 0.8280\n",
      "Epoch 105/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5507 - acc: 0.8603 - val_loss: 0.6006 - val_acc: 0.8280\n",
      "Epoch 106/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5500 - acc: 0.8595 - val_loss: 0.6001 - val_acc: 0.8290\n",
      "Epoch 107/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5492 - acc: 0.8600 - val_loss: 0.6038 - val_acc: 0.8280\n",
      "Epoch 108/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5480 - acc: 0.8597 - val_loss: 0.6095 - val_acc: 0.8300\n",
      "Epoch 109/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5476 - acc: 0.8603 - val_loss: 0.6003 - val_acc: 0.8300\n",
      "Epoch 110/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5469 - acc: 0.8604 - val_loss: 0.5998 - val_acc: 0.8320\n",
      "Epoch 111/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5464 - acc: 0.8601 - val_loss: 0.5988 - val_acc: 0.8220\n",
      "Epoch 112/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5453 - acc: 0.8610 - val_loss: 0.5973 - val_acc: 0.8260\n",
      "Epoch 113/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5447 - acc: 0.8608 - val_loss: 0.5982 - val_acc: 0.8280\n",
      "Epoch 114/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5443 - acc: 0.8606 - val_loss: 0.5946 - val_acc: 0.8280\n",
      "Epoch 115/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5438 - acc: 0.8604 - val_loss: 0.5989 - val_acc: 0.8370\n",
      "Epoch 116/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5429 - acc: 0.8607 - val_loss: 0.5970 - val_acc: 0.8300\n",
      "Epoch 117/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5424 - acc: 0.8612 - val_loss: 0.5985 - val_acc: 0.8270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5418 - acc: 0.8614 - val_loss: 0.5986 - val_acc: 0.8230\n",
      "Epoch 119/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.5411 - acc: 0.8619 - val_loss: 0.5951 - val_acc: 0.8280\n",
      "Epoch 120/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.5407 - acc: 0.8614 - val_loss: 0.5970 - val_acc: 0.8220\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 13.6098 - acc: 0.2207 - val_loss: 11.0606 - val_acc: 0.2750\n",
      "Epoch 2/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 8.9207 - acc: 0.4006 - val_loss: 6.9514 - val_acc: 0.4890\n",
      "Epoch 3/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 5.4333 - acc: 0.5619 - val_loss: 4.1055 - val_acc: 0.5850\n",
      "Epoch 4/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 3.2208 - acc: 0.6273 - val_loss: 2.5185 - val_acc: 0.6430\n",
      "Epoch 5/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 2.1998 - acc: 0.6511 - val_loss: 2.0155 - val_acc: 0.6460\n",
      "Epoch 6/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.9490 - acc: 0.6598 - val_loss: 1.8810 - val_acc: 0.6600\n",
      "Epoch 7/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.8351 - acc: 0.6683 - val_loss: 1.7804 - val_acc: 0.6690\n",
      "Epoch 8/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.7468 - acc: 0.6752 - val_loss: 1.7013 - val_acc: 0.6730\n",
      "Epoch 9/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.6729 - acc: 0.6801 - val_loss: 1.6287 - val_acc: 0.6840\n",
      "Epoch 10/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.6083 - acc: 0.6849 - val_loss: 1.5676 - val_acc: 0.6890\n",
      "Epoch 11/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.5505 - acc: 0.6896 - val_loss: 1.5105 - val_acc: 0.6970\n",
      "Epoch 12/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.4980 - acc: 0.6941 - val_loss: 1.4593 - val_acc: 0.7020\n",
      "Epoch 13/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.4497 - acc: 0.6981 - val_loss: 1.4140 - val_acc: 0.7070\n",
      "Epoch 14/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.4053 - acc: 0.7024 - val_loss: 1.3698 - val_acc: 0.7080\n",
      "Epoch 15/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.3637 - acc: 0.7058 - val_loss: 1.3283 - val_acc: 0.7130\n",
      "Epoch 16/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.3256 - acc: 0.7096 - val_loss: 1.2896 - val_acc: 0.7210\n",
      "Epoch 17/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.2906 - acc: 0.7126 - val_loss: 1.2579 - val_acc: 0.7170\n",
      "Epoch 18/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.2583 - acc: 0.7150 - val_loss: 1.2244 - val_acc: 0.7260\n",
      "Epoch 19/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.2283 - acc: 0.7173 - val_loss: 1.1940 - val_acc: 0.7310\n",
      "Epoch 20/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.2007 - acc: 0.7197 - val_loss: 1.1688 - val_acc: 0.7320\n",
      "Epoch 21/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1756 - acc: 0.7220 - val_loss: 1.1482 - val_acc: 0.7350\n",
      "Epoch 22/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 1.1529 - acc: 0.7239 - val_loss: 1.1244 - val_acc: 0.7360\n",
      "Epoch 23/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1327 - acc: 0.7258 - val_loss: 1.1006 - val_acc: 0.7330\n",
      "Epoch 24/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.1150 - acc: 0.7274 - val_loss: 1.0883 - val_acc: 0.7370\n",
      "Epoch 25/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0999 - acc: 0.7289 - val_loss: 1.0738 - val_acc: 0.7340\n",
      "Epoch 26/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0869 - acc: 0.7301 - val_loss: 1.0576 - val_acc: 0.7360\n",
      "Epoch 27/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0751 - acc: 0.7324 - val_loss: 1.0464 - val_acc: 0.7370\n",
      "Epoch 28/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0647 - acc: 0.7331 - val_loss: 1.0387 - val_acc: 0.7350\n",
      "Epoch 29/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0554 - acc: 0.7340 - val_loss: 1.0286 - val_acc: 0.7440\n",
      "Epoch 30/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0468 - acc: 0.7353 - val_loss: 1.0207 - val_acc: 0.7380\n",
      "Epoch 31/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0391 - acc: 0.7365 - val_loss: 1.0179 - val_acc: 0.7400\n",
      "Epoch 32/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0321 - acc: 0.7374 - val_loss: 1.0041 - val_acc: 0.7490\n",
      "Epoch 33/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0255 - acc: 0.7378 - val_loss: 0.9977 - val_acc: 0.7470\n",
      "Epoch 34/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0193 - acc: 0.7389 - val_loss: 0.9919 - val_acc: 0.7480\n",
      "Epoch 35/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0133 - acc: 0.7398 - val_loss: 0.9862 - val_acc: 0.7500\n",
      "Epoch 36/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0079 - acc: 0.7404 - val_loss: 0.9859 - val_acc: 0.7490\n",
      "Epoch 37/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0028 - acc: 0.7409 - val_loss: 0.9786 - val_acc: 0.7450\n",
      "Epoch 38/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9979 - acc: 0.7422 - val_loss: 0.9712 - val_acc: 0.7530\n",
      "Epoch 39/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.9934 - acc: 0.7422 - val_loss: 0.9670 - val_acc: 0.7530\n",
      "Epoch 40/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.9886 - acc: 0.7431 - val_loss: 0.9646 - val_acc: 0.7520\n",
      "Epoch 41/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9845 - acc: 0.7439 - val_loss: 0.9623 - val_acc: 0.7520\n",
      "Epoch 42/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9806 - acc: 0.7445 - val_loss: 0.9570 - val_acc: 0.7560\n",
      "Epoch 43/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9769 - acc: 0.7450 - val_loss: 0.9518 - val_acc: 0.7590\n",
      "Epoch 44/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9731 - acc: 0.7457 - val_loss: 0.9473 - val_acc: 0.7540\n",
      "Epoch 45/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9695 - acc: 0.7458 - val_loss: 0.9431 - val_acc: 0.7600\n",
      "Epoch 46/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9665 - acc: 0.7469 - val_loss: 0.9408 - val_acc: 0.7540\n",
      "Epoch 47/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9636 - acc: 0.7475 - val_loss: 0.9383 - val_acc: 0.7590\n",
      "Epoch 48/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9604 - acc: 0.7477 - val_loss: 0.9384 - val_acc: 0.7620\n",
      "Epoch 49/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9576 - acc: 0.7487 - val_loss: 0.9361 - val_acc: 0.7570\n",
      "Epoch 50/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9551 - acc: 0.7489 - val_loss: 0.9294 - val_acc: 0.7590\n",
      "Epoch 51/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9523 - acc: 0.7491 - val_loss: 0.9327 - val_acc: 0.7600\n",
      "Epoch 52/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9495 - acc: 0.7500 - val_loss: 0.9279 - val_acc: 0.7610\n",
      "Epoch 53/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9475 - acc: 0.7503 - val_loss: 0.9251 - val_acc: 0.7560\n",
      "Epoch 54/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9446 - acc: 0.7503 - val_loss: 0.9247 - val_acc: 0.7670\n",
      "Epoch 55/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9425 - acc: 0.7507 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 56/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9400 - acc: 0.7514 - val_loss: 0.9151 - val_acc: 0.7650\n",
      "Epoch 57/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.9378 - acc: 0.7520 - val_loss: 0.9178 - val_acc: 0.7690\n",
      "Epoch 58/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9358 - acc: 0.7514 - val_loss: 0.9148 - val_acc: 0.7610\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9335 - acc: 0.7532 - val_loss: 0.9072 - val_acc: 0.7670\n",
      "Epoch 60/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9315 - acc: 0.7535 - val_loss: 0.9043 - val_acc: 0.7620\n",
      "Epoch 61/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9294 - acc: 0.7533 - val_loss: 0.9023 - val_acc: 0.7650\n",
      "Epoch 62/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9279 - acc: 0.7540 - val_loss: 0.9078 - val_acc: 0.7670\n",
      "Epoch 63/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9260 - acc: 0.7545 - val_loss: 0.9024 - val_acc: 0.7660\n",
      "Epoch 64/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.9237 - acc: 0.7549 - val_loss: 0.8989 - val_acc: 0.7640\n",
      "Epoch 65/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9224 - acc: 0.7547 - val_loss: 0.8957 - val_acc: 0.7660\n",
      "Epoch 66/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.9208 - acc: 0.7558 - val_loss: 0.8921 - val_acc: 0.7650\n",
      "Epoch 67/120\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.9188 - acc: 0.7562 - val_loss: 0.8933 - val_acc: 0.7650\n",
      "Epoch 68/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9169 - acc: 0.7566 - val_loss: 0.8902 - val_acc: 0.7670\n",
      "Epoch 69/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9157 - acc: 0.7561 - val_loss: 0.8944 - val_acc: 0.7660\n",
      "Epoch 70/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9141 - acc: 0.7561 - val_loss: 0.8871 - val_acc: 0.7660\n",
      "Epoch 71/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.9126 - acc: 0.7567 - val_loss: 0.8888 - val_acc: 0.7630\n",
      "Epoch 72/120\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.9112 - acc: 0.7569 - val_loss: 0.8968 - val_acc: 0.7720\n",
      "Epoch 73/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9099 - acc: 0.7571 - val_loss: 0.8822 - val_acc: 0.7690\n",
      "Epoch 74/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.9082 - acc: 0.7574 - val_loss: 0.8802 - val_acc: 0.7680\n",
      "Epoch 75/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9068 - acc: 0.7581 - val_loss: 0.8823 - val_acc: 0.7690\n",
      "Epoch 76/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9058 - acc: 0.7577 - val_loss: 0.8787 - val_acc: 0.7690\n",
      "Epoch 77/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9045 - acc: 0.7582 - val_loss: 0.8784 - val_acc: 0.7680\n",
      "Epoch 78/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9027 - acc: 0.7596 - val_loss: 0.8767 - val_acc: 0.7720\n",
      "Epoch 79/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9018 - acc: 0.7590 - val_loss: 0.8747 - val_acc: 0.7700\n",
      "Epoch 80/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9002 - acc: 0.7591 - val_loss: 0.8755 - val_acc: 0.7660\n",
      "Epoch 81/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8989 - acc: 0.7592 - val_loss: 0.8727 - val_acc: 0.7680\n",
      "Epoch 82/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8982 - acc: 0.7601 - val_loss: 0.8746 - val_acc: 0.7720\n",
      "Epoch 83/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8976 - acc: 0.7593 - val_loss: 0.8715 - val_acc: 0.7740\n",
      "Epoch 84/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8955 - acc: 0.7608 - val_loss: 0.8706 - val_acc: 0.7760\n",
      "Epoch 85/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8950 - acc: 0.7609 - val_loss: 0.8764 - val_acc: 0.7710\n",
      "Epoch 86/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8939 - acc: 0.7605 - val_loss: 0.8664 - val_acc: 0.7720\n",
      "Epoch 87/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8924 - acc: 0.7606 - val_loss: 0.8652 - val_acc: 0.7730\n",
      "Epoch 88/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8918 - acc: 0.7613 - val_loss: 0.8685 - val_acc: 0.7680\n",
      "Epoch 89/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8901 - acc: 0.7618 - val_loss: 0.8728 - val_acc: 0.7670\n",
      "Epoch 90/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8899 - acc: 0.7610 - val_loss: 0.8624 - val_acc: 0.7720\n",
      "Epoch 91/120\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.8889 - acc: 0.7613 - val_loss: 0.8663 - val_acc: 0.7740\n",
      "Epoch 92/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8874 - acc: 0.7619 - val_loss: 0.8661 - val_acc: 0.7710\n",
      "Epoch 93/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8867 - acc: 0.7619 - val_loss: 0.8641 - val_acc: 0.7770\n",
      "Epoch 94/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8859 - acc: 0.7622 - val_loss: 0.8585 - val_acc: 0.7740\n",
      "Epoch 95/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8846 - acc: 0.7631 - val_loss: 0.8590 - val_acc: 0.7710\n",
      "Epoch 96/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8840 - acc: 0.7629 - val_loss: 0.8571 - val_acc: 0.7710\n",
      "Epoch 97/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8836 - acc: 0.7628 - val_loss: 0.8603 - val_acc: 0.7770\n",
      "Epoch 98/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8822 - acc: 0.7630 - val_loss: 0.8599 - val_acc: 0.7760\n",
      "Epoch 99/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8807 - acc: 0.7642 - val_loss: 0.8631 - val_acc: 0.7720\n",
      "Epoch 100/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8806 - acc: 0.7633 - val_loss: 0.8553 - val_acc: 0.7750\n",
      "Epoch 101/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8793 - acc: 0.7642 - val_loss: 0.8548 - val_acc: 0.7740\n",
      "Epoch 102/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8780 - acc: 0.7632 - val_loss: 0.8552 - val_acc: 0.7760\n",
      "Epoch 103/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8774 - acc: 0.7641 - val_loss: 0.8493 - val_acc: 0.7770\n",
      "Epoch 104/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8767 - acc: 0.7637 - val_loss: 0.8505 - val_acc: 0.7740\n",
      "Epoch 105/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8756 - acc: 0.7644 - val_loss: 0.8475 - val_acc: 0.7760\n",
      "Epoch 106/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8740 - acc: 0.7653 - val_loss: 0.8492 - val_acc: 0.7770\n",
      "Epoch 107/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8738 - acc: 0.7656 - val_loss: 0.8537 - val_acc: 0.7790\n",
      "Epoch 108/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8739 - acc: 0.7650 - val_loss: 0.8587 - val_acc: 0.7700\n",
      "Epoch 109/120\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8725 - acc: 0.7648 - val_loss: 0.8442 - val_acc: 0.7740\n",
      "Epoch 110/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8719 - acc: 0.7652 - val_loss: 0.8477 - val_acc: 0.7740\n",
      "Epoch 111/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8712 - acc: 0.7648 - val_loss: 0.8435 - val_acc: 0.7800\n",
      "Epoch 112/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8707 - acc: 0.7643 - val_loss: 0.8441 - val_acc: 0.7740\n",
      "Epoch 113/120\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8692 - acc: 0.7656 - val_loss: 0.8420 - val_acc: 0.7740\n",
      "Epoch 114/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8686 - acc: 0.7666 - val_loss: 0.8406 - val_acc: 0.7770\n",
      "Epoch 115/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8681 - acc: 0.7658 - val_loss: 0.8402 - val_acc: 0.7780\n",
      "Epoch 116/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8677 - acc: 0.7664 - val_loss: 0.8503 - val_acc: 0.7760\n",
      "Epoch 117/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8673 - acc: 0.7662 - val_loss: 0.8496 - val_acc: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8658 - acc: 0.7662 - val_loss: 0.8377 - val_acc: 0.7710\n",
      "Epoch 119/120\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8654 - acc: 0.7662 - val_loss: 0.8366 - val_acc: 0.7820\n",
      "Epoch 120/120\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8643 - acc: 0.7668 - val_loss: 0.8374 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 13.7017 - acc: 0.2068 - val_loss: 11.1667 - val_acc: 0.2510\n",
      "Epoch 2/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 9.0412 - acc: 0.3284 - val_loss: 7.0697 - val_acc: 0.4530\n",
      "Epoch 3/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 5.5442 - acc: 0.5469 - val_loss: 4.2037 - val_acc: 0.6160\n",
      "Epoch 4/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 3.3043 - acc: 0.6476 - val_loss: 2.5850 - val_acc: 0.6660\n",
      "Epoch 5/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 2.2554 - acc: 0.6793 - val_loss: 2.0568 - val_acc: 0.6900\n",
      "Epoch 6/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.9819 - acc: 0.6926 - val_loss: 1.8999 - val_acc: 0.7020\n",
      "Epoch 7/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.8519 - acc: 0.6998 - val_loss: 1.7887 - val_acc: 0.7120\n",
      "Epoch 8/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.7551 - acc: 0.7041 - val_loss: 1.7020 - val_acc: 0.7150\n",
      "Epoch 9/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.6779 - acc: 0.7075 - val_loss: 1.6311 - val_acc: 0.7160\n",
      "Epoch 10/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.6127 - acc: 0.7103 - val_loss: 1.5691 - val_acc: 0.7160\n",
      "Epoch 11/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.5556 - acc: 0.7122 - val_loss: 1.5156 - val_acc: 0.7190\n",
      "Epoch 12/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.5049 - acc: 0.7149 - val_loss: 1.4653 - val_acc: 0.7220\n",
      "Epoch 13/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.4589 - acc: 0.7166 - val_loss: 1.4202 - val_acc: 0.7210\n",
      "Epoch 14/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 1.4161 - acc: 0.7187 - val_loss: 1.3815 - val_acc: 0.7220\n",
      "Epoch 15/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.3767 - acc: 0.7209 - val_loss: 1.3416 - val_acc: 0.7300\n",
      "Epoch 16/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.3399 - acc: 0.7225 - val_loss: 1.3097 - val_acc: 0.7320\n",
      "Epoch 17/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.3057 - acc: 0.7242 - val_loss: 1.2731 - val_acc: 0.7370\n",
      "Epoch 18/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.2735 - acc: 0.7256 - val_loss: 1.2437 - val_acc: 0.7390\n",
      "Epoch 19/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.2442 - acc: 0.7272 - val_loss: 1.2131 - val_acc: 0.7380\n",
      "Epoch 20/1000\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 1.2175 - acc: 0.7289 - val_loss: 1.1906 - val_acc: 0.7410\n",
      "Epoch 21/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1935 - acc: 0.7307 - val_loss: 1.1643 - val_acc: 0.7410\n",
      "Epoch 22/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1719 - acc: 0.7314 - val_loss: 1.1441 - val_acc: 0.7480\n",
      "Epoch 23/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1524 - acc: 0.7330 - val_loss: 1.1245 - val_acc: 0.7470\n",
      "Epoch 24/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1355 - acc: 0.7339 - val_loss: 1.1125 - val_acc: 0.7520\n",
      "Epoch 25/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.1210 - acc: 0.7347 - val_loss: 1.0957 - val_acc: 0.7470\n",
      "Epoch 26/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.1073 - acc: 0.7371 - val_loss: 1.0825 - val_acc: 0.7520\n",
      "Epoch 27/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0954 - acc: 0.7371 - val_loss: 1.0730 - val_acc: 0.7510\n",
      "Epoch 28/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0848 - acc: 0.7382 - val_loss: 1.0596 - val_acc: 0.7530\n",
      "Epoch 29/1000\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 1.0747 - acc: 0.7394 - val_loss: 1.0499 - val_acc: 0.7520\n",
      "Epoch 30/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0654 - acc: 0.7398 - val_loss: 1.0420 - val_acc: 0.7530\n",
      "Epoch 31/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 1.0569 - acc: 0.7404 - val_loss: 1.0339 - val_acc: 0.7540\n",
      "Epoch 32/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0486 - acc: 0.7418 - val_loss: 1.0259 - val_acc: 0.7570\n",
      "Epoch 33/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0411 - acc: 0.7424 - val_loss: 1.0163 - val_acc: 0.7510\n",
      "Epoch 34/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0340 - acc: 0.7431 - val_loss: 1.0118 - val_acc: 0.7590\n",
      "Epoch 35/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0272 - acc: 0.7444 - val_loss: 1.0046 - val_acc: 0.7600\n",
      "Epoch 36/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0209 - acc: 0.7448 - val_loss: 0.9979 - val_acc: 0.7600\n",
      "Epoch 37/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0147 - acc: 0.7456 - val_loss: 0.9917 - val_acc: 0.7580\n",
      "Epoch 38/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 1.0090 - acc: 0.7458 - val_loss: 0.9851 - val_acc: 0.7620\n",
      "Epoch 39/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 1.0034 - acc: 0.7467 - val_loss: 0.9784 - val_acc: 0.7620\n",
      "Epoch 40/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9980 - acc: 0.7477 - val_loss: 0.9730 - val_acc: 0.7650\n",
      "Epoch 41/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9930 - acc: 0.7475 - val_loss: 0.9672 - val_acc: 0.7590\n",
      "Epoch 42/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9886 - acc: 0.7482 - val_loss: 0.9639 - val_acc: 0.7670\n",
      "Epoch 43/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9834 - acc: 0.7489 - val_loss: 0.9605 - val_acc: 0.7660\n",
      "Epoch 44/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9792 - acc: 0.7500 - val_loss: 0.9524 - val_acc: 0.7660\n",
      "Epoch 45/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9747 - acc: 0.7502 - val_loss: 0.9511 - val_acc: 0.7670\n",
      "Epoch 46/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9702 - acc: 0.7515 - val_loss: 0.9437 - val_acc: 0.7690\n",
      "Epoch 47/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9661 - acc: 0.7515 - val_loss: 0.9447 - val_acc: 0.7650\n",
      "Epoch 48/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9620 - acc: 0.7519 - val_loss: 0.9339 - val_acc: 0.7700\n",
      "Epoch 49/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.9579 - acc: 0.7523 - val_loss: 0.9302 - val_acc: 0.7710\n",
      "Epoch 50/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9539 - acc: 0.7532 - val_loss: 0.9277 - val_acc: 0.7680\n",
      "Epoch 51/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9499 - acc: 0.7536 - val_loss: 0.9230 - val_acc: 0.7710\n",
      "Epoch 52/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9464 - acc: 0.7534 - val_loss: 0.9222 - val_acc: 0.7710\n",
      "Epoch 53/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9428 - acc: 0.7545 - val_loss: 0.9137 - val_acc: 0.7730\n",
      "Epoch 54/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9393 - acc: 0.7546 - val_loss: 0.9098 - val_acc: 0.7720\n",
      "Epoch 55/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9358 - acc: 0.7550 - val_loss: 0.9068 - val_acc: 0.7720\n",
      "Epoch 56/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9322 - acc: 0.7555 - val_loss: 0.9048 - val_acc: 0.7680\n",
      "Epoch 57/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9291 - acc: 0.7561 - val_loss: 0.9014 - val_acc: 0.7740\n",
      "Epoch 58/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9258 - acc: 0.7566 - val_loss: 0.8963 - val_acc: 0.7710\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9226 - acc: 0.7563 - val_loss: 0.8961 - val_acc: 0.7720\n",
      "Epoch 60/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9195 - acc: 0.7567 - val_loss: 0.8981 - val_acc: 0.7610\n",
      "Epoch 61/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9168 - acc: 0.7581 - val_loss: 0.8897 - val_acc: 0.7740\n",
      "Epoch 62/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9137 - acc: 0.7585 - val_loss: 0.8864 - val_acc: 0.7720\n",
      "Epoch 63/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9111 - acc: 0.7576 - val_loss: 0.8813 - val_acc: 0.7770\n",
      "Epoch 64/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9075 - acc: 0.7592 - val_loss: 0.8832 - val_acc: 0.7730\n",
      "Epoch 65/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.9047 - acc: 0.7587 - val_loss: 0.8796 - val_acc: 0.7710\n",
      "Epoch 66/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.9020 - acc: 0.7596 - val_loss: 0.8771 - val_acc: 0.7740\n",
      "Epoch 67/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8993 - acc: 0.7596 - val_loss: 0.8727 - val_acc: 0.7750\n",
      "Epoch 68/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8966 - acc: 0.7598 - val_loss: 0.8695 - val_acc: 0.7730\n",
      "Epoch 69/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8941 - acc: 0.7602 - val_loss: 0.8635 - val_acc: 0.7810\n",
      "Epoch 70/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8920 - acc: 0.7606 - val_loss: 0.8626 - val_acc: 0.7740\n",
      "Epoch 71/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8895 - acc: 0.7609 - val_loss: 0.8601 - val_acc: 0.7770\n",
      "Epoch 72/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8878 - acc: 0.7609 - val_loss: 0.8605 - val_acc: 0.7760\n",
      "Epoch 73/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8855 - acc: 0.7613 - val_loss: 0.8575 - val_acc: 0.7790\n",
      "Epoch 74/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8836 - acc: 0.7617 - val_loss: 0.8551 - val_acc: 0.7810\n",
      "Epoch 75/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8818 - acc: 0.7618 - val_loss: 0.8541 - val_acc: 0.7770\n",
      "Epoch 76/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8799 - acc: 0.7623 - val_loss: 0.8529 - val_acc: 0.7700\n",
      "Epoch 77/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8784 - acc: 0.7618 - val_loss: 0.8484 - val_acc: 0.7780\n",
      "Epoch 78/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8767 - acc: 0.7623 - val_loss: 0.8447 - val_acc: 0.7780\n",
      "Epoch 79/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8750 - acc: 0.7634 - val_loss: 0.8454 - val_acc: 0.7770\n",
      "Epoch 80/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8736 - acc: 0.7633 - val_loss: 0.8441 - val_acc: 0.7830\n",
      "Epoch 81/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8724 - acc: 0.7635 - val_loss: 0.8438 - val_acc: 0.7800\n",
      "Epoch 82/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8707 - acc: 0.7635 - val_loss: 0.8428 - val_acc: 0.7740\n",
      "Epoch 83/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8691 - acc: 0.7649 - val_loss: 0.8375 - val_acc: 0.7760\n",
      "Epoch 84/1000\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.8679 - acc: 0.7640 - val_loss: 0.8414 - val_acc: 0.7770\n",
      "Epoch 85/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8666 - acc: 0.7649 - val_loss: 0.8395 - val_acc: 0.7830\n",
      "Epoch 86/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8661 - acc: 0.7647 - val_loss: 0.8385 - val_acc: 0.7770\n",
      "Epoch 87/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8651 - acc: 0.7642 - val_loss: 0.8338 - val_acc: 0.7760\n",
      "Epoch 88/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8640 - acc: 0.7644 - val_loss: 0.8353 - val_acc: 0.7770\n",
      "Epoch 89/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8634 - acc: 0.7650 - val_loss: 0.8414 - val_acc: 0.7780\n",
      "Epoch 90/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8620 - acc: 0.7653 - val_loss: 0.8330 - val_acc: 0.7830\n",
      "Epoch 91/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8624 - acc: 0.7652 - val_loss: 0.8350 - val_acc: 0.7830\n",
      "Epoch 92/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8605 - acc: 0.7659 - val_loss: 0.8351 - val_acc: 0.7700\n",
      "Epoch 93/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8599 - acc: 0.7668 - val_loss: 0.8306 - val_acc: 0.7820\n",
      "Epoch 94/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8594 - acc: 0.7664 - val_loss: 0.8373 - val_acc: 0.7810\n",
      "Epoch 95/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8588 - acc: 0.7667 - val_loss: 0.8255 - val_acc: 0.7840\n",
      "Epoch 96/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8578 - acc: 0.7672 - val_loss: 0.8361 - val_acc: 0.7820\n",
      "Epoch 97/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8573 - acc: 0.7674 - val_loss: 0.8370 - val_acc: 0.7800\n",
      "Epoch 98/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8568 - acc: 0.7669 - val_loss: 0.8243 - val_acc: 0.7830\n",
      "Epoch 99/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8554 - acc: 0.7678 - val_loss: 0.8229 - val_acc: 0.7900\n",
      "Epoch 100/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8552 - acc: 0.7684 - val_loss: 0.8219 - val_acc: 0.7860\n",
      "Epoch 101/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8542 - acc: 0.7681 - val_loss: 0.8239 - val_acc: 0.7840\n",
      "Epoch 102/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8538 - acc: 0.7686 - val_loss: 0.8263 - val_acc: 0.7790\n",
      "Epoch 103/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8527 - acc: 0.7691 - val_loss: 0.8243 - val_acc: 0.7810\n",
      "Epoch 104/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8520 - acc: 0.7696 - val_loss: 0.8284 - val_acc: 0.7810\n",
      "Epoch 105/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8523 - acc: 0.7680 - val_loss: 0.8209 - val_acc: 0.7840\n",
      "Epoch 106/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8507 - acc: 0.7696 - val_loss: 0.8209 - val_acc: 0.7850\n",
      "Epoch 107/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8503 - acc: 0.7688 - val_loss: 0.8158 - val_acc: 0.7870\n",
      "Epoch 108/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8496 - acc: 0.7694 - val_loss: 0.8200 - val_acc: 0.7860\n",
      "Epoch 109/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8490 - acc: 0.7694 - val_loss: 0.8183 - val_acc: 0.7850\n",
      "Epoch 110/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8483 - acc: 0.7697 - val_loss: 0.8155 - val_acc: 0.7860\n",
      "Epoch 111/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8483 - acc: 0.7703 - val_loss: 0.8220 - val_acc: 0.7850\n",
      "Epoch 112/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8476 - acc: 0.7713 - val_loss: 0.8153 - val_acc: 0.7850\n",
      "Epoch 113/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8464 - acc: 0.7709 - val_loss: 0.8201 - val_acc: 0.7820\n",
      "Epoch 114/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8462 - acc: 0.7711 - val_loss: 0.8130 - val_acc: 0.7870\n",
      "Epoch 115/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8450 - acc: 0.7711 - val_loss: 0.8141 - val_acc: 0.7840\n",
      "Epoch 116/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8447 - acc: 0.7705 - val_loss: 0.8128 - val_acc: 0.7870\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8441 - acc: 0.7713 - val_loss: 0.8106 - val_acc: 0.7900\n",
      "Epoch 118/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8436 - acc: 0.7716 - val_loss: 0.8113 - val_acc: 0.7850\n",
      "Epoch 119/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.8428 - acc: 0.7713 - val_loss: 0.8096 - val_acc: 0.7850\n",
      "Epoch 120/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8430 - acc: 0.7715 - val_loss: 0.8079 - val_acc: 0.7900\n",
      "Epoch 121/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8413 - acc: 0.7724 - val_loss: 0.8154 - val_acc: 0.7850\n",
      "Epoch 122/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8412 - acc: 0.7718 - val_loss: 0.8098 - val_acc: 0.7840\n",
      "Epoch 123/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8405 - acc: 0.7719 - val_loss: 0.8111 - val_acc: 0.7900\n",
      "Epoch 124/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8400 - acc: 0.7729 - val_loss: 0.8118 - val_acc: 0.7810\n",
      "Epoch 125/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8391 - acc: 0.7736 - val_loss: 0.8077 - val_acc: 0.7890\n",
      "Epoch 126/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8384 - acc: 0.7729 - val_loss: 0.8064 - val_acc: 0.7820\n",
      "Epoch 127/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8380 - acc: 0.7731 - val_loss: 0.8040 - val_acc: 0.7940\n",
      "Epoch 128/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8372 - acc: 0.7734 - val_loss: 0.8030 - val_acc: 0.7950\n",
      "Epoch 129/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8368 - acc: 0.7730 - val_loss: 0.8187 - val_acc: 0.7850\n",
      "Epoch 130/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8360 - acc: 0.7745 - val_loss: 0.8149 - val_acc: 0.7800\n",
      "Epoch 131/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8355 - acc: 0.7736 - val_loss: 0.8025 - val_acc: 0.7860\n",
      "Epoch 132/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8350 - acc: 0.7753 - val_loss: 0.8024 - val_acc: 0.7920\n",
      "Epoch 133/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8346 - acc: 0.7731 - val_loss: 0.8032 - val_acc: 0.7860\n",
      "Epoch 134/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8339 - acc: 0.7747 - val_loss: 0.8005 - val_acc: 0.7940\n",
      "Epoch 135/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8338 - acc: 0.7741 - val_loss: 0.8027 - val_acc: 0.7900\n",
      "Epoch 136/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8331 - acc: 0.7748 - val_loss: 0.7996 - val_acc: 0.7960\n",
      "Epoch 137/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.8319 - acc: 0.7753 - val_loss: 0.7973 - val_acc: 0.7960\n",
      "Epoch 138/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8313 - acc: 0.7758 - val_loss: 0.8098 - val_acc: 0.7920\n",
      "Epoch 139/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8308 - acc: 0.7745 - val_loss: 0.8016 - val_acc: 0.7870\n",
      "Epoch 140/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8302 - acc: 0.7755 - val_loss: 0.8016 - val_acc: 0.7930\n",
      "Epoch 141/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8297 - acc: 0.7761 - val_loss: 0.7975 - val_acc: 0.7910\n",
      "Epoch 142/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8287 - acc: 0.7763 - val_loss: 0.7999 - val_acc: 0.7870\n",
      "Epoch 143/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8286 - acc: 0.7766 - val_loss: 0.8033 - val_acc: 0.7890\n",
      "Epoch 144/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8283 - acc: 0.7765 - val_loss: 0.7954 - val_acc: 0.7960\n",
      "Epoch 145/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8276 - acc: 0.7762 - val_loss: 0.7936 - val_acc: 0.7960\n",
      "Epoch 146/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8268 - acc: 0.7772 - val_loss: 0.7935 - val_acc: 0.7990\n",
      "Epoch 147/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8262 - acc: 0.7773 - val_loss: 0.8016 - val_acc: 0.7980\n",
      "Epoch 148/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8259 - acc: 0.7769 - val_loss: 0.8049 - val_acc: 0.7910\n",
      "Epoch 149/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8255 - acc: 0.7775 - val_loss: 0.7963 - val_acc: 0.8000\n",
      "Epoch 150/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8248 - acc: 0.7780 - val_loss: 0.7942 - val_acc: 0.7960\n",
      "Epoch 151/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8247 - acc: 0.7768 - val_loss: 0.7909 - val_acc: 0.8040\n",
      "Epoch 152/1000\n",
      "57500/57500 [==============================] - 2s 35us/step - loss: 0.8247 - acc: 0.7774 - val_loss: 0.8224 - val_acc: 0.7620\n",
      "Epoch 153/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8232 - acc: 0.7777 - val_loss: 0.7951 - val_acc: 0.7900\n",
      "Epoch 154/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8223 - acc: 0.7780 - val_loss: 0.7916 - val_acc: 0.7970\n",
      "Epoch 155/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8230 - acc: 0.7780 - val_loss: 0.8063 - val_acc: 0.7880\n",
      "Epoch 156/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8220 - acc: 0.7781 - val_loss: 0.7936 - val_acc: 0.7920\n",
      "Epoch 157/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8211 - acc: 0.7790 - val_loss: 0.7877 - val_acc: 0.7970\n",
      "Epoch 158/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8206 - acc: 0.7789 - val_loss: 0.7897 - val_acc: 0.7920\n",
      "Epoch 159/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8199 - acc: 0.7794 - val_loss: 0.7836 - val_acc: 0.8050\n",
      "Epoch 160/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8195 - acc: 0.7807 - val_loss: 0.7867 - val_acc: 0.7960\n",
      "Epoch 161/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8196 - acc: 0.7794 - val_loss: 0.7879 - val_acc: 0.7960\n",
      "Epoch 162/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8182 - acc: 0.7797 - val_loss: 0.7845 - val_acc: 0.8020\n",
      "Epoch 163/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8176 - acc: 0.7802 - val_loss: 0.7836 - val_acc: 0.7970\n",
      "Epoch 164/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8180 - acc: 0.7803 - val_loss: 0.7863 - val_acc: 0.7920\n",
      "Epoch 165/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8173 - acc: 0.7800 - val_loss: 0.7921 - val_acc: 0.7880\n",
      "Epoch 166/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8173 - acc: 0.7803 - val_loss: 0.7844 - val_acc: 0.7970\n",
      "Epoch 167/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8158 - acc: 0.7807 - val_loss: 0.7822 - val_acc: 0.8040\n",
      "Epoch 168/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8155 - acc: 0.7812 - val_loss: 0.7858 - val_acc: 0.7960\n",
      "Epoch 169/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8153 - acc: 0.7808 - val_loss: 0.7898 - val_acc: 0.7980\n",
      "Epoch 170/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8153 - acc: 0.7811 - val_loss: 0.7825 - val_acc: 0.8020\n",
      "Epoch 171/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8149 - acc: 0.7799 - val_loss: 0.7818 - val_acc: 0.7930\n",
      "Epoch 172/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8143 - acc: 0.7828 - val_loss: 0.7915 - val_acc: 0.7950\n",
      "Epoch 173/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8141 - acc: 0.7811 - val_loss: 0.7895 - val_acc: 0.7950\n",
      "Epoch 174/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8125 - acc: 0.7828 - val_loss: 0.7791 - val_acc: 0.8050\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8121 - acc: 0.7820 - val_loss: 0.7851 - val_acc: 0.8030\n",
      "Epoch 176/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8115 - acc: 0.7835 - val_loss: 0.7928 - val_acc: 0.7930\n",
      "Epoch 177/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8106 - acc: 0.7831 - val_loss: 0.7795 - val_acc: 0.7980\n",
      "Epoch 178/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8108 - acc: 0.7833 - val_loss: 0.7790 - val_acc: 0.8020\n",
      "Epoch 179/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8110 - acc: 0.7828 - val_loss: 0.7761 - val_acc: 0.8030\n",
      "Epoch 180/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8093 - acc: 0.7833 - val_loss: 0.7871 - val_acc: 0.7960\n",
      "Epoch 181/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8108 - acc: 0.7839 - val_loss: 0.7778 - val_acc: 0.8010\n",
      "Epoch 182/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8089 - acc: 0.7848 - val_loss: 0.7758 - val_acc: 0.7940\n",
      "Epoch 183/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8086 - acc: 0.7836 - val_loss: 0.7804 - val_acc: 0.8000\n",
      "Epoch 184/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8080 - acc: 0.7847 - val_loss: 0.7710 - val_acc: 0.8050\n",
      "Epoch 185/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8083 - acc: 0.7848 - val_loss: 0.7712 - val_acc: 0.8020\n",
      "Epoch 186/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8075 - acc: 0.7838 - val_loss: 0.7838 - val_acc: 0.7940\n",
      "Epoch 187/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8068 - acc: 0.7855 - val_loss: 0.7933 - val_acc: 0.7860\n",
      "Epoch 188/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8083 - acc: 0.7839 - val_loss: 0.7740 - val_acc: 0.7960\n",
      "Epoch 189/1000\n",
      "57500/57500 [==============================] - 2s 35us/step - loss: 0.8060 - acc: 0.7866 - val_loss: 0.7801 - val_acc: 0.8060\n",
      "Epoch 190/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.8058 - acc: 0.7850 - val_loss: 0.7724 - val_acc: 0.8060\n",
      "Epoch 191/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.8057 - acc: 0.7868 - val_loss: 0.7717 - val_acc: 0.8040\n",
      "Epoch 192/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8048 - acc: 0.7852 - val_loss: 0.7653 - val_acc: 0.8100\n",
      "Epoch 193/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.8042 - acc: 0.7855 - val_loss: 0.7741 - val_acc: 0.8070\n",
      "Epoch 194/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8041 - acc: 0.7850 - val_loss: 0.7731 - val_acc: 0.8030\n",
      "Epoch 195/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8036 - acc: 0.7866 - val_loss: 0.7660 - val_acc: 0.8050\n",
      "Epoch 196/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8034 - acc: 0.7868 - val_loss: 0.7670 - val_acc: 0.8080\n",
      "Epoch 197/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8024 - acc: 0.7858 - val_loss: 0.7702 - val_acc: 0.8040\n",
      "Epoch 198/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8016 - acc: 0.7875 - val_loss: 0.7750 - val_acc: 0.8010\n",
      "Epoch 199/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8017 - acc: 0.7871 - val_loss: 0.7708 - val_acc: 0.8110\n",
      "Epoch 200/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8036 - acc: 0.7862 - val_loss: 0.7651 - val_acc: 0.8030\n",
      "Epoch 201/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8023 - acc: 0.7858 - val_loss: 0.7670 - val_acc: 0.8060\n",
      "Epoch 202/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8021 - acc: 0.7868 - val_loss: 0.7714 - val_acc: 0.8000\n",
      "Epoch 203/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8014 - acc: 0.7858 - val_loss: 0.7747 - val_acc: 0.8060\n",
      "Epoch 204/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8005 - acc: 0.7866 - val_loss: 0.7734 - val_acc: 0.7970\n",
      "Epoch 205/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8012 - acc: 0.7878 - val_loss: 0.7609 - val_acc: 0.8080\n",
      "Epoch 206/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7991 - acc: 0.7876 - val_loss: 0.7668 - val_acc: 0.8100\n",
      "Epoch 207/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.8004 - acc: 0.7872 - val_loss: 0.7784 - val_acc: 0.7980\n",
      "Epoch 208/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7991 - acc: 0.7874 - val_loss: 0.7972 - val_acc: 0.7990\n",
      "Epoch 209/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7986 - acc: 0.7881 - val_loss: 0.7755 - val_acc: 0.8000\n",
      "Epoch 210/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7975 - acc: 0.7890 - val_loss: 0.7773 - val_acc: 0.8050\n",
      "Epoch 211/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7984 - acc: 0.7884 - val_loss: 0.7684 - val_acc: 0.8070\n",
      "Epoch 212/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7968 - acc: 0.7887 - val_loss: 0.7593 - val_acc: 0.8120\n",
      "Epoch 213/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7966 - acc: 0.7879 - val_loss: 0.7781 - val_acc: 0.8000\n",
      "Epoch 214/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7989 - acc: 0.7890 - val_loss: 0.7571 - val_acc: 0.8120\n",
      "Epoch 215/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7964 - acc: 0.7890 - val_loss: 0.7730 - val_acc: 0.8060\n",
      "Epoch 216/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7952 - acc: 0.7897 - val_loss: 0.7731 - val_acc: 0.8030\n",
      "Epoch 217/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7962 - acc: 0.7891 - val_loss: 0.7561 - val_acc: 0.8140\n",
      "Epoch 218/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7953 - acc: 0.7889 - val_loss: 0.7720 - val_acc: 0.7990\n",
      "Epoch 219/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7948 - acc: 0.7887 - val_loss: 0.7641 - val_acc: 0.8070\n",
      "Epoch 220/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7959 - acc: 0.7892 - val_loss: 0.7726 - val_acc: 0.8050\n",
      "Epoch 221/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7939 - acc: 0.7906 - val_loss: 0.7942 - val_acc: 0.7790\n",
      "Epoch 222/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7938 - acc: 0.7888 - val_loss: 0.7966 - val_acc: 0.7910\n",
      "Epoch 223/1000\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.7941 - acc: 0.7887 - val_loss: 0.7651 - val_acc: 0.8030\n",
      "Epoch 224/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7925 - acc: 0.7901 - val_loss: 0.7555 - val_acc: 0.8060\n",
      "Epoch 225/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7931 - acc: 0.7900 - val_loss: 0.7642 - val_acc: 0.8070\n",
      "Epoch 226/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7931 - acc: 0.7905 - val_loss: 0.7606 - val_acc: 0.7980\n",
      "Epoch 227/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7926 - acc: 0.7913 - val_loss: 0.7585 - val_acc: 0.8020\n",
      "Epoch 228/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7912 - acc: 0.7897 - val_loss: 0.7526 - val_acc: 0.8050\n",
      "Epoch 229/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7919 - acc: 0.7905 - val_loss: 0.7566 - val_acc: 0.8050\n",
      "Epoch 230/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7918 - acc: 0.7897 - val_loss: 0.7568 - val_acc: 0.7980\n",
      "Epoch 231/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7903 - acc: 0.7906 - val_loss: 0.7560 - val_acc: 0.8090\n",
      "Epoch 232/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7906 - acc: 0.7913 - val_loss: 0.7656 - val_acc: 0.8040\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7909 - acc: 0.7899 - val_loss: 0.7491 - val_acc: 0.8140\n",
      "Epoch 234/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7912 - acc: 0.7911 - val_loss: 0.7687 - val_acc: 0.8050\n",
      "Epoch 235/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7929 - acc: 0.7899 - val_loss: 0.7588 - val_acc: 0.8010\n",
      "Epoch 236/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7906 - acc: 0.7899 - val_loss: 0.7639 - val_acc: 0.8040\n",
      "Epoch 237/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7912 - acc: 0.7905 - val_loss: 0.7793 - val_acc: 0.8090\n",
      "Epoch 238/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7909 - acc: 0.7905 - val_loss: 0.7485 - val_acc: 0.8110\n",
      "Epoch 239/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7891 - acc: 0.7921 - val_loss: 0.7546 - val_acc: 0.8150\n",
      "Epoch 240/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7898 - acc: 0.7918 - val_loss: 0.7714 - val_acc: 0.8020\n",
      "Epoch 241/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7881 - acc: 0.7918 - val_loss: 0.7504 - val_acc: 0.8130\n",
      "Epoch 242/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7878 - acc: 0.7919 - val_loss: 0.7756 - val_acc: 0.7920\n",
      "Epoch 243/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7892 - acc: 0.7927 - val_loss: 0.7531 - val_acc: 0.8060\n",
      "Epoch 244/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7894 - acc: 0.7913 - val_loss: 0.7765 - val_acc: 0.8040\n",
      "Epoch 245/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7893 - acc: 0.7917 - val_loss: 0.7938 - val_acc: 0.7810\n",
      "Epoch 246/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7886 - acc: 0.7915 - val_loss: 0.7487 - val_acc: 0.8120\n",
      "Epoch 247/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7860 - acc: 0.7932 - val_loss: 0.7406 - val_acc: 0.8160\n",
      "Epoch 248/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7871 - acc: 0.7938 - val_loss: 0.7624 - val_acc: 0.8060\n",
      "Epoch 249/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7863 - acc: 0.7927 - val_loss: 0.7483 - val_acc: 0.8130\n",
      "Epoch 250/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7872 - acc: 0.7945 - val_loss: 0.7500 - val_acc: 0.8120\n",
      "Epoch 251/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7872 - acc: 0.7926 - val_loss: 0.7561 - val_acc: 0.8030\n",
      "Epoch 252/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7851 - acc: 0.7930 - val_loss: 0.7731 - val_acc: 0.7980\n",
      "Epoch 253/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7866 - acc: 0.7915 - val_loss: 0.7798 - val_acc: 0.7930\n",
      "Epoch 254/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7867 - acc: 0.7915 - val_loss: 0.7476 - val_acc: 0.8150\n",
      "Epoch 255/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7891 - acc: 0.7909 - val_loss: 0.7410 - val_acc: 0.8180\n",
      "Epoch 256/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7854 - acc: 0.7943 - val_loss: 0.7533 - val_acc: 0.8060\n",
      "Epoch 257/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7871 - acc: 0.7913 - val_loss: 0.7446 - val_acc: 0.8160\n",
      "Epoch 258/1000\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.7864 - acc: 0.7922 - val_loss: 0.7732 - val_acc: 0.7970\n",
      "Epoch 259/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7846 - acc: 0.7929 - val_loss: 0.7932 - val_acc: 0.8010\n",
      "Epoch 260/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7846 - acc: 0.7946 - val_loss: 0.7641 - val_acc: 0.7970\n",
      "Epoch 261/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7858 - acc: 0.7927 - val_loss: 0.7670 - val_acc: 0.7990\n",
      "Epoch 262/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7877 - acc: 0.7917 - val_loss: 0.7789 - val_acc: 0.8040\n",
      "Epoch 263/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7849 - acc: 0.7927 - val_loss: 0.7421 - val_acc: 0.8150\n",
      "Epoch 264/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7852 - acc: 0.7931 - val_loss: 0.7545 - val_acc: 0.8080\n",
      "Epoch 265/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7854 - acc: 0.7926 - val_loss: 0.7544 - val_acc: 0.8000\n",
      "Epoch 266/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7853 - acc: 0.7931 - val_loss: 0.7456 - val_acc: 0.8060\n",
      "Epoch 267/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7850 - acc: 0.7934 - val_loss: 0.7786 - val_acc: 0.7900\n",
      "Epoch 268/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7874 - acc: 0.7912 - val_loss: 0.7559 - val_acc: 0.8110\n",
      "Epoch 269/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7850 - acc: 0.7915 - val_loss: 0.7608 - val_acc: 0.8050\n",
      "Epoch 270/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7868 - acc: 0.7935 - val_loss: 0.7764 - val_acc: 0.7940\n",
      "Epoch 271/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7880 - acc: 0.7908 - val_loss: 0.7463 - val_acc: 0.8060\n",
      "Epoch 272/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7836 - acc: 0.7943 - val_loss: 0.7488 - val_acc: 0.8080\n",
      "Epoch 273/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7829 - acc: 0.7917 - val_loss: 0.7564 - val_acc: 0.7950\n",
      "Epoch 274/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7869 - acc: 0.7915 - val_loss: 0.7989 - val_acc: 0.7820\n",
      "Epoch 275/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7837 - acc: 0.7942 - val_loss: 0.7728 - val_acc: 0.7930\n",
      "Epoch 276/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7805 - acc: 0.7947 - val_loss: 0.8138 - val_acc: 0.7880\n",
      "Epoch 277/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7855 - acc: 0.7921 - val_loss: 0.7598 - val_acc: 0.8110\n",
      "Epoch 278/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7815 - acc: 0.7949 - val_loss: 0.7484 - val_acc: 0.8160\n",
      "Epoch 279/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7839 - acc: 0.7931 - val_loss: 0.7522 - val_acc: 0.8090\n",
      "Epoch 280/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7854 - acc: 0.7922 - val_loss: 0.7660 - val_acc: 0.8090\n",
      "Epoch 281/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7816 - acc: 0.7932 - val_loss: 0.7391 - val_acc: 0.8210\n",
      "Epoch 282/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7817 - acc: 0.7945 - val_loss: 0.7708 - val_acc: 0.8050\n",
      "Epoch 283/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7836 - acc: 0.7946 - val_loss: 0.7831 - val_acc: 0.7940\n",
      "Epoch 284/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7838 - acc: 0.7932 - val_loss: 0.7722 - val_acc: 0.7920\n",
      "Epoch 285/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7840 - acc: 0.7925 - val_loss: 0.7620 - val_acc: 0.7980\n",
      "Epoch 286/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7854 - acc: 0.7920 - val_loss: 0.7416 - val_acc: 0.8130\n",
      "Epoch 287/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7840 - acc: 0.7934 - val_loss: 0.7450 - val_acc: 0.8050\n",
      "Epoch 288/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7835 - acc: 0.7920 - val_loss: 0.7457 - val_acc: 0.8170\n",
      "Epoch 289/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7814 - acc: 0.7945 - val_loss: 0.7703 - val_acc: 0.8000\n",
      "Epoch 290/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7850 - acc: 0.7930 - val_loss: 0.7826 - val_acc: 0.7850\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7826 - acc: 0.7920 - val_loss: 0.7553 - val_acc: 0.7980\n",
      "Epoch 292/1000\n",
      "57500/57500 [==============================] - 2s 35us/step - loss: 0.7867 - acc: 0.7919 - val_loss: 0.7536 - val_acc: 0.8070\n",
      "Epoch 293/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7827 - acc: 0.7937 - val_loss: 0.8527 - val_acc: 0.7490\n",
      "Epoch 294/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7828 - acc: 0.7935 - val_loss: 0.7427 - val_acc: 0.8110\n",
      "Epoch 295/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7842 - acc: 0.7919 - val_loss: 0.7413 - val_acc: 0.8100\n",
      "Epoch 296/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7832 - acc: 0.7951 - val_loss: 0.7463 - val_acc: 0.8120\n",
      "Epoch 297/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7887 - acc: 0.7914 - val_loss: 0.7386 - val_acc: 0.8140\n",
      "Epoch 298/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7862 - acc: 0.7906 - val_loss: 0.7580 - val_acc: 0.7990\n",
      "Epoch 299/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7814 - acc: 0.7938 - val_loss: 0.7490 - val_acc: 0.8110\n",
      "Epoch 300/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7795 - acc: 0.7933 - val_loss: 0.7523 - val_acc: 0.8140\n",
      "Epoch 301/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7819 - acc: 0.7932 - val_loss: 0.8449 - val_acc: 0.7620\n",
      "Epoch 302/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7840 - acc: 0.7923 - val_loss: 0.7444 - val_acc: 0.8070\n",
      "Epoch 303/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7824 - acc: 0.7935 - val_loss: 0.7480 - val_acc: 0.8070\n",
      "Epoch 304/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7832 - acc: 0.7922 - val_loss: 0.7606 - val_acc: 0.7990\n",
      "Epoch 305/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7910 - acc: 0.7894 - val_loss: 0.7441 - val_acc: 0.8070\n",
      "Epoch 306/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7805 - acc: 0.7952 - val_loss: 0.7587 - val_acc: 0.8010\n",
      "Epoch 307/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7840 - acc: 0.7929 - val_loss: 0.7424 - val_acc: 0.8120\n",
      "Epoch 308/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7810 - acc: 0.7927 - val_loss: 0.7646 - val_acc: 0.8000\n",
      "Epoch 309/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7796 - acc: 0.7939 - val_loss: 0.7468 - val_acc: 0.8090\n",
      "Epoch 310/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7837 - acc: 0.7930 - val_loss: 0.8032 - val_acc: 0.7860\n",
      "Epoch 311/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7821 - acc: 0.7936 - val_loss: 0.7892 - val_acc: 0.7800\n",
      "Epoch 312/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7777 - acc: 0.7962 - val_loss: 0.7511 - val_acc: 0.8020\n",
      "Epoch 313/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7845 - acc: 0.7929 - val_loss: 0.7792 - val_acc: 0.7910\n",
      "Epoch 314/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7826 - acc: 0.7922 - val_loss: 0.7516 - val_acc: 0.8030\n",
      "Epoch 315/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7861 - acc: 0.7909 - val_loss: 0.8103 - val_acc: 0.7830\n",
      "Epoch 316/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7827 - acc: 0.7921 - val_loss: 0.7440 - val_acc: 0.8130\n",
      "Epoch 317/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7808 - acc: 0.7951 - val_loss: 0.7452 - val_acc: 0.8090\n",
      "Epoch 318/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7788 - acc: 0.7951 - val_loss: 0.7670 - val_acc: 0.7880\n",
      "Epoch 319/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7804 - acc: 0.7950 - val_loss: 0.7461 - val_acc: 0.7940\n",
      "Epoch 320/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7814 - acc: 0.7939 - val_loss: 0.9100 - val_acc: 0.7300\n",
      "Epoch 321/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7817 - acc: 0.7936 - val_loss: 0.7431 - val_acc: 0.8140\n",
      "Epoch 322/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7812 - acc: 0.7935 - val_loss: 0.7427 - val_acc: 0.8070\n",
      "Epoch 323/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7815 - acc: 0.7935 - val_loss: 0.7486 - val_acc: 0.8090\n",
      "Epoch 324/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7811 - acc: 0.7942 - val_loss: 0.7318 - val_acc: 0.8150\n",
      "Epoch 325/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7808 - acc: 0.7951 - val_loss: 0.7935 - val_acc: 0.7920\n",
      "Epoch 326/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7856 - acc: 0.7914 - val_loss: 0.7500 - val_acc: 0.8150\n",
      "Epoch 327/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7813 - acc: 0.7932 - val_loss: 0.7591 - val_acc: 0.8040\n",
      "Epoch 328/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7796 - acc: 0.7954 - val_loss: 0.7522 - val_acc: 0.8110\n",
      "Epoch 329/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7803 - acc: 0.7941 - val_loss: 0.7403 - val_acc: 0.8040\n",
      "Epoch 330/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7800 - acc: 0.7931 - val_loss: 0.7467 - val_acc: 0.8130\n",
      "Epoch 331/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7817 - acc: 0.7919 - val_loss: 0.7680 - val_acc: 0.7910\n",
      "Epoch 332/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7818 - acc: 0.7922 - val_loss: 0.7255 - val_acc: 0.8210\n",
      "Epoch 333/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7768 - acc: 0.7946 - val_loss: 0.8166 - val_acc: 0.7610\n",
      "Epoch 334/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7804 - acc: 0.7936 - val_loss: 0.8606 - val_acc: 0.7560\n",
      "Epoch 335/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7844 - acc: 0.7910 - val_loss: 0.7586 - val_acc: 0.7840\n",
      "Epoch 336/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7790 - acc: 0.7935 - val_loss: 0.7630 - val_acc: 0.7990\n",
      "Epoch 337/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7851 - acc: 0.7909 - val_loss: 0.7371 - val_acc: 0.8100\n",
      "Epoch 338/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7768 - acc: 0.7946 - val_loss: 0.7774 - val_acc: 0.7940\n",
      "Epoch 339/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7783 - acc: 0.7936 - val_loss: 0.7533 - val_acc: 0.8010\n",
      "Epoch 340/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7803 - acc: 0.7934 - val_loss: 0.7364 - val_acc: 0.8110\n",
      "Epoch 341/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7772 - acc: 0.7954 - val_loss: 0.7358 - val_acc: 0.8050\n",
      "Epoch 342/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7820 - acc: 0.7928 - val_loss: 0.7798 - val_acc: 0.7950\n",
      "Epoch 343/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7791 - acc: 0.7936 - val_loss: 0.7438 - val_acc: 0.8080\n",
      "Epoch 344/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7786 - acc: 0.7938 - val_loss: 0.7481 - val_acc: 0.8110\n",
      "Epoch 345/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7797 - acc: 0.7923 - val_loss: 0.7468 - val_acc: 0.8050\n",
      "Epoch 346/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7841 - acc: 0.7919 - val_loss: 0.7782 - val_acc: 0.8020\n",
      "Epoch 347/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7789 - acc: 0.7944 - val_loss: 0.7463 - val_acc: 0.8090\n",
      "Epoch 348/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7842 - acc: 0.7910 - val_loss: 0.7368 - val_acc: 0.8080\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7777 - acc: 0.7951 - val_loss: 0.7526 - val_acc: 0.8030\n",
      "Epoch 350/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7886 - acc: 0.7914 - val_loss: 0.7555 - val_acc: 0.7930\n",
      "Epoch 351/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7791 - acc: 0.7931 - val_loss: 0.7472 - val_acc: 0.8060\n",
      "Epoch 352/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7746 - acc: 0.7956 - val_loss: 0.7735 - val_acc: 0.7860\n",
      "Epoch 353/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7784 - acc: 0.7935 - val_loss: 1.0383 - val_acc: 0.7140\n",
      "Epoch 354/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7862 - acc: 0.7917 - val_loss: 0.7382 - val_acc: 0.8220\n",
      "Epoch 355/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7763 - acc: 0.7946 - val_loss: 0.7569 - val_acc: 0.8060\n",
      "Epoch 356/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7775 - acc: 0.7941 - val_loss: 0.7728 - val_acc: 0.7900\n",
      "Epoch 357/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7794 - acc: 0.7933 - val_loss: 0.7525 - val_acc: 0.8020\n",
      "Epoch 358/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7854 - acc: 0.7915 - val_loss: 0.7368 - val_acc: 0.8150\n",
      "Epoch 359/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7806 - acc: 0.7943 - val_loss: 0.7520 - val_acc: 0.8070\n",
      "Epoch 360/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7759 - acc: 0.7954 - val_loss: 0.7324 - val_acc: 0.8100\n",
      "Epoch 361/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7829 - acc: 0.7915 - val_loss: 0.7501 - val_acc: 0.8040\n",
      "Epoch 362/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7796 - acc: 0.7929 - val_loss: 0.7487 - val_acc: 0.8200\n",
      "Epoch 363/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7784 - acc: 0.7928 - val_loss: 0.7266 - val_acc: 0.8130\n",
      "Epoch 364/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7755 - acc: 0.7951 - val_loss: 0.7384 - val_acc: 0.8050\n",
      "Epoch 365/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7758 - acc: 0.7942 - val_loss: 0.7454 - val_acc: 0.8060\n",
      "Epoch 366/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7772 - acc: 0.7938 - val_loss: 0.7658 - val_acc: 0.7890\n",
      "Epoch 367/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7786 - acc: 0.7945 - val_loss: 0.7805 - val_acc: 0.7920\n",
      "Epoch 368/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7762 - acc: 0.7950 - val_loss: 0.7245 - val_acc: 0.8090\n",
      "Epoch 369/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7769 - acc: 0.7937 - val_loss: 0.7627 - val_acc: 0.7930\n",
      "Epoch 370/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7799 - acc: 0.7928 - val_loss: 0.7330 - val_acc: 0.8210\n",
      "Epoch 371/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7749 - acc: 0.7937 - val_loss: 0.7252 - val_acc: 0.8220\n",
      "Epoch 372/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7795 - acc: 0.7922 - val_loss: 0.7310 - val_acc: 0.8200\n",
      "Epoch 373/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7785 - acc: 0.7933 - val_loss: 0.8446 - val_acc: 0.7620\n",
      "Epoch 374/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7812 - acc: 0.7920 - val_loss: 0.7612 - val_acc: 0.8010\n",
      "Epoch 375/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7761 - acc: 0.7944 - val_loss: 0.7348 - val_acc: 0.8140\n",
      "Epoch 376/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7806 - acc: 0.7926 - val_loss: 0.7757 - val_acc: 0.8070\n",
      "Epoch 377/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7758 - acc: 0.7941 - val_loss: 0.7322 - val_acc: 0.8090\n",
      "Epoch 378/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7778 - acc: 0.7933 - val_loss: 0.7403 - val_acc: 0.8140\n",
      "Epoch 379/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7800 - acc: 0.7946 - val_loss: 0.7705 - val_acc: 0.7970\n",
      "Epoch 380/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7799 - acc: 0.7917 - val_loss: 0.7533 - val_acc: 0.8090\n",
      "Epoch 381/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7803 - acc: 0.7947 - val_loss: 0.7543 - val_acc: 0.8060\n",
      "Epoch 382/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7740 - acc: 0.7952 - val_loss: 0.8059 - val_acc: 0.7760\n",
      "Epoch 383/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7778 - acc: 0.7939 - val_loss: 0.7720 - val_acc: 0.8060\n",
      "Epoch 384/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7780 - acc: 0.7923 - val_loss: 0.7613 - val_acc: 0.8040\n",
      "Epoch 385/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7798 - acc: 0.7921 - val_loss: 0.7339 - val_acc: 0.8120\n",
      "Epoch 386/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7742 - acc: 0.7953 - val_loss: 0.7385 - val_acc: 0.8120\n",
      "Epoch 387/1000\n",
      "57500/57500 [==============================] - 2s 38us/step - loss: 0.7816 - acc: 0.7910 - val_loss: 0.7435 - val_acc: 0.8030\n",
      "Epoch 388/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7773 - acc: 0.7935 - val_loss: 0.7353 - val_acc: 0.7970\n",
      "Epoch 389/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7812 - acc: 0.7909 - val_loss: 0.7326 - val_acc: 0.8150\n",
      "Epoch 390/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7761 - acc: 0.7943 - val_loss: 0.7963 - val_acc: 0.7910\n",
      "Epoch 391/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7833 - acc: 0.7914 - val_loss: 0.7522 - val_acc: 0.7950\n",
      "Epoch 392/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7773 - acc: 0.7937 - val_loss: 0.7336 - val_acc: 0.8160\n",
      "Epoch 393/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7819 - acc: 0.7911 - val_loss: 0.7299 - val_acc: 0.8110\n",
      "Epoch 394/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7779 - acc: 0.7942 - val_loss: 0.7481 - val_acc: 0.8050\n",
      "Epoch 395/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7760 - acc: 0.7941 - val_loss: 0.7386 - val_acc: 0.8130\n",
      "Epoch 396/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7746 - acc: 0.7938 - val_loss: 0.7430 - val_acc: 0.8000\n",
      "Epoch 397/1000\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.7768 - acc: 0.7944 - val_loss: 0.8191 - val_acc: 0.7690\n",
      "Epoch 398/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7776 - acc: 0.7931 - val_loss: 0.7468 - val_acc: 0.8120\n",
      "Epoch 399/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7748 - acc: 0.7943 - val_loss: 0.7359 - val_acc: 0.8130\n",
      "Epoch 400/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7832 - acc: 0.7905 - val_loss: 0.7324 - val_acc: 0.8080\n",
      "Epoch 401/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7832 - acc: 0.7920 - val_loss: 0.7289 - val_acc: 0.8160\n",
      "Epoch 402/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7765 - acc: 0.7959 - val_loss: 0.7587 - val_acc: 0.8010\n",
      "Epoch 403/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7786 - acc: 0.7935 - val_loss: 0.7599 - val_acc: 0.7910\n",
      "Epoch 404/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7759 - acc: 0.7930 - val_loss: 0.7421 - val_acc: 0.8200\n",
      "Epoch 405/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7769 - acc: 0.7938 - val_loss: 0.7504 - val_acc: 0.8040\n",
      "Epoch 406/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7753 - acc: 0.7935 - val_loss: 0.7321 - val_acc: 0.8130\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7776 - acc: 0.7942 - val_loss: 0.7944 - val_acc: 0.7890\n",
      "Epoch 408/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7727 - acc: 0.7952 - val_loss: 0.7517 - val_acc: 0.7970\n",
      "Epoch 409/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7854 - acc: 0.7890 - val_loss: 0.7237 - val_acc: 0.8180\n",
      "Epoch 410/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7761 - acc: 0.7935 - val_loss: 0.7471 - val_acc: 0.8020\n",
      "Epoch 411/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7746 - acc: 0.7937 - val_loss: 0.7485 - val_acc: 0.8100\n",
      "Epoch 412/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7758 - acc: 0.7948 - val_loss: 0.7390 - val_acc: 0.8130\n",
      "Epoch 413/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7770 - acc: 0.7935 - val_loss: 0.7534 - val_acc: 0.8060\n",
      "Epoch 414/1000\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.7818 - acc: 0.7902 - val_loss: 0.7554 - val_acc: 0.8000\n",
      "Epoch 415/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7788 - acc: 0.7926 - val_loss: 0.7301 - val_acc: 0.8120\n",
      "Epoch 416/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7772 - acc: 0.7942 - val_loss: 0.7677 - val_acc: 0.7830\n",
      "Epoch 417/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7818 - acc: 0.7910 - val_loss: 0.7701 - val_acc: 0.7940\n",
      "Epoch 418/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7752 - acc: 0.7940 - val_loss: 0.7270 - val_acc: 0.8270\n",
      "Epoch 419/1000\n",
      "57500/57500 [==============================] - 2s 35us/step - loss: 0.7783 - acc: 0.7930 - val_loss: 0.7516 - val_acc: 0.8080\n",
      "Epoch 420/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7813 - acc: 0.7912 - val_loss: 0.7743 - val_acc: 0.7910\n",
      "Epoch 421/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7805 - acc: 0.7920 - val_loss: 0.7301 - val_acc: 0.8220\n",
      "Epoch 422/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7761 - acc: 0.7933 - val_loss: 0.7247 - val_acc: 0.8150\n",
      "Epoch 423/1000\n",
      "57500/57500 [==============================] - 2s 39us/step - loss: 0.7766 - acc: 0.7938 - val_loss: 0.7205 - val_acc: 0.8200\n",
      "Epoch 424/1000\n",
      "57500/57500 [==============================] - 2s 40us/step - loss: 0.7795 - acc: 0.7905 - val_loss: 0.7375 - val_acc: 0.8160\n",
      "Epoch 425/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7717 - acc: 0.7962 - val_loss: 0.7452 - val_acc: 0.8100\n",
      "Epoch 426/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7767 - acc: 0.7931 - val_loss: 0.7556 - val_acc: 0.8050\n",
      "Epoch 427/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7767 - acc: 0.7927 - val_loss: 0.7279 - val_acc: 0.8130\n",
      "Epoch 428/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7780 - acc: 0.7921 - val_loss: 0.7644 - val_acc: 0.8010\n",
      "Epoch 429/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7735 - acc: 0.7931 - val_loss: 0.7513 - val_acc: 0.7870\n",
      "Epoch 430/1000\n",
      "57500/57500 [==============================] - 2s 37us/step - loss: 0.7716 - acc: 0.7948 - val_loss: 0.7398 - val_acc: 0.8110\n",
      "Epoch 431/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7776 - acc: 0.7927 - val_loss: 0.7323 - val_acc: 0.8120\n",
      "Epoch 432/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7828 - acc: 0.7897 - val_loss: 0.7370 - val_acc: 0.8010\n",
      "Epoch 433/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7776 - acc: 0.7911 - val_loss: 0.7499 - val_acc: 0.7940\n",
      "Epoch 434/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7768 - acc: 0.7924 - val_loss: 0.7581 - val_acc: 0.8030\n",
      "Epoch 435/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7719 - acc: 0.7951 - val_loss: 0.7220 - val_acc: 0.8210\n",
      "Epoch 436/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7790 - acc: 0.7917 - val_loss: 0.7322 - val_acc: 0.8200\n",
      "Epoch 437/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7740 - acc: 0.7941 - val_loss: 0.7244 - val_acc: 0.8240\n",
      "Epoch 438/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7797 - acc: 0.7921 - val_loss: 0.7500 - val_acc: 0.8090\n",
      "Epoch 439/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7739 - acc: 0.7952 - val_loss: 0.7534 - val_acc: 0.8100\n",
      "Epoch 440/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7727 - acc: 0.7947 - val_loss: 0.7456 - val_acc: 0.8180\n",
      "Epoch 441/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7767 - acc: 0.7915 - val_loss: 0.7929 - val_acc: 0.7760\n",
      "Epoch 442/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7741 - acc: 0.7942 - val_loss: 0.7209 - val_acc: 0.8190\n",
      "Epoch 443/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7725 - acc: 0.7930 - val_loss: 0.7229 - val_acc: 0.8130\n",
      "Epoch 444/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7768 - acc: 0.7924 - val_loss: 0.7529 - val_acc: 0.7960\n",
      "Epoch 445/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7743 - acc: 0.7931 - val_loss: 0.7614 - val_acc: 0.8050\n",
      "Epoch 446/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7742 - acc: 0.7942 - val_loss: 0.7490 - val_acc: 0.8100\n",
      "Epoch 447/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7775 - acc: 0.7940 - val_loss: 0.7465 - val_acc: 0.8000\n",
      "Epoch 448/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7843 - acc: 0.7906 - val_loss: 0.7267 - val_acc: 0.8160\n",
      "Epoch 449/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7732 - acc: 0.7945 - val_loss: 0.7477 - val_acc: 0.8100\n",
      "Epoch 450/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7714 - acc: 0.7953 - val_loss: 0.7367 - val_acc: 0.8160\n",
      "Epoch 451/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7746 - acc: 0.7922 - val_loss: 0.8568 - val_acc: 0.7590\n",
      "Epoch 452/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7752 - acc: 0.7930 - val_loss: 0.7358 - val_acc: 0.8100\n",
      "Epoch 453/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7840 - acc: 0.7884 - val_loss: 0.7390 - val_acc: 0.8120\n",
      "Epoch 454/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7781 - acc: 0.7919 - val_loss: 0.7467 - val_acc: 0.8030\n",
      "Epoch 455/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7769 - acc: 0.7924 - val_loss: 0.7644 - val_acc: 0.7980\n",
      "Epoch 456/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7700 - acc: 0.7953 - val_loss: 0.7672 - val_acc: 0.7890\n",
      "Epoch 457/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7698 - acc: 0.7953 - val_loss: 0.7631 - val_acc: 0.7910\n",
      "Epoch 458/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7796 - acc: 0.7913 - val_loss: 0.7353 - val_acc: 0.7970\n",
      "Epoch 459/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7766 - acc: 0.7917 - val_loss: 0.7392 - val_acc: 0.8230\n",
      "Epoch 460/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7744 - acc: 0.7929 - val_loss: 0.7965 - val_acc: 0.7900\n",
      "Epoch 461/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7781 - acc: 0.7927 - val_loss: 0.7463 - val_acc: 0.7940\n",
      "Epoch 462/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7712 - acc: 0.7951 - val_loss: 0.7958 - val_acc: 0.7800\n",
      "Epoch 463/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7779 - acc: 0.7916 - val_loss: 0.7193 - val_acc: 0.8200\n",
      "Epoch 464/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7747 - acc: 0.7926 - val_loss: 0.7749 - val_acc: 0.7940\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7709 - acc: 0.7952 - val_loss: 0.7651 - val_acc: 0.8060\n",
      "Epoch 466/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7764 - acc: 0.7926 - val_loss: 0.7337 - val_acc: 0.8190\n",
      "Epoch 467/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7708 - acc: 0.7946 - val_loss: 0.7124 - val_acc: 0.8270\n",
      "Epoch 468/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7730 - acc: 0.7931 - val_loss: 0.7359 - val_acc: 0.8070\n",
      "Epoch 469/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7737 - acc: 0.7950 - val_loss: 0.7252 - val_acc: 0.8130\n",
      "Epoch 470/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7681 - acc: 0.7957 - val_loss: 0.7585 - val_acc: 0.8070\n",
      "Epoch 471/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7671 - acc: 0.7964 - val_loss: 0.7275 - val_acc: 0.8070\n",
      "Epoch 472/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7702 - acc: 0.7951 - val_loss: 0.7575 - val_acc: 0.7960\n",
      "Epoch 473/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7746 - acc: 0.7932 - val_loss: 0.7729 - val_acc: 0.7880\n",
      "Epoch 474/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7721 - acc: 0.7922 - val_loss: 0.7232 - val_acc: 0.8070\n",
      "Epoch 475/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7800 - acc: 0.7905 - val_loss: 0.7237 - val_acc: 0.8230\n",
      "Epoch 476/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7731 - acc: 0.7930 - val_loss: 0.7266 - val_acc: 0.8210\n",
      "Epoch 477/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7679 - acc: 0.7944 - val_loss: 0.7185 - val_acc: 0.8230\n",
      "Epoch 478/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7705 - acc: 0.7952 - val_loss: 0.7212 - val_acc: 0.8200\n",
      "Epoch 479/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7767 - acc: 0.7919 - val_loss: 0.7132 - val_acc: 0.8270\n",
      "Epoch 480/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7709 - acc: 0.7946 - val_loss: 0.7343 - val_acc: 0.8030\n",
      "Epoch 481/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7731 - acc: 0.7929 - val_loss: 0.7844 - val_acc: 0.7990\n",
      "Epoch 482/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7785 - acc: 0.7928 - val_loss: 0.7356 - val_acc: 0.7990\n",
      "Epoch 483/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7718 - acc: 0.7923 - val_loss: 0.7421 - val_acc: 0.8070\n",
      "Epoch 484/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7740 - acc: 0.7936 - val_loss: 0.7512 - val_acc: 0.8060\n",
      "Epoch 485/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7694 - acc: 0.7960 - val_loss: 0.7281 - val_acc: 0.8180\n",
      "Epoch 486/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7816 - acc: 0.7902 - val_loss: 0.7337 - val_acc: 0.8150\n",
      "Epoch 487/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7688 - acc: 0.7959 - val_loss: 0.7999 - val_acc: 0.7750\n",
      "Epoch 488/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7703 - acc: 0.7961 - val_loss: 0.8103 - val_acc: 0.7810\n",
      "Epoch 489/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7721 - acc: 0.7948 - val_loss: 0.7575 - val_acc: 0.7850\n",
      "Epoch 490/1000\n",
      "57500/57500 [==============================] - 2s 35us/step - loss: 0.7738 - acc: 0.7920 - val_loss: 0.7187 - val_acc: 0.8110\n",
      "Epoch 491/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7751 - acc: 0.7919 - val_loss: 0.7439 - val_acc: 0.7930\n",
      "Epoch 492/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7755 - acc: 0.7930 - val_loss: 0.7476 - val_acc: 0.8020\n",
      "Epoch 493/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7687 - acc: 0.7946 - val_loss: 0.7650 - val_acc: 0.8030\n",
      "Epoch 494/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7725 - acc: 0.7933 - val_loss: 0.7351 - val_acc: 0.8060\n",
      "Epoch 495/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7725 - acc: 0.7941 - val_loss: 0.7473 - val_acc: 0.8060\n",
      "Epoch 496/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7712 - acc: 0.7952 - val_loss: 0.7608 - val_acc: 0.7930\n",
      "Epoch 497/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7696 - acc: 0.7937 - val_loss: 0.7421 - val_acc: 0.8010\n",
      "Epoch 498/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7708 - acc: 0.7944 - val_loss: 0.7206 - val_acc: 0.8260\n",
      "Epoch 499/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7778 - acc: 0.7921 - val_loss: 0.7652 - val_acc: 0.8050\n",
      "Epoch 500/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7718 - acc: 0.7932 - val_loss: 0.7324 - val_acc: 0.8090\n",
      "Epoch 501/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7735 - acc: 0.7932 - val_loss: 0.7322 - val_acc: 0.8110\n",
      "Epoch 502/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7800 - acc: 0.7911 - val_loss: 0.7169 - val_acc: 0.8200\n",
      "Epoch 503/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7704 - acc: 0.7939 - val_loss: 0.7479 - val_acc: 0.8090\n",
      "Epoch 504/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7718 - acc: 0.7919 - val_loss: 0.7610 - val_acc: 0.7950\n",
      "Epoch 505/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7755 - acc: 0.7943 - val_loss: 0.7285 - val_acc: 0.8180\n",
      "Epoch 506/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7762 - acc: 0.7936 - val_loss: 0.7235 - val_acc: 0.8060\n",
      "Epoch 507/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7718 - acc: 0.7931 - val_loss: 0.7466 - val_acc: 0.8070\n",
      "Epoch 508/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7736 - acc: 0.7933 - val_loss: 0.7213 - val_acc: 0.8120\n",
      "Epoch 509/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7721 - acc: 0.7920 - val_loss: 0.7876 - val_acc: 0.7780\n",
      "Epoch 510/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7820 - acc: 0.7914 - val_loss: 0.7456 - val_acc: 0.8100\n",
      "Epoch 511/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7686 - acc: 0.7953 - val_loss: 0.7349 - val_acc: 0.8190\n",
      "Epoch 512/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7697 - acc: 0.7950 - val_loss: 0.7443 - val_acc: 0.8080\n",
      "Epoch 513/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7712 - acc: 0.7947 - val_loss: 0.7301 - val_acc: 0.8000\n",
      "Epoch 514/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7746 - acc: 0.7924 - val_loss: 0.7621 - val_acc: 0.7900\n",
      "Epoch 515/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7696 - acc: 0.7938 - val_loss: 0.7899 - val_acc: 0.7790\n",
      "Epoch 516/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7726 - acc: 0.7942 - val_loss: 0.7271 - val_acc: 0.8090\n",
      "Epoch 517/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7727 - acc: 0.7930 - val_loss: 0.7203 - val_acc: 0.8190\n",
      "Epoch 518/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7698 - acc: 0.7948 - val_loss: 0.7278 - val_acc: 0.8070\n",
      "Epoch 519/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7764 - acc: 0.7907 - val_loss: 0.7173 - val_acc: 0.8270\n",
      "Epoch 520/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7690 - acc: 0.7943 - val_loss: 0.8029 - val_acc: 0.7790\n",
      "Epoch 521/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7676 - acc: 0.7953 - val_loss: 0.7443 - val_acc: 0.8070\n",
      "Epoch 522/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7735 - acc: 0.7921 - val_loss: 0.7144 - val_acc: 0.8120\n",
      "Epoch 523/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7727 - acc: 0.7929 - val_loss: 0.7548 - val_acc: 0.7910\n",
      "Epoch 524/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7660 - acc: 0.7958 - val_loss: 0.7146 - val_acc: 0.8160\n",
      "Epoch 525/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7699 - acc: 0.7940 - val_loss: 0.7335 - val_acc: 0.8170\n",
      "Epoch 526/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7773 - acc: 0.7930 - val_loss: 0.7269 - val_acc: 0.8220\n",
      "Epoch 527/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7924 - acc: 0.7891 - val_loss: 0.7575 - val_acc: 0.8140\n",
      "Epoch 528/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7745 - acc: 0.7931 - val_loss: 0.7292 - val_acc: 0.8150\n",
      "Epoch 529/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7712 - acc: 0.7948 - val_loss: 0.7227 - val_acc: 0.8150\n",
      "Epoch 530/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7677 - acc: 0.7949 - val_loss: 0.7212 - val_acc: 0.8110\n",
      "Epoch 531/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7719 - acc: 0.7929 - val_loss: 0.8040 - val_acc: 0.7650\n",
      "Epoch 532/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7686 - acc: 0.7958 - val_loss: 0.7287 - val_acc: 0.8050\n",
      "Epoch 533/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7673 - acc: 0.7953 - val_loss: 0.7104 - val_acc: 0.8170\n",
      "Epoch 534/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7676 - acc: 0.7948 - val_loss: 0.7288 - val_acc: 0.8140\n",
      "Epoch 535/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7718 - acc: 0.7921 - val_loss: 0.7185 - val_acc: 0.8170\n",
      "Epoch 536/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7760 - acc: 0.7902 - val_loss: 0.8125 - val_acc: 0.7670\n",
      "Epoch 537/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7659 - acc: 0.7949 - val_loss: 0.7890 - val_acc: 0.7930\n",
      "Epoch 538/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7673 - acc: 0.7951 - val_loss: 0.7312 - val_acc: 0.8090\n",
      "Epoch 539/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7788 - acc: 0.7932 - val_loss: 0.7204 - val_acc: 0.8000\n",
      "Epoch 540/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7738 - acc: 0.7920 - val_loss: 0.7333 - val_acc: 0.8180\n",
      "Epoch 541/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7702 - acc: 0.7953 - val_loss: 0.7215 - val_acc: 0.8140\n",
      "Epoch 542/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7623 - acc: 0.7966 - val_loss: 0.7540 - val_acc: 0.8050\n",
      "Epoch 543/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7760 - acc: 0.7890 - val_loss: 0.7352 - val_acc: 0.7980\n",
      "Epoch 544/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7685 - acc: 0.7952 - val_loss: 0.7343 - val_acc: 0.8050\n",
      "Epoch 545/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7681 - acc: 0.7958 - val_loss: 0.7434 - val_acc: 0.8080\n",
      "Epoch 546/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7706 - acc: 0.7930 - val_loss: 0.7249 - val_acc: 0.8120\n",
      "Epoch 547/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7662 - acc: 0.7945 - val_loss: 0.7132 - val_acc: 0.8170\n",
      "Epoch 548/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7739 - acc: 0.7912 - val_loss: 0.7765 - val_acc: 0.7960\n",
      "Epoch 549/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7740 - acc: 0.7917 - val_loss: 0.7533 - val_acc: 0.7920\n",
      "Epoch 550/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7669 - acc: 0.7945 - val_loss: 0.7339 - val_acc: 0.7940\n",
      "Epoch 551/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7715 - acc: 0.7940 - val_loss: 0.7411 - val_acc: 0.8040\n",
      "Epoch 552/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7659 - acc: 0.7959 - val_loss: 0.7527 - val_acc: 0.7980\n",
      "Epoch 553/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7724 - acc: 0.7923 - val_loss: 0.8265 - val_acc: 0.7560\n",
      "Epoch 554/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7685 - acc: 0.7935 - val_loss: 0.7330 - val_acc: 0.8050\n",
      "Epoch 555/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7727 - acc: 0.7922 - val_loss: 0.7568 - val_acc: 0.8000\n",
      "Epoch 556/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7762 - acc: 0.7919 - val_loss: 0.7406 - val_acc: 0.7980\n",
      "Epoch 557/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7712 - acc: 0.7922 - val_loss: 0.7128 - val_acc: 0.8220\n",
      "Epoch 558/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7721 - acc: 0.7937 - val_loss: 0.7254 - val_acc: 0.8150\n",
      "Epoch 559/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7665 - acc: 0.7947 - val_loss: 0.7158 - val_acc: 0.8200\n",
      "Epoch 560/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7665 - acc: 0.7951 - val_loss: 0.7702 - val_acc: 0.8000\n",
      "Epoch 561/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7691 - acc: 0.7932 - val_loss: 0.7224 - val_acc: 0.8150\n",
      "Epoch 562/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7710 - acc: 0.7929 - val_loss: 0.7244 - val_acc: 0.8040\n",
      "Epoch 563/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7678 - acc: 0.7934 - val_loss: 0.7553 - val_acc: 0.7880\n",
      "Epoch 564/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7739 - acc: 0.7919 - val_loss: 0.7221 - val_acc: 0.8080\n",
      "Epoch 565/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7674 - acc: 0.7945 - val_loss: 0.7452 - val_acc: 0.8070\n",
      "Epoch 566/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7741 - acc: 0.7926 - val_loss: 0.8839 - val_acc: 0.7420\n",
      "Epoch 567/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7715 - acc: 0.7925 - val_loss: 0.8084 - val_acc: 0.7700\n",
      "Epoch 568/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7712 - acc: 0.7929 - val_loss: 0.7960 - val_acc: 0.7900\n",
      "Epoch 569/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7695 - acc: 0.7941 - val_loss: 0.7275 - val_acc: 0.8200\n",
      "Epoch 570/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7701 - acc: 0.7927 - val_loss: 0.7158 - val_acc: 0.8200\n",
      "Epoch 571/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7657 - acc: 0.7942 - val_loss: 0.7651 - val_acc: 0.7900\n",
      "Epoch 572/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7713 - acc: 0.7931 - val_loss: 0.8225 - val_acc: 0.7650\n",
      "Epoch 573/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7688 - acc: 0.7921 - val_loss: 0.7111 - val_acc: 0.8230\n",
      "Epoch 574/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7683 - acc: 0.7941 - val_loss: 0.7427 - val_acc: 0.8050\n",
      "Epoch 575/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7706 - acc: 0.7921 - val_loss: 0.7168 - val_acc: 0.8110\n",
      "Epoch 576/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7720 - acc: 0.7917 - val_loss: 0.7183 - val_acc: 0.8160\n",
      "Epoch 577/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7687 - acc: 0.7936 - val_loss: 0.7122 - val_acc: 0.8170\n",
      "Epoch 578/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7685 - acc: 0.7935 - val_loss: 0.7272 - val_acc: 0.8190\n",
      "Epoch 579/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7691 - acc: 0.7938 - val_loss: 0.7576 - val_acc: 0.8040\n",
      "Epoch 580/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7661 - acc: 0.7954 - val_loss: 0.7302 - val_acc: 0.8150\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7668 - acc: 0.7941 - val_loss: 0.8555 - val_acc: 0.7450\n",
      "Epoch 582/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7643 - acc: 0.7949 - val_loss: 0.7595 - val_acc: 0.8010\n",
      "Epoch 583/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7705 - acc: 0.7929 - val_loss: 0.7846 - val_acc: 0.7950\n",
      "Epoch 584/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7661 - acc: 0.7947 - val_loss: 0.7232 - val_acc: 0.8050\n",
      "Epoch 585/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7723 - acc: 0.7930 - val_loss: 0.7346 - val_acc: 0.8100\n",
      "Epoch 586/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7629 - acc: 0.7949 - val_loss: 0.7560 - val_acc: 0.7930\n",
      "Epoch 587/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7702 - acc: 0.7938 - val_loss: 0.7702 - val_acc: 0.7850\n",
      "Epoch 588/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7695 - acc: 0.7944 - val_loss: 0.7526 - val_acc: 0.8010\n",
      "Epoch 589/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7783 - acc: 0.7885 - val_loss: 0.7847 - val_acc: 0.7890\n",
      "Epoch 590/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7754 - acc: 0.7928 - val_loss: 0.7520 - val_acc: 0.7970\n",
      "Epoch 591/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7705 - acc: 0.7951 - val_loss: 0.7373 - val_acc: 0.7970\n",
      "Epoch 592/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7929 - acc: 0.7869 - val_loss: 0.7113 - val_acc: 0.8240\n",
      "Epoch 593/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7622 - acc: 0.7956 - val_loss: 0.7313 - val_acc: 0.8170\n",
      "Epoch 594/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7718 - acc: 0.7926 - val_loss: 0.7589 - val_acc: 0.7890\n",
      "Epoch 595/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7648 - acc: 0.7956 - val_loss: 0.7455 - val_acc: 0.8040\n",
      "Epoch 596/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7665 - acc: 0.7944 - val_loss: 0.7403 - val_acc: 0.8110\n",
      "Epoch 597/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7666 - acc: 0.7930 - val_loss: 0.7464 - val_acc: 0.8010\n",
      "Epoch 598/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7674 - acc: 0.7936 - val_loss: 0.8049 - val_acc: 0.7660\n",
      "Epoch 599/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7700 - acc: 0.7931 - val_loss: 0.7605 - val_acc: 0.7940\n",
      "Epoch 600/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7660 - acc: 0.7958 - val_loss: 0.7798 - val_acc: 0.7720\n",
      "Epoch 601/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7719 - acc: 0.7920 - val_loss: 0.8072 - val_acc: 0.7780\n",
      "Epoch 602/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7673 - acc: 0.7933 - val_loss: 0.7742 - val_acc: 0.7960\n",
      "Epoch 603/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7723 - acc: 0.7930 - val_loss: 0.7432 - val_acc: 0.8010\n",
      "Epoch 604/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7666 - acc: 0.7931 - val_loss: 0.7717 - val_acc: 0.7960\n",
      "Epoch 605/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7649 - acc: 0.7951 - val_loss: 0.7196 - val_acc: 0.8080\n",
      "Epoch 606/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7749 - acc: 0.7915 - val_loss: 0.7230 - val_acc: 0.8060\n",
      "Epoch 607/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7680 - acc: 0.7941 - val_loss: 0.8378 - val_acc: 0.7560\n",
      "Epoch 608/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7672 - acc: 0.7932 - val_loss: 0.7249 - val_acc: 0.8150\n",
      "Epoch 609/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7699 - acc: 0.7922 - val_loss: 0.7958 - val_acc: 0.7940\n",
      "Epoch 610/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7706 - acc: 0.7924 - val_loss: 0.7185 - val_acc: 0.8040\n",
      "Epoch 611/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7653 - acc: 0.7929 - val_loss: 0.7434 - val_acc: 0.7970\n",
      "Epoch 612/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7631 - acc: 0.7965 - val_loss: 0.7414 - val_acc: 0.8010\n",
      "Epoch 613/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7694 - acc: 0.7938 - val_loss: 0.7237 - val_acc: 0.8150\n",
      "Epoch 614/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7779 - acc: 0.7911 - val_loss: 0.7454 - val_acc: 0.7970\n",
      "Epoch 615/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7597 - acc: 0.7964 - val_loss: 0.7402 - val_acc: 0.7960\n",
      "Epoch 616/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7665 - acc: 0.7945 - val_loss: 0.7415 - val_acc: 0.7960\n",
      "Epoch 617/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7693 - acc: 0.7914 - val_loss: 0.7079 - val_acc: 0.8230\n",
      "Epoch 618/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7651 - acc: 0.7952 - val_loss: 0.7230 - val_acc: 0.8150\n",
      "Epoch 619/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7719 - acc: 0.7913 - val_loss: 0.7700 - val_acc: 0.8000\n",
      "Epoch 620/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7656 - acc: 0.7939 - val_loss: 0.7147 - val_acc: 0.8060\n",
      "Epoch 621/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7721 - acc: 0.7907 - val_loss: 0.7810 - val_acc: 0.7780\n",
      "Epoch 622/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7631 - acc: 0.7967 - val_loss: 0.7356 - val_acc: 0.8140\n",
      "Epoch 623/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7685 - acc: 0.7920 - val_loss: 0.7303 - val_acc: 0.8020\n",
      "Epoch 624/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7704 - acc: 0.7925 - val_loss: 0.7172 - val_acc: 0.8090\n",
      "Epoch 625/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7668 - acc: 0.7952 - val_loss: 0.7200 - val_acc: 0.8130\n",
      "Epoch 626/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7652 - acc: 0.7945 - val_loss: 0.7341 - val_acc: 0.8050\n",
      "Epoch 627/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7678 - acc: 0.7938 - val_loss: 0.7260 - val_acc: 0.8160\n",
      "Epoch 628/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7645 - acc: 0.7951 - val_loss: 0.7334 - val_acc: 0.7980\n",
      "Epoch 629/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7628 - acc: 0.7965 - val_loss: 0.7449 - val_acc: 0.8000\n",
      "Epoch 630/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7668 - acc: 0.7941 - val_loss: 0.7714 - val_acc: 0.7700\n",
      "Epoch 631/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7671 - acc: 0.7934 - val_loss: 0.7226 - val_acc: 0.8120\n",
      "Epoch 632/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7677 - acc: 0.7945 - val_loss: 0.7047 - val_acc: 0.8160\n",
      "Epoch 633/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7631 - acc: 0.7945 - val_loss: 0.7488 - val_acc: 0.7910\n",
      "Epoch 634/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7635 - acc: 0.7935 - val_loss: 0.7062 - val_acc: 0.8200\n",
      "Epoch 635/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7736 - acc: 0.7926 - val_loss: 0.7251 - val_acc: 0.8040\n",
      "Epoch 636/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7689 - acc: 0.7929 - val_loss: 0.7127 - val_acc: 0.8270\n",
      "Epoch 637/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7683 - acc: 0.7924 - val_loss: 0.7161 - val_acc: 0.8160\n",
      "Epoch 638/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7633 - acc: 0.7948 - val_loss: 0.7610 - val_acc: 0.8020\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7685 - acc: 0.7928 - val_loss: 0.7155 - val_acc: 0.8170\n",
      "Epoch 640/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7647 - acc: 0.7952 - val_loss: 0.7151 - val_acc: 0.8090\n",
      "Epoch 641/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7658 - acc: 0.7950 - val_loss: 0.7460 - val_acc: 0.7940\n",
      "Epoch 642/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7689 - acc: 0.7938 - val_loss: 0.7593 - val_acc: 0.8070\n",
      "Epoch 643/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7718 - acc: 0.7913 - val_loss: 0.7387 - val_acc: 0.8160\n",
      "Epoch 644/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7622 - acc: 0.7955 - val_loss: 0.7398 - val_acc: 0.8020\n",
      "Epoch 645/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7654 - acc: 0.7939 - val_loss: 0.7141 - val_acc: 0.8130\n",
      "Epoch 646/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7633 - acc: 0.7942 - val_loss: 0.7190 - val_acc: 0.8150\n",
      "Epoch 647/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7724 - acc: 0.7921 - val_loss: 0.8605 - val_acc: 0.7620\n",
      "Epoch 648/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7763 - acc: 0.7934 - val_loss: 0.7443 - val_acc: 0.8010\n",
      "Epoch 649/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7613 - acc: 0.7962 - val_loss: 0.7796 - val_acc: 0.7940\n",
      "Epoch 650/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7680 - acc: 0.7932 - val_loss: 0.7848 - val_acc: 0.7900\n",
      "Epoch 651/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7650 - acc: 0.7940 - val_loss: 0.7192 - val_acc: 0.8110\n",
      "Epoch 652/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7755 - acc: 0.7906 - val_loss: 0.8056 - val_acc: 0.7750\n",
      "Epoch 653/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7650 - acc: 0.7947 - val_loss: 0.7546 - val_acc: 0.8020\n",
      "Epoch 654/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7710 - acc: 0.7915 - val_loss: 0.8344 - val_acc: 0.7800\n",
      "Epoch 655/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7673 - acc: 0.7939 - val_loss: 0.7419 - val_acc: 0.8080\n",
      "Epoch 656/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7646 - acc: 0.7954 - val_loss: 0.7694 - val_acc: 0.8030\n",
      "Epoch 657/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7646 - acc: 0.7938 - val_loss: 0.7194 - val_acc: 0.8130\n",
      "Epoch 658/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7777 - acc: 0.7898 - val_loss: 0.8006 - val_acc: 0.7820\n",
      "Epoch 659/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7653 - acc: 0.7934 - val_loss: 0.7168 - val_acc: 0.8190\n",
      "Epoch 660/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7646 - acc: 0.7960 - val_loss: 0.7240 - val_acc: 0.8170\n",
      "Epoch 661/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7664 - acc: 0.7922 - val_loss: 0.7409 - val_acc: 0.8130\n",
      "Epoch 662/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7637 - acc: 0.7948 - val_loss: 0.7656 - val_acc: 0.7990\n",
      "Epoch 663/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7696 - acc: 0.7915 - val_loss: 0.7894 - val_acc: 0.7860\n",
      "Epoch 664/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7680 - acc: 0.7933 - val_loss: 0.7173 - val_acc: 0.8060\n",
      "Epoch 665/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7619 - acc: 0.7969 - val_loss: 0.7131 - val_acc: 0.8210\n",
      "Epoch 666/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7609 - acc: 0.7947 - val_loss: 0.7252 - val_acc: 0.8000\n",
      "Epoch 667/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7698 - acc: 0.7919 - val_loss: 0.7216 - val_acc: 0.8130\n",
      "Epoch 668/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7683 - acc: 0.7924 - val_loss: 0.7446 - val_acc: 0.8040\n",
      "Epoch 669/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7767 - acc: 0.7900 - val_loss: 0.8457 - val_acc: 0.7400\n",
      "Epoch 670/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7614 - acc: 0.7959 - val_loss: 0.7089 - val_acc: 0.8180\n",
      "Epoch 671/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7611 - acc: 0.7950 - val_loss: 0.7165 - val_acc: 0.8130\n",
      "Epoch 672/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7707 - acc: 0.7920 - val_loss: 0.8796 - val_acc: 0.7340\n",
      "Epoch 673/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7652 - acc: 0.7937 - val_loss: 0.7334 - val_acc: 0.8120\n",
      "Epoch 674/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7639 - acc: 0.7949 - val_loss: 0.7107 - val_acc: 0.8200\n",
      "Epoch 675/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7669 - acc: 0.7920 - val_loss: 0.7848 - val_acc: 0.7880\n",
      "Epoch 676/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7619 - acc: 0.7958 - val_loss: 0.7166 - val_acc: 0.8110\n",
      "Epoch 677/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7631 - acc: 0.7947 - val_loss: 0.7687 - val_acc: 0.7900\n",
      "Epoch 678/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7665 - acc: 0.7927 - val_loss: 0.7111 - val_acc: 0.8210\n",
      "Epoch 679/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7681 - acc: 0.7927 - val_loss: 0.7251 - val_acc: 0.8050\n",
      "Epoch 680/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7629 - acc: 0.7959 - val_loss: 0.7334 - val_acc: 0.8020\n",
      "Epoch 681/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7638 - acc: 0.7950 - val_loss: 0.7501 - val_acc: 0.8030\n",
      "Epoch 682/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7600 - acc: 0.7961 - val_loss: 0.7717 - val_acc: 0.7850\n",
      "Epoch 683/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7662 - acc: 0.7933 - val_loss: 0.7222 - val_acc: 0.8130\n",
      "Epoch 684/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7645 - acc: 0.7944 - val_loss: 0.7493 - val_acc: 0.7940\n",
      "Epoch 685/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7658 - acc: 0.7942 - val_loss: 0.7184 - val_acc: 0.8200\n",
      "Epoch 686/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7654 - acc: 0.7934 - val_loss: 0.7569 - val_acc: 0.8080\n",
      "Epoch 687/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7724 - acc: 0.7905 - val_loss: 0.7426 - val_acc: 0.8010\n",
      "Epoch 688/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7668 - acc: 0.7925 - val_loss: 0.7519 - val_acc: 0.7980\n",
      "Epoch 689/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7672 - acc: 0.7927 - val_loss: 0.7224 - val_acc: 0.8120\n",
      "Epoch 690/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7651 - acc: 0.7937 - val_loss: 0.7342 - val_acc: 0.8090\n",
      "Epoch 691/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7616 - acc: 0.7942 - val_loss: 0.7381 - val_acc: 0.8060\n",
      "Epoch 692/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7626 - acc: 0.7952 - val_loss: 0.7277 - val_acc: 0.8080\n",
      "Epoch 693/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7648 - acc: 0.7949 - val_loss: 0.7322 - val_acc: 0.8140\n",
      "Epoch 694/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7617 - acc: 0.7970 - val_loss: 0.7110 - val_acc: 0.8220\n",
      "Epoch 695/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7743 - acc: 0.7911 - val_loss: 0.7401 - val_acc: 0.8090\n",
      "Epoch 696/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7623 - acc: 0.7950 - val_loss: 0.7464 - val_acc: 0.8050\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7701 - acc: 0.7905 - val_loss: 0.7219 - val_acc: 0.8100\n",
      "Epoch 698/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7622 - acc: 0.7935 - val_loss: 0.7581 - val_acc: 0.7970\n",
      "Epoch 699/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7697 - acc: 0.7919 - val_loss: 0.7162 - val_acc: 0.8220\n",
      "Epoch 700/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7677 - acc: 0.7936 - val_loss: 0.7209 - val_acc: 0.8140\n",
      "Epoch 701/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7723 - acc: 0.7914 - val_loss: 0.7425 - val_acc: 0.8090\n",
      "Epoch 702/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7585 - acc: 0.7962 - val_loss: 0.7529 - val_acc: 0.7850\n",
      "Epoch 703/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7627 - acc: 0.7950 - val_loss: 0.7830 - val_acc: 0.7790\n",
      "Epoch 704/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7643 - acc: 0.7927 - val_loss: 0.7306 - val_acc: 0.8000\n",
      "Epoch 705/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7625 - acc: 0.7959 - val_loss: 0.7298 - val_acc: 0.8050\n",
      "Epoch 706/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7616 - acc: 0.7953 - val_loss: 0.7800 - val_acc: 0.8000\n",
      "Epoch 707/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7621 - acc: 0.7941 - val_loss: 0.7342 - val_acc: 0.7990\n",
      "Epoch 708/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7670 - acc: 0.7918 - val_loss: 0.8223 - val_acc: 0.7850\n",
      "Epoch 709/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7651 - acc: 0.7937 - val_loss: 0.7276 - val_acc: 0.8150\n",
      "Epoch 710/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7638 - acc: 0.7944 - val_loss: 0.7144 - val_acc: 0.8140\n",
      "Epoch 711/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7675 - acc: 0.7936 - val_loss: 0.7252 - val_acc: 0.8050\n",
      "Epoch 712/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7610 - acc: 0.7951 - val_loss: 0.7477 - val_acc: 0.7960\n",
      "Epoch 713/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7696 - acc: 0.7925 - val_loss: 0.7180 - val_acc: 0.8150\n",
      "Epoch 714/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7676 - acc: 0.7922 - val_loss: 0.7317 - val_acc: 0.8020\n",
      "Epoch 715/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7652 - acc: 0.7930 - val_loss: 0.7137 - val_acc: 0.8170\n",
      "Epoch 716/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7672 - acc: 0.7927 - val_loss: 0.7265 - val_acc: 0.8150\n",
      "Epoch 717/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7602 - acc: 0.7949 - val_loss: 0.7640 - val_acc: 0.7920\n",
      "Epoch 718/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7623 - acc: 0.7939 - val_loss: 0.7253 - val_acc: 0.8130\n",
      "Epoch 719/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7660 - acc: 0.7936 - val_loss: 0.7192 - val_acc: 0.8200\n",
      "Epoch 720/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7638 - acc: 0.7929 - val_loss: 0.7237 - val_acc: 0.8040\n",
      "Epoch 721/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8671 - acc: 0.7790 - val_loss: 0.7265 - val_acc: 0.8100\n",
      "Epoch 722/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7586 - acc: 0.7968 - val_loss: 0.7160 - val_acc: 0.8050\n",
      "Epoch 723/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7645 - acc: 0.7947 - val_loss: 0.7750 - val_acc: 0.7770\n",
      "Epoch 724/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7680 - acc: 0.7928 - val_loss: 0.7091 - val_acc: 0.8230\n",
      "Epoch 725/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7638 - acc: 0.7939 - val_loss: 0.7065 - val_acc: 0.8170\n",
      "Epoch 726/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7627 - acc: 0.7949 - val_loss: 0.7105 - val_acc: 0.8220\n",
      "Epoch 727/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7648 - acc: 0.7926 - val_loss: 0.7229 - val_acc: 0.8060\n",
      "Epoch 728/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7633 - acc: 0.7933 - val_loss: 0.7446 - val_acc: 0.7960\n",
      "Epoch 729/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7605 - acc: 0.7953 - val_loss: 0.7795 - val_acc: 0.7800\n",
      "Epoch 730/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7658 - acc: 0.7922 - val_loss: 0.7354 - val_acc: 0.8120\n",
      "Epoch 731/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7666 - acc: 0.7922 - val_loss: 0.7491 - val_acc: 0.7960\n",
      "Epoch 732/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7665 - acc: 0.7925 - val_loss: 0.7675 - val_acc: 0.7820\n",
      "Epoch 733/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7658 - acc: 0.7928 - val_loss: 0.7178 - val_acc: 0.8080\n",
      "Epoch 734/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7606 - acc: 0.7945 - val_loss: 0.7635 - val_acc: 0.7930\n",
      "Epoch 735/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7646 - acc: 0.7948 - val_loss: 0.7193 - val_acc: 0.8150\n",
      "Epoch 736/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7588 - acc: 0.7943 - val_loss: 0.7520 - val_acc: 0.7990\n",
      "Epoch 737/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7652 - acc: 0.7934 - val_loss: 0.7057 - val_acc: 0.8230\n",
      "Epoch 738/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7631 - acc: 0.7934 - val_loss: 0.7151 - val_acc: 0.8180\n",
      "Epoch 739/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7622 - acc: 0.7936 - val_loss: 0.7482 - val_acc: 0.7810\n",
      "Epoch 740/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7656 - acc: 0.7919 - val_loss: 0.7214 - val_acc: 0.8140\n",
      "Epoch 741/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7635 - acc: 0.7940 - val_loss: 0.8144 - val_acc: 0.7660\n",
      "Epoch 742/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7633 - acc: 0.7949 - val_loss: 0.7078 - val_acc: 0.8230\n",
      "Epoch 743/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7666 - acc: 0.7932 - val_loss: 0.7307 - val_acc: 0.8150\n",
      "Epoch 744/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7675 - acc: 0.7924 - val_loss: 0.7499 - val_acc: 0.8050\n",
      "Epoch 745/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7577 - acc: 0.7964 - val_loss: 0.7164 - val_acc: 0.8110\n",
      "Epoch 746/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7599 - acc: 0.7950 - val_loss: 0.7712 - val_acc: 0.7850\n",
      "Epoch 747/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7618 - acc: 0.7940 - val_loss: 0.7152 - val_acc: 0.8160\n",
      "Epoch 748/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7646 - acc: 0.7939 - val_loss: 0.7695 - val_acc: 0.7940\n",
      "Epoch 749/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7626 - acc: 0.7941 - val_loss: 0.7869 - val_acc: 0.7840\n",
      "Epoch 750/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7632 - acc: 0.7924 - val_loss: 0.7149 - val_acc: 0.8190\n",
      "Epoch 751/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7645 - acc: 0.7930 - val_loss: 0.7145 - val_acc: 0.8080\n",
      "Epoch 752/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7655 - acc: 0.7931 - val_loss: 0.7550 - val_acc: 0.7900\n",
      "Epoch 753/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7631 - acc: 0.7936 - val_loss: 0.7125 - val_acc: 0.8220\n",
      "Epoch 754/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7635 - acc: 0.7938 - val_loss: 0.7335 - val_acc: 0.8070\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.7627 - acc: 0.7951 - val_loss: 0.7221 - val_acc: 0.8060\n",
      "Epoch 756/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7628 - acc: 0.7927 - val_loss: 0.7348 - val_acc: 0.8150\n",
      "Epoch 757/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7627 - acc: 0.7938 - val_loss: 0.7215 - val_acc: 0.8080\n",
      "Epoch 758/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7647 - acc: 0.7934 - val_loss: 0.7986 - val_acc: 0.7750\n",
      "Epoch 759/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7646 - acc: 0.7933 - val_loss: 0.7485 - val_acc: 0.7930\n",
      "Epoch 760/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7582 - acc: 0.7937 - val_loss: 0.7015 - val_acc: 0.8280\n",
      "Epoch 761/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7640 - acc: 0.7938 - val_loss: 0.8225 - val_acc: 0.7790\n",
      "Epoch 762/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7564 - acc: 0.7962 - val_loss: 0.7435 - val_acc: 0.7970\n",
      "Epoch 763/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7629 - acc: 0.7951 - val_loss: 0.7411 - val_acc: 0.8090\n",
      "Epoch 764/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7670 - acc: 0.7898 - val_loss: 0.7241 - val_acc: 0.8130\n",
      "Epoch 765/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7634 - acc: 0.7927 - val_loss: 0.7502 - val_acc: 0.8050\n",
      "Epoch 766/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7690 - acc: 0.7909 - val_loss: 0.7357 - val_acc: 0.8130\n",
      "Epoch 767/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7674 - acc: 0.7924 - val_loss: 0.7233 - val_acc: 0.8110\n",
      "Epoch 768/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7629 - acc: 0.7926 - val_loss: 0.7114 - val_acc: 0.8150\n",
      "Epoch 769/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7612 - acc: 0.7935 - val_loss: 0.7773 - val_acc: 0.8000\n",
      "Epoch 770/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7604 - acc: 0.7946 - val_loss: 0.7112 - val_acc: 0.8070\n",
      "Epoch 771/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7567 - acc: 0.7962 - val_loss: 0.8171 - val_acc: 0.7730\n",
      "Epoch 772/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7677 - acc: 0.7912 - val_loss: 0.7447 - val_acc: 0.8090\n",
      "Epoch 773/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7593 - acc: 0.7950 - val_loss: 0.7523 - val_acc: 0.8080\n",
      "Epoch 774/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7637 - acc: 0.7935 - val_loss: 0.7213 - val_acc: 0.8080\n",
      "Epoch 775/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7598 - acc: 0.7953 - val_loss: 0.7325 - val_acc: 0.8030\n",
      "Epoch 776/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7609 - acc: 0.7954 - val_loss: 0.7438 - val_acc: 0.8060\n",
      "Epoch 777/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7629 - acc: 0.7937 - val_loss: 0.7126 - val_acc: 0.8100\n",
      "Epoch 778/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7622 - acc: 0.7939 - val_loss: 0.7122 - val_acc: 0.8220\n",
      "Epoch 779/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.8364 - acc: 0.7833 - val_loss: 0.7064 - val_acc: 0.8270\n",
      "Epoch 780/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7666 - acc: 0.7928 - val_loss: 0.8668 - val_acc: 0.7390\n",
      "Epoch 781/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7645 - acc: 0.7922 - val_loss: 0.7139 - val_acc: 0.8090\n",
      "Epoch 782/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7549 - acc: 0.7967 - val_loss: 0.7221 - val_acc: 0.8010\n",
      "Epoch 783/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7639 - acc: 0.7922 - val_loss: 0.7234 - val_acc: 0.8030\n",
      "Epoch 784/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7636 - acc: 0.7942 - val_loss: 0.7108 - val_acc: 0.8060\n",
      "Epoch 785/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7636 - acc: 0.7947 - val_loss: 0.8024 - val_acc: 0.7770\n",
      "Epoch 786/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7684 - acc: 0.7918 - val_loss: 0.7099 - val_acc: 0.8100\n",
      "Epoch 787/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7596 - acc: 0.7943 - val_loss: 0.7140 - val_acc: 0.8180\n",
      "Epoch 788/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7602 - acc: 0.7935 - val_loss: 0.7179 - val_acc: 0.8020\n",
      "Epoch 789/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7656 - acc: 0.7926 - val_loss: 0.7085 - val_acc: 0.8150\n",
      "Epoch 790/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7608 - acc: 0.7938 - val_loss: 0.7350 - val_acc: 0.8100\n",
      "Epoch 791/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7625 - acc: 0.7931 - val_loss: 0.8006 - val_acc: 0.7700\n",
      "Epoch 792/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7610 - acc: 0.7934 - val_loss: 0.7207 - val_acc: 0.8010\n",
      "Epoch 793/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7589 - acc: 0.7956 - val_loss: 0.6988 - val_acc: 0.8160\n",
      "Epoch 794/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7591 - acc: 0.7951 - val_loss: 0.7233 - val_acc: 0.8030\n",
      "Epoch 795/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7635 - acc: 0.7918 - val_loss: 0.7426 - val_acc: 0.7920\n",
      "Epoch 796/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7656 - acc: 0.7917 - val_loss: 0.7674 - val_acc: 0.7930\n",
      "Epoch 797/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7692 - acc: 0.7919 - val_loss: 0.7242 - val_acc: 0.8050\n",
      "Epoch 798/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7613 - acc: 0.7953 - val_loss: 0.7258 - val_acc: 0.8030\n",
      "Epoch 799/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7650 - acc: 0.7933 - val_loss: 0.7743 - val_acc: 0.7950\n",
      "Epoch 800/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7546 - acc: 0.7963 - val_loss: 0.7236 - val_acc: 0.7990\n",
      "Epoch 801/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7622 - acc: 0.7932 - val_loss: 0.7381 - val_acc: 0.7940\n",
      "Epoch 802/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7633 - acc: 0.7933 - val_loss: 0.7585 - val_acc: 0.8050\n",
      "Epoch 803/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7600 - acc: 0.7950 - val_loss: 0.7737 - val_acc: 0.7720\n",
      "Epoch 804/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7689 - acc: 0.7888 - val_loss: 0.7167 - val_acc: 0.8080\n",
      "Epoch 805/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7587 - acc: 0.7954 - val_loss: 0.8009 - val_acc: 0.7910\n",
      "Epoch 806/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7662 - acc: 0.7922 - val_loss: 0.7202 - val_acc: 0.7980\n",
      "Epoch 807/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7725 - acc: 0.7899 - val_loss: 0.7687 - val_acc: 0.7970\n",
      "Epoch 808/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7642 - acc: 0.7930 - val_loss: 0.7295 - val_acc: 0.7980\n",
      "Epoch 809/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7579 - acc: 0.7964 - val_loss: 0.7521 - val_acc: 0.7740\n",
      "Epoch 810/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7612 - acc: 0.7939 - val_loss: 0.7279 - val_acc: 0.8150\n",
      "Epoch 811/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7606 - acc: 0.7953 - val_loss: 0.7248 - val_acc: 0.8200\n",
      "Epoch 812/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7640 - acc: 0.7933 - val_loss: 0.7360 - val_acc: 0.8050\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7588 - acc: 0.7945 - val_loss: 0.7343 - val_acc: 0.8000\n",
      "Epoch 814/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7648 - acc: 0.7926 - val_loss: 0.7111 - val_acc: 0.8040\n",
      "Epoch 815/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7632 - acc: 0.7932 - val_loss: 0.7655 - val_acc: 0.7880\n",
      "Epoch 816/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7620 - acc: 0.7931 - val_loss: 0.7083 - val_acc: 0.8070\n",
      "Epoch 817/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7710 - acc: 0.7913 - val_loss: 0.7284 - val_acc: 0.8010\n",
      "Epoch 818/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7668 - acc: 0.7918 - val_loss: 0.7547 - val_acc: 0.8020\n",
      "Epoch 819/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7567 - acc: 0.7947 - val_loss: 0.7576 - val_acc: 0.7930\n",
      "Epoch 820/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7610 - acc: 0.7949 - val_loss: 0.7605 - val_acc: 0.7860\n",
      "Epoch 821/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7636 - acc: 0.7934 - val_loss: 0.7789 - val_acc: 0.7910\n",
      "Epoch 822/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7576 - acc: 0.7966 - val_loss: 0.7212 - val_acc: 0.8010\n",
      "Epoch 823/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7661 - acc: 0.7927 - val_loss: 0.7298 - val_acc: 0.7960\n",
      "Epoch 824/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7592 - acc: 0.7947 - val_loss: 0.7194 - val_acc: 0.8220\n",
      "Epoch 825/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7612 - acc: 0.7949 - val_loss: 0.7061 - val_acc: 0.8180\n",
      "Epoch 826/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7641 - acc: 0.7916 - val_loss: 0.7154 - val_acc: 0.8140\n",
      "Epoch 827/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7553 - acc: 0.7951 - val_loss: 0.7589 - val_acc: 0.7890\n",
      "Epoch 828/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7652 - acc: 0.7923 - val_loss: 0.7213 - val_acc: 0.8110\n",
      "Epoch 829/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7612 - acc: 0.7955 - val_loss: 0.7537 - val_acc: 0.7980\n",
      "Epoch 830/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7566 - acc: 0.7967 - val_loss: 0.7346 - val_acc: 0.8050\n",
      "Epoch 831/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7580 - acc: 0.7961 - val_loss: 0.7255 - val_acc: 0.8170\n",
      "Epoch 832/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7717 - acc: 0.7893 - val_loss: 0.7259 - val_acc: 0.8090\n",
      "Epoch 833/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7634 - acc: 0.7923 - val_loss: 0.7415 - val_acc: 0.8010\n",
      "Epoch 834/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7712 - acc: 0.7909 - val_loss: 0.7200 - val_acc: 0.8110\n",
      "Epoch 835/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7637 - acc: 0.7935 - val_loss: 0.7747 - val_acc: 0.7780\n",
      "Epoch 836/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7595 - acc: 0.7943 - val_loss: 0.7200 - val_acc: 0.8090\n",
      "Epoch 837/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7610 - acc: 0.7951 - val_loss: 0.7087 - val_acc: 0.8130\n",
      "Epoch 838/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7640 - acc: 0.7935 - val_loss: 0.7174 - val_acc: 0.8050\n",
      "Epoch 839/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7603 - acc: 0.7947 - val_loss: 0.7085 - val_acc: 0.8250\n",
      "Epoch 840/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7576 - acc: 0.7961 - val_loss: 0.7560 - val_acc: 0.7780\n",
      "Epoch 841/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7728 - acc: 0.7922 - val_loss: 0.7625 - val_acc: 0.7860\n",
      "Epoch 842/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7540 - acc: 0.7966 - val_loss: 0.7682 - val_acc: 0.7820\n",
      "Epoch 843/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7642 - acc: 0.7927 - val_loss: 0.7253 - val_acc: 0.8040\n",
      "Epoch 844/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7550 - acc: 0.7964 - val_loss: 0.7105 - val_acc: 0.8150\n",
      "Epoch 845/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7667 - acc: 0.7923 - val_loss: 0.7271 - val_acc: 0.7940\n",
      "Epoch 846/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7603 - acc: 0.7924 - val_loss: 0.7343 - val_acc: 0.8070\n",
      "Epoch 847/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7686 - acc: 0.7913 - val_loss: 0.7785 - val_acc: 0.7810\n",
      "Epoch 848/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7628 - acc: 0.7942 - val_loss: 0.7235 - val_acc: 0.8110\n",
      "Epoch 849/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7654 - acc: 0.7932 - val_loss: 0.7063 - val_acc: 0.8130\n",
      "Epoch 850/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7590 - acc: 0.7948 - val_loss: 0.7314 - val_acc: 0.8000\n",
      "Epoch 851/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7637 - acc: 0.7928 - val_loss: 0.7342 - val_acc: 0.8020\n",
      "Epoch 852/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7628 - acc: 0.7914 - val_loss: 0.7214 - val_acc: 0.8140\n",
      "Epoch 853/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.8972 - acc: 0.7742 - val_loss: 0.7121 - val_acc: 0.8190\n",
      "Epoch 854/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7565 - acc: 0.7958 - val_loss: 0.7499 - val_acc: 0.7940\n",
      "Epoch 855/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7560 - acc: 0.7959 - val_loss: 0.7145 - val_acc: 0.8030\n",
      "Epoch 856/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7586 - acc: 0.7937 - val_loss: 0.7288 - val_acc: 0.8120\n",
      "Epoch 857/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7574 - acc: 0.7932 - val_loss: 0.7923 - val_acc: 0.7810\n",
      "Epoch 858/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7617 - acc: 0.7924 - val_loss: 0.7282 - val_acc: 0.8130\n",
      "Epoch 859/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7571 - acc: 0.7959 - val_loss: 0.7086 - val_acc: 0.8190\n",
      "Epoch 860/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7621 - acc: 0.7947 - val_loss: 0.7118 - val_acc: 0.8210\n",
      "Epoch 861/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7602 - acc: 0.7925 - val_loss: 0.7078 - val_acc: 0.8140\n",
      "Epoch 862/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7579 - acc: 0.7942 - val_loss: 0.7488 - val_acc: 0.7980\n",
      "Epoch 863/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7634 - acc: 0.7913 - val_loss: 0.7706 - val_acc: 0.7910\n",
      "Epoch 864/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7560 - acc: 0.7963 - val_loss: 0.7081 - val_acc: 0.8210\n",
      "Epoch 865/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7588 - acc: 0.7949 - val_loss: 0.7484 - val_acc: 0.7830\n",
      "Epoch 866/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7611 - acc: 0.7939 - val_loss: 0.7715 - val_acc: 0.7920\n",
      "Epoch 867/1000\n",
      "57500/57500 [==============================] - 2s 36us/step - loss: 0.7590 - acc: 0.7950 - val_loss: 0.7519 - val_acc: 0.7940\n",
      "Epoch 868/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7598 - acc: 0.7927 - val_loss: 0.7405 - val_acc: 0.7980\n",
      "Epoch 869/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7638 - acc: 0.7926 - val_loss: 0.7240 - val_acc: 0.8060\n",
      "Epoch 870/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7580 - acc: 0.7942 - val_loss: 0.8248 - val_acc: 0.7510\n",
      "Epoch 871/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7596 - acc: 0.7934 - val_loss: 0.7351 - val_acc: 0.7960\n",
      "Epoch 872/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7622 - acc: 0.7927 - val_loss: 0.7351 - val_acc: 0.8140\n",
      "Epoch 873/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7601 - acc: 0.7943 - val_loss: 0.7016 - val_acc: 0.8150\n",
      "Epoch 874/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7612 - acc: 0.7929 - val_loss: 0.7765 - val_acc: 0.7870\n",
      "Epoch 875/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7703 - acc: 0.7906 - val_loss: 0.7514 - val_acc: 0.7890\n",
      "Epoch 876/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7642 - acc: 0.7930 - val_loss: 0.7214 - val_acc: 0.8130\n",
      "Epoch 877/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7620 - acc: 0.7940 - val_loss: 0.7521 - val_acc: 0.7880\n",
      "Epoch 878/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7609 - acc: 0.7931 - val_loss: 0.7146 - val_acc: 0.8150\n",
      "Epoch 879/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7639 - acc: 0.7927 - val_loss: 0.7317 - val_acc: 0.8070\n",
      "Epoch 880/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7603 - acc: 0.7944 - val_loss: 0.7053 - val_acc: 0.8200\n",
      "Epoch 881/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7581 - acc: 0.7935 - val_loss: 0.7030 - val_acc: 0.8170\n",
      "Epoch 882/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7646 - acc: 0.7914 - val_loss: 0.7241 - val_acc: 0.8150\n",
      "Epoch 883/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7614 - acc: 0.7919 - val_loss: 0.7221 - val_acc: 0.8140\n",
      "Epoch 884/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7599 - acc: 0.7937 - val_loss: 0.7252 - val_acc: 0.8100\n",
      "Epoch 885/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7597 - acc: 0.7929 - val_loss: 0.7270 - val_acc: 0.8090\n",
      "Epoch 886/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7686 - acc: 0.7912 - val_loss: 0.7151 - val_acc: 0.8140\n",
      "Epoch 887/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7616 - acc: 0.7921 - val_loss: 0.7289 - val_acc: 0.7960\n",
      "Epoch 888/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7598 - acc: 0.7935 - val_loss: 0.8781 - val_acc: 0.7410\n",
      "Epoch 889/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7592 - acc: 0.7934 - val_loss: 0.8491 - val_acc: 0.7590\n",
      "Epoch 890/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7573 - acc: 0.7953 - val_loss: 0.7880 - val_acc: 0.7810\n",
      "Epoch 891/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7589 - acc: 0.7938 - val_loss: 0.7698 - val_acc: 0.8010\n",
      "Epoch 892/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7613 - acc: 0.7930 - val_loss: 0.7805 - val_acc: 0.7830\n",
      "Epoch 893/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7636 - acc: 0.7924 - val_loss: 0.7474 - val_acc: 0.7960\n",
      "Epoch 894/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7608 - acc: 0.7933 - val_loss: 0.8159 - val_acc: 0.7780\n",
      "Epoch 895/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7644 - acc: 0.7919 - val_loss: 0.7439 - val_acc: 0.7890\n",
      "Epoch 896/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7603 - acc: 0.7936 - val_loss: 0.7121 - val_acc: 0.8070\n",
      "Epoch 897/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7596 - acc: 0.7953 - val_loss: 0.7566 - val_acc: 0.8090\n",
      "Epoch 898/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7566 - acc: 0.7929 - val_loss: 0.7051 - val_acc: 0.8180\n",
      "Epoch 899/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7542 - acc: 0.7954 - val_loss: 0.7367 - val_acc: 0.8060\n",
      "Epoch 900/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7586 - acc: 0.7938 - val_loss: 0.7399 - val_acc: 0.8120\n",
      "Epoch 901/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7595 - acc: 0.7943 - val_loss: 0.7196 - val_acc: 0.8020\n",
      "Epoch 902/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7599 - acc: 0.7935 - val_loss: 0.7029 - val_acc: 0.8160\n",
      "Epoch 903/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7681 - acc: 0.7908 - val_loss: 0.7782 - val_acc: 0.7780\n",
      "Epoch 904/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7623 - acc: 0.7926 - val_loss: 0.7453 - val_acc: 0.7850\n",
      "Epoch 905/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7634 - acc: 0.7915 - val_loss: 0.7296 - val_acc: 0.8070\n",
      "Epoch 906/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7583 - acc: 0.7955 - val_loss: 0.7104 - val_acc: 0.8090\n",
      "Epoch 907/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7644 - acc: 0.7935 - val_loss: 0.7410 - val_acc: 0.8050\n",
      "Epoch 908/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7645 - acc: 0.7932 - val_loss: 0.7257 - val_acc: 0.8000\n",
      "Epoch 909/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7585 - acc: 0.7946 - val_loss: 0.7743 - val_acc: 0.7830\n",
      "Epoch 910/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7588 - acc: 0.7940 - val_loss: 0.7955 - val_acc: 0.7740\n",
      "Epoch 911/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7572 - acc: 0.7954 - val_loss: 0.7094 - val_acc: 0.8120\n",
      "Epoch 912/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7582 - acc: 0.7946 - val_loss: 0.7390 - val_acc: 0.7940\n",
      "Epoch 913/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7634 - acc: 0.7929 - val_loss: 0.7542 - val_acc: 0.7870\n",
      "Epoch 914/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7679 - acc: 0.7910 - val_loss: 0.7206 - val_acc: 0.8160\n",
      "Epoch 915/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7647 - acc: 0.7935 - val_loss: 0.7454 - val_acc: 0.7800\n",
      "Epoch 916/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7587 - acc: 0.7946 - val_loss: 0.7121 - val_acc: 0.8060\n",
      "Epoch 917/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7603 - acc: 0.7929 - val_loss: 0.7664 - val_acc: 0.7910\n",
      "Epoch 918/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7546 - acc: 0.7947 - val_loss: 0.7039 - val_acc: 0.8190\n",
      "Epoch 919/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7624 - acc: 0.7927 - val_loss: 0.7179 - val_acc: 0.8080\n",
      "Epoch 920/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7626 - acc: 0.7933 - val_loss: 0.7744 - val_acc: 0.7940\n",
      "Epoch 921/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7628 - acc: 0.7932 - val_loss: 0.7118 - val_acc: 0.8080\n",
      "Epoch 922/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7603 - acc: 0.7933 - val_loss: 0.7710 - val_acc: 0.7970\n",
      "Epoch 923/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7560 - acc: 0.7957 - val_loss: 0.8106 - val_acc: 0.7660\n",
      "Epoch 924/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7623 - acc: 0.7928 - val_loss: 0.7338 - val_acc: 0.8090\n",
      "Epoch 925/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7646 - acc: 0.7917 - val_loss: 0.7290 - val_acc: 0.8070\n",
      "Epoch 926/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7626 - acc: 0.7913 - val_loss: 0.7241 - val_acc: 0.8110\n",
      "Epoch 927/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7554 - acc: 0.7951 - val_loss: 0.7127 - val_acc: 0.8110\n",
      "Epoch 928/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7586 - acc: 0.7938 - val_loss: 0.7043 - val_acc: 0.8180\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7653 - acc: 0.7912 - val_loss: 0.7143 - val_acc: 0.8200\n",
      "Epoch 930/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7578 - acc: 0.7944 - val_loss: 0.7157 - val_acc: 0.8100\n",
      "Epoch 931/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7599 - acc: 0.7938 - val_loss: 0.7147 - val_acc: 0.8090\n",
      "Epoch 932/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7702 - acc: 0.7905 - val_loss: 0.7529 - val_acc: 0.8000\n",
      "Epoch 933/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7642 - acc: 0.7925 - val_loss: 0.7647 - val_acc: 0.7940\n",
      "Epoch 934/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7642 - acc: 0.7920 - val_loss: 0.8067 - val_acc: 0.7760\n",
      "Epoch 935/1000\n",
      "57500/57500 [==============================] - 2s 28us/step - loss: 0.7610 - acc: 0.7941 - val_loss: 0.7321 - val_acc: 0.8000\n",
      "Epoch 936/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7581 - acc: 0.7925 - val_loss: 0.7926 - val_acc: 0.7860\n",
      "Epoch 937/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7610 - acc: 0.7931 - val_loss: 0.7928 - val_acc: 0.7800\n",
      "Epoch 938/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7705 - acc: 0.7911 - val_loss: 0.7224 - val_acc: 0.8050\n",
      "Epoch 939/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7597 - acc: 0.7932 - val_loss: 0.7590 - val_acc: 0.7890\n",
      "Epoch 940/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7612 - acc: 0.7925 - val_loss: 0.7191 - val_acc: 0.8120\n",
      "Epoch 941/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7554 - acc: 0.7937 - val_loss: 0.7188 - val_acc: 0.8150\n",
      "Epoch 942/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7549 - acc: 0.7956 - val_loss: 0.7069 - val_acc: 0.8100\n",
      "Epoch 943/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7615 - acc: 0.7938 - val_loss: 0.7483 - val_acc: 0.7990\n",
      "Epoch 944/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7632 - acc: 0.7913 - val_loss: 0.7228 - val_acc: 0.8130\n",
      "Epoch 945/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7595 - acc: 0.7934 - val_loss: 0.7328 - val_acc: 0.7990\n",
      "Epoch 946/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7587 - acc: 0.7939 - val_loss: 0.7455 - val_acc: 0.8020\n",
      "Epoch 947/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7638 - acc: 0.7918 - val_loss: 0.7564 - val_acc: 0.8080\n",
      "Epoch 948/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7587 - acc: 0.7939 - val_loss: 0.7512 - val_acc: 0.7850\n",
      "Epoch 949/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7656 - acc: 0.7905 - val_loss: 0.8396 - val_acc: 0.7780\n",
      "Epoch 950/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7586 - acc: 0.7938 - val_loss: 0.7680 - val_acc: 0.7810\n",
      "Epoch 951/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7554 - acc: 0.7966 - val_loss: 0.7274 - val_acc: 0.8090\n",
      "Epoch 952/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7509 - acc: 0.7970 - val_loss: 0.7128 - val_acc: 0.8100\n",
      "Epoch 953/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7616 - acc: 0.7938 - val_loss: 0.7270 - val_acc: 0.8080\n",
      "Epoch 954/1000\n",
      "57500/57500 [==============================] - 2s 34us/step - loss: 0.7576 - acc: 0.7944 - val_loss: 0.7885 - val_acc: 0.7700\n",
      "Epoch 955/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7619 - acc: 0.7927 - val_loss: 0.7200 - val_acc: 0.8010\n",
      "Epoch 956/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7652 - acc: 0.7916 - val_loss: 0.7313 - val_acc: 0.7900\n",
      "Epoch 957/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7631 - acc: 0.7928 - val_loss: 0.7103 - val_acc: 0.8180\n",
      "Epoch 958/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7571 - acc: 0.7937 - val_loss: 0.7119 - val_acc: 0.8140\n",
      "Epoch 959/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7583 - acc: 0.7954 - val_loss: 0.7330 - val_acc: 0.8100\n",
      "Epoch 960/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7627 - acc: 0.7914 - val_loss: 0.7104 - val_acc: 0.8090\n",
      "Epoch 961/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7626 - acc: 0.7913 - val_loss: 0.8070 - val_acc: 0.7760\n",
      "Epoch 962/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7622 - acc: 0.7913 - val_loss: 0.7246 - val_acc: 0.7990\n",
      "Epoch 963/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7587 - acc: 0.7931 - val_loss: 0.7161 - val_acc: 0.8120\n",
      "Epoch 964/1000\n",
      "57500/57500 [==============================] - 2s 36us/step - loss: 0.7678 - acc: 0.7904 - val_loss: 0.8298 - val_acc: 0.7700\n",
      "Epoch 965/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7620 - acc: 0.7923 - val_loss: 0.7387 - val_acc: 0.7970\n",
      "Epoch 966/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7617 - acc: 0.7931 - val_loss: 0.7264 - val_acc: 0.8070\n",
      "Epoch 967/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7592 - acc: 0.7937 - val_loss: 0.7060 - val_acc: 0.8150\n",
      "Epoch 968/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7690 - acc: 0.7909 - val_loss: 0.7698 - val_acc: 0.7860\n",
      "Epoch 969/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7609 - acc: 0.7942 - val_loss: 0.7097 - val_acc: 0.8150\n",
      "Epoch 970/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7641 - acc: 0.7930 - val_loss: 0.7568 - val_acc: 0.7940\n",
      "Epoch 971/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7587 - acc: 0.7927 - val_loss: 0.7270 - val_acc: 0.8090\n",
      "Epoch 972/1000\n",
      "57500/57500 [==============================] - 2s 32us/step - loss: 0.7679 - acc: 0.7901 - val_loss: 0.7208 - val_acc: 0.8110\n",
      "Epoch 973/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7615 - acc: 0.7919 - val_loss: 0.7141 - val_acc: 0.8130\n",
      "Epoch 974/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7614 - acc: 0.7933 - val_loss: 0.7884 - val_acc: 0.7890\n",
      "Epoch 975/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7634 - acc: 0.7913 - val_loss: 0.7367 - val_acc: 0.8020\n",
      "Epoch 976/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7606 - acc: 0.7932 - val_loss: 0.7243 - val_acc: 0.8140\n",
      "Epoch 977/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7588 - acc: 0.7948 - val_loss: 0.7312 - val_acc: 0.8040\n",
      "Epoch 978/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7547 - acc: 0.7953 - val_loss: 0.7450 - val_acc: 0.8050\n",
      "Epoch 979/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7578 - acc: 0.7954 - val_loss: 0.7720 - val_acc: 0.7830\n",
      "Epoch 980/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7619 - acc: 0.7927 - val_loss: 0.7506 - val_acc: 0.7970\n",
      "Epoch 981/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7621 - acc: 0.7920 - val_loss: 0.7353 - val_acc: 0.8010\n",
      "Epoch 982/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7542 - acc: 0.7968 - val_loss: 0.7046 - val_acc: 0.8100\n",
      "Epoch 983/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7637 - acc: 0.7949 - val_loss: 0.8017 - val_acc: 0.7790\n",
      "Epoch 984/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7586 - acc: 0.7947 - val_loss: 0.7623 - val_acc: 0.8000\n",
      "Epoch 985/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7699 - acc: 0.7906 - val_loss: 0.7869 - val_acc: 0.7820\n",
      "Epoch 986/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7585 - acc: 0.7939 - val_loss: 0.7816 - val_acc: 0.7690\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7570 - acc: 0.7935 - val_loss: 0.7718 - val_acc: 0.8030\n",
      "Epoch 988/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7606 - acc: 0.7935 - val_loss: 0.7705 - val_acc: 0.7860\n",
      "Epoch 989/1000\n",
      "57500/57500 [==============================] - 2s 33us/step - loss: 0.7599 - acc: 0.7927 - val_loss: 0.7121 - val_acc: 0.8100\n",
      "Epoch 990/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7610 - acc: 0.7934 - val_loss: 0.7157 - val_acc: 0.8110\n",
      "Epoch 991/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7577 - acc: 0.7949 - val_loss: 0.7325 - val_acc: 0.8080\n",
      "Epoch 992/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7570 - acc: 0.7944 - val_loss: 0.7653 - val_acc: 0.7870\n",
      "Epoch 993/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7629 - acc: 0.7914 - val_loss: 0.7307 - val_acc: 0.7960\n",
      "Epoch 994/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7633 - acc: 0.7939 - val_loss: 0.7162 - val_acc: 0.8150\n",
      "Epoch 995/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7592 - acc: 0.7942 - val_loss: 0.7100 - val_acc: 0.8100\n",
      "Epoch 996/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7602 - acc: 0.7918 - val_loss: 0.7905 - val_acc: 0.7790\n",
      "Epoch 997/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7574 - acc: 0.7949 - val_loss: 0.7544 - val_acc: 0.8010\n",
      "Epoch 998/1000\n",
      "57500/57500 [==============================] - 2s 29us/step - loss: 0.7600 - acc: 0.7935 - val_loss: 0.7736 - val_acc: 0.7830\n",
      "Epoch 999/1000\n",
      "57500/57500 [==============================] - 2s 30us/step - loss: 0.7609 - acc: 0.7935 - val_loss: 0.7701 - val_acc: 0.7820\n",
      "Epoch 1000/1000\n",
      "57500/57500 [==============================] - 2s 31us/step - loss: 0.7614 - acc: 0.7942 - val_loss: 0.7253 - val_acc: 0.8170\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 3s 48us/step\n",
      "1500/1500 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7505290083968121, 0.8008695652049521]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7843376790682475, 0.7753333338101704]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "57500/57500 [==============================] - 3s 56us/step - loss: 1.9224 - acc: 0.1966 - val_loss: 1.8443 - val_acc: 0.2840\n",
      "Epoch 2/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.8207 - acc: 0.2740 - val_loss: 1.6860 - val_acc: 0.4280\n",
      "Epoch 3/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.6919 - acc: 0.3455 - val_loss: 1.4884 - val_acc: 0.5410\n",
      "Epoch 4/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.5549 - acc: 0.4114 - val_loss: 1.3009 - val_acc: 0.6290\n",
      "Epoch 5/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 1.4363 - acc: 0.4603 - val_loss: 1.1517 - val_acc: 0.6840\n",
      "Epoch 6/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 1.3349 - acc: 0.4962 - val_loss: 1.0379 - val_acc: 0.7060\n",
      "Epoch 7/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.2620 - acc: 0.5293 - val_loss: 0.9570 - val_acc: 0.7240\n",
      "Epoch 8/200\n",
      "57500/57500 [==============================] - 3s 49us/step - loss: 1.1970 - acc: 0.5506 - val_loss: 0.8897 - val_acc: 0.7390\n",
      "Epoch 9/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.1448 - acc: 0.5732 - val_loss: 0.8399 - val_acc: 0.7380\n",
      "Epoch 10/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 1.1114 - acc: 0.5858 - val_loss: 0.8015 - val_acc: 0.7490\n",
      "Epoch 11/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.0726 - acc: 0.5993 - val_loss: 0.7678 - val_acc: 0.7510\n",
      "Epoch 12/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 1.0410 - acc: 0.6113 - val_loss: 0.7440 - val_acc: 0.7560\n",
      "Epoch 13/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 1.0208 - acc: 0.6229 - val_loss: 0.7209 - val_acc: 0.7640\n",
      "Epoch 14/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.9938 - acc: 0.6305 - val_loss: 0.7007 - val_acc: 0.7660\n",
      "Epoch 15/200\n",
      "57500/57500 [==============================] - 3s 56us/step - loss: 0.9780 - acc: 0.6378 - val_loss: 0.6868 - val_acc: 0.7690\n",
      "Epoch 16/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.9601 - acc: 0.6451 - val_loss: 0.6685 - val_acc: 0.7730\n",
      "Epoch 17/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.9487 - acc: 0.6490 - val_loss: 0.6589 - val_acc: 0.7760\n",
      "Epoch 18/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.9299 - acc: 0.6569 - val_loss: 0.6482 - val_acc: 0.7750\n",
      "Epoch 19/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.9174 - acc: 0.6620 - val_loss: 0.6369 - val_acc: 0.7790\n",
      "Epoch 20/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.9050 - acc: 0.6645 - val_loss: 0.6269 - val_acc: 0.7830\n",
      "Epoch 21/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8947 - acc: 0.6703 - val_loss: 0.6202 - val_acc: 0.7800\n",
      "Epoch 22/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8845 - acc: 0.6742 - val_loss: 0.6113 - val_acc: 0.7900\n",
      "Epoch 23/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.8769 - acc: 0.6777 - val_loss: 0.6028 - val_acc: 0.7880\n",
      "Epoch 24/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.8732 - acc: 0.6785 - val_loss: 0.5990 - val_acc: 0.7860\n",
      "Epoch 25/200\n",
      "57500/57500 [==============================] - 3s 56us/step - loss: 0.8619 - acc: 0.6814 - val_loss: 0.5921 - val_acc: 0.7910\n",
      "Epoch 26/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.8527 - acc: 0.6854 - val_loss: 0.5850 - val_acc: 0.7900\n",
      "Epoch 27/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8467 - acc: 0.6861 - val_loss: 0.5812 - val_acc: 0.7930\n",
      "Epoch 28/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8391 - acc: 0.6911 - val_loss: 0.5769 - val_acc: 0.7930\n",
      "Epoch 29/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.8332 - acc: 0.6900 - val_loss: 0.5747 - val_acc: 0.7960\n",
      "Epoch 30/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.8256 - acc: 0.6953 - val_loss: 0.5683 - val_acc: 0.7960\n",
      "Epoch 31/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8129 - acc: 0.7013 - val_loss: 0.5667 - val_acc: 0.7980\n",
      "Epoch 32/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8139 - acc: 0.6983 - val_loss: 0.5623 - val_acc: 0.7930\n",
      "Epoch 33/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.8116 - acc: 0.6997 - val_loss: 0.5610 - val_acc: 0.7950\n",
      "Epoch 34/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7998 - acc: 0.7048 - val_loss: 0.5555 - val_acc: 0.7960\n",
      "Epoch 35/200\n",
      "57500/57500 [==============================] - 3s 54us/step - loss: 0.7995 - acc: 0.7041 - val_loss: 0.5527 - val_acc: 0.7940\n",
      "Epoch 36/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7939 - acc: 0.7085 - val_loss: 0.5493 - val_acc: 0.8000\n",
      "Epoch 37/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7956 - acc: 0.7071 - val_loss: 0.5458 - val_acc: 0.8000\n",
      "Epoch 38/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.7916 - acc: 0.7090 - val_loss: 0.5476 - val_acc: 0.7990\n",
      "Epoch 39/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7849 - acc: 0.7090 - val_loss: 0.5414 - val_acc: 0.8040\n",
      "Epoch 40/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7789 - acc: 0.7123 - val_loss: 0.5383 - val_acc: 0.7980\n",
      "Epoch 41/200\n",
      "57500/57500 [==============================] - 3s 49us/step - loss: 0.7720 - acc: 0.7139 - val_loss: 0.5371 - val_acc: 0.8000\n",
      "Epoch 42/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7697 - acc: 0.7160 - val_loss: 0.5348 - val_acc: 0.8020\n",
      "Epoch 43/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7690 - acc: 0.7161 - val_loss: 0.5339 - val_acc: 0.8020\n",
      "Epoch 44/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7674 - acc: 0.7158 - val_loss: 0.5333 - val_acc: 0.7970\n",
      "Epoch 45/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.7623 - acc: 0.7199 - val_loss: 0.5299 - val_acc: 0.8000\n",
      "Epoch 46/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7598 - acc: 0.7194 - val_loss: 0.5328 - val_acc: 0.7980\n",
      "Epoch 47/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.7585 - acc: 0.7216 - val_loss: 0.5295 - val_acc: 0.8010\n",
      "Epoch 48/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7568 - acc: 0.7228 - val_loss: 0.5257 - val_acc: 0.8040\n",
      "Epoch 49/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.7480 - acc: 0.7240 - val_loss: 0.5214 - val_acc: 0.8030\n",
      "Epoch 50/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7509 - acc: 0.7216 - val_loss: 0.5200 - val_acc: 0.8060\n",
      "Epoch 51/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.7455 - acc: 0.7259 - val_loss: 0.5230 - val_acc: 0.8060\n",
      "Epoch 52/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7451 - acc: 0.7250 - val_loss: 0.5201 - val_acc: 0.8010\n",
      "Epoch 53/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7401 - acc: 0.7274 - val_loss: 0.5187 - val_acc: 0.8070\n",
      "Epoch 54/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7388 - acc: 0.7271 - val_loss: 0.5150 - val_acc: 0.8070\n",
      "Epoch 55/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.7356 - acc: 0.7288 - val_loss: 0.5166 - val_acc: 0.8080\n",
      "Epoch 56/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.7362 - acc: 0.7287 - val_loss: 0.5171 - val_acc: 0.7990\n",
      "Epoch 57/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7332 - acc: 0.7295 - val_loss: 0.5150 - val_acc: 0.8040\n",
      "Epoch 58/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7297 - acc: 0.7299 - val_loss: 0.5131 - val_acc: 0.8040\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7242 - acc: 0.7322 - val_loss: 0.5110 - val_acc: 0.8060\n",
      "Epoch 60/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.7270 - acc: 0.7328 - val_loss: 0.5101 - val_acc: 0.8110\n",
      "Epoch 61/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7257 - acc: 0.7308 - val_loss: 0.5105 - val_acc: 0.8080\n",
      "Epoch 62/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7242 - acc: 0.7330 - val_loss: 0.5111 - val_acc: 0.8100\n",
      "Epoch 63/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7195 - acc: 0.7329 - val_loss: 0.5108 - val_acc: 0.8080\n",
      "Epoch 64/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7184 - acc: 0.7340 - val_loss: 0.5068 - val_acc: 0.8110\n",
      "Epoch 65/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7171 - acc: 0.7364 - val_loss: 0.5060 - val_acc: 0.8100\n",
      "Epoch 66/200\n",
      "57500/57500 [==============================] - 3s 54us/step - loss: 0.7159 - acc: 0.7380 - val_loss: 0.5045 - val_acc: 0.8140\n",
      "Epoch 67/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7130 - acc: 0.7385 - val_loss: 0.5045 - val_acc: 0.8060\n",
      "Epoch 68/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7093 - acc: 0.7394 - val_loss: 0.4999 - val_acc: 0.8170\n",
      "Epoch 69/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7137 - acc: 0.7388 - val_loss: 0.5021 - val_acc: 0.8050\n",
      "Epoch 70/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7102 - acc: 0.7403 - val_loss: 0.5000 - val_acc: 0.8100\n",
      "Epoch 71/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.7071 - acc: 0.7401 - val_loss: 0.5003 - val_acc: 0.8140\n",
      "Epoch 72/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.7047 - acc: 0.7413 - val_loss: 0.5007 - val_acc: 0.8080\n",
      "Epoch 73/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.7052 - acc: 0.7411 - val_loss: 0.5006 - val_acc: 0.8160\n",
      "Epoch 74/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6986 - acc: 0.7420 - val_loss: 0.4952 - val_acc: 0.8200\n",
      "Epoch 75/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6979 - acc: 0.7437 - val_loss: 0.4960 - val_acc: 0.8230\n",
      "Epoch 76/200\n",
      "57500/57500 [==============================] - 3s 54us/step - loss: 0.6969 - acc: 0.7462 - val_loss: 0.4935 - val_acc: 0.8200\n",
      "Epoch 77/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6986 - acc: 0.7422 - val_loss: 0.4988 - val_acc: 0.8170\n",
      "Epoch 78/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6939 - acc: 0.7465 - val_loss: 0.4942 - val_acc: 0.8230\n",
      "Epoch 79/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6972 - acc: 0.7444 - val_loss: 0.4937 - val_acc: 0.8190\n",
      "Epoch 80/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6956 - acc: 0.7443 - val_loss: 0.4931 - val_acc: 0.8190\n",
      "Epoch 81/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6945 - acc: 0.7449 - val_loss: 0.4912 - val_acc: 0.8260\n",
      "Epoch 82/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6892 - acc: 0.7467 - val_loss: 0.4907 - val_acc: 0.8190\n",
      "Epoch 83/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6867 - acc: 0.7514 - val_loss: 0.4893 - val_acc: 0.8200\n",
      "Epoch 84/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6877 - acc: 0.7490 - val_loss: 0.4900 - val_acc: 0.8240\n",
      "Epoch 85/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6872 - acc: 0.7497 - val_loss: 0.4902 - val_acc: 0.8250\n",
      "Epoch 86/200\n",
      "57500/57500 [==============================] - 3s 54us/step - loss: 0.6841 - acc: 0.7504 - val_loss: 0.4870 - val_acc: 0.8280\n",
      "Epoch 87/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6832 - acc: 0.7511 - val_loss: 0.4856 - val_acc: 0.8290\n",
      "Epoch 88/200\n",
      "57500/57500 [==============================] - 3s 49us/step - loss: 0.6832 - acc: 0.7514 - val_loss: 0.4855 - val_acc: 0.8220\n",
      "Epoch 89/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6822 - acc: 0.7512 - val_loss: 0.4867 - val_acc: 0.8250\n",
      "Epoch 90/200\n",
      "57500/57500 [==============================] - 3s 49us/step - loss: 0.6784 - acc: 0.7533 - val_loss: 0.4868 - val_acc: 0.8270\n",
      "Epoch 91/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6813 - acc: 0.7520 - val_loss: 0.4842 - val_acc: 0.8270\n",
      "Epoch 92/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6802 - acc: 0.7517 - val_loss: 0.4866 - val_acc: 0.8290\n",
      "Epoch 93/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6795 - acc: 0.7520 - val_loss: 0.4874 - val_acc: 0.8260\n",
      "Epoch 94/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6791 - acc: 0.7529 - val_loss: 0.4853 - val_acc: 0.8290\n",
      "Epoch 95/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6773 - acc: 0.7542 - val_loss: 0.4841 - val_acc: 0.8300\n",
      "Epoch 96/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6712 - acc: 0.7566 - val_loss: 0.4856 - val_acc: 0.8310\n",
      "Epoch 97/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6739 - acc: 0.7548 - val_loss: 0.4807 - val_acc: 0.8310\n",
      "Epoch 98/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6711 - acc: 0.7549 - val_loss: 0.4833 - val_acc: 0.8280\n",
      "Epoch 99/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6695 - acc: 0.7569 - val_loss: 0.4807 - val_acc: 0.8300\n",
      "Epoch 100/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6670 - acc: 0.7573 - val_loss: 0.4806 - val_acc: 0.8250\n",
      "Epoch 101/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6655 - acc: 0.7575 - val_loss: 0.4778 - val_acc: 0.8320\n",
      "Epoch 102/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6653 - acc: 0.7584 - val_loss: 0.4805 - val_acc: 0.8330\n",
      "Epoch 103/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6715 - acc: 0.7574 - val_loss: 0.4807 - val_acc: 0.8310\n",
      "Epoch 104/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6675 - acc: 0.7584 - val_loss: 0.4803 - val_acc: 0.8290\n",
      "Epoch 105/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.6656 - acc: 0.7589 - val_loss: 0.4802 - val_acc: 0.8290\n",
      "Epoch 106/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6626 - acc: 0.7607 - val_loss: 0.4771 - val_acc: 0.8340\n",
      "Epoch 107/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.6647 - acc: 0.7585 - val_loss: 0.4806 - val_acc: 0.8320\n",
      "Epoch 108/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6670 - acc: 0.7596 - val_loss: 0.4784 - val_acc: 0.8330\n",
      "Epoch 109/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6644 - acc: 0.7606 - val_loss: 0.4786 - val_acc: 0.8350\n",
      "Epoch 110/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6624 - acc: 0.7610 - val_loss: 0.4778 - val_acc: 0.8280\n",
      "Epoch 111/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6588 - acc: 0.7627 - val_loss: 0.4725 - val_acc: 0.8260\n",
      "Epoch 112/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6644 - acc: 0.7609 - val_loss: 0.4748 - val_acc: 0.8400\n",
      "Epoch 113/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6596 - acc: 0.7630 - val_loss: 0.4747 - val_acc: 0.8310\n",
      "Epoch 114/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6590 - acc: 0.7627 - val_loss: 0.4737 - val_acc: 0.8360\n",
      "Epoch 115/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6554 - acc: 0.7646 - val_loss: 0.4738 - val_acc: 0.8360\n",
      "Epoch 116/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6597 - acc: 0.7646 - val_loss: 0.4717 - val_acc: 0.8330\n",
      "Epoch 117/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6582 - acc: 0.7646 - val_loss: 0.4753 - val_acc: 0.8330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6580 - acc: 0.7654 - val_loss: 0.4756 - val_acc: 0.8340\n",
      "Epoch 119/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6520 - acc: 0.7661 - val_loss: 0.4756 - val_acc: 0.8330\n",
      "Epoch 120/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6522 - acc: 0.7672 - val_loss: 0.4762 - val_acc: 0.8320\n",
      "Epoch 121/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.6515 - acc: 0.7650 - val_loss: 0.4718 - val_acc: 0.8390\n",
      "Epoch 122/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6476 - acc: 0.7654 - val_loss: 0.4705 - val_acc: 0.8330\n",
      "Epoch 123/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6454 - acc: 0.7685 - val_loss: 0.4721 - val_acc: 0.8320\n",
      "Epoch 124/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6499 - acc: 0.7660 - val_loss: 0.4715 - val_acc: 0.8320\n",
      "Epoch 125/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6497 - acc: 0.7667 - val_loss: 0.4718 - val_acc: 0.8390\n",
      "Epoch 126/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6511 - acc: 0.7670 - val_loss: 0.4717 - val_acc: 0.8300\n",
      "Epoch 127/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6390 - acc: 0.7695 - val_loss: 0.4730 - val_acc: 0.8350\n",
      "Epoch 128/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6497 - acc: 0.7658 - val_loss: 0.4681 - val_acc: 0.8320\n",
      "Epoch 129/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6433 - acc: 0.7694 - val_loss: 0.4697 - val_acc: 0.8370\n",
      "Epoch 130/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6489 - acc: 0.7681 - val_loss: 0.4692 - val_acc: 0.8360\n",
      "Epoch 131/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6365 - acc: 0.7695 - val_loss: 0.4698 - val_acc: 0.8330\n",
      "Epoch 132/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6374 - acc: 0.7682 - val_loss: 0.4671 - val_acc: 0.8380\n",
      "Epoch 133/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6453 - acc: 0.7692 - val_loss: 0.4692 - val_acc: 0.8360\n",
      "Epoch 134/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6358 - acc: 0.7729 - val_loss: 0.4690 - val_acc: 0.8390\n",
      "Epoch 135/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6414 - acc: 0.7684 - val_loss: 0.4707 - val_acc: 0.8370\n",
      "Epoch 136/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6400 - acc: 0.7714 - val_loss: 0.4653 - val_acc: 0.8320\n",
      "Epoch 137/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6415 - acc: 0.7694 - val_loss: 0.4678 - val_acc: 0.8350\n",
      "Epoch 138/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6407 - acc: 0.7707 - val_loss: 0.4663 - val_acc: 0.8320\n",
      "Epoch 139/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6398 - acc: 0.7706 - val_loss: 0.4663 - val_acc: 0.8360\n",
      "Epoch 140/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6372 - acc: 0.7722 - val_loss: 0.4667 - val_acc: 0.8360\n",
      "Epoch 141/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6347 - acc: 0.7719 - val_loss: 0.4702 - val_acc: 0.8330\n",
      "Epoch 142/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6336 - acc: 0.7706 - val_loss: 0.4653 - val_acc: 0.8350\n",
      "Epoch 143/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6365 - acc: 0.7703 - val_loss: 0.4665 - val_acc: 0.8350\n",
      "Epoch 144/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6373 - acc: 0.7715 - val_loss: 0.4675 - val_acc: 0.8360\n",
      "Epoch 145/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6337 - acc: 0.7735 - val_loss: 0.4652 - val_acc: 0.8330\n",
      "Epoch 146/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6368 - acc: 0.7722 - val_loss: 0.4659 - val_acc: 0.8370\n",
      "Epoch 147/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6348 - acc: 0.7741 - val_loss: 0.4641 - val_acc: 0.8360\n",
      "Epoch 148/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6263 - acc: 0.7745 - val_loss: 0.4622 - val_acc: 0.8370\n",
      "Epoch 149/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6303 - acc: 0.7760 - val_loss: 0.4668 - val_acc: 0.8290\n",
      "Epoch 150/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6302 - acc: 0.7748 - val_loss: 0.4628 - val_acc: 0.8340\n",
      "Epoch 151/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6305 - acc: 0.7735 - val_loss: 0.4625 - val_acc: 0.8360\n",
      "Epoch 152/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6291 - acc: 0.7728 - val_loss: 0.4588 - val_acc: 0.8370\n",
      "Epoch 153/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6300 - acc: 0.7746 - val_loss: 0.4609 - val_acc: 0.8350\n",
      "Epoch 154/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6288 - acc: 0.7753 - val_loss: 0.4615 - val_acc: 0.8310\n",
      "Epoch 155/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6283 - acc: 0.7748 - val_loss: 0.4596 - val_acc: 0.8360\n",
      "Epoch 156/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6239 - acc: 0.7785 - val_loss: 0.4600 - val_acc: 0.8390\n",
      "Epoch 157/200\n",
      "57500/57500 [==============================] - 3s 54us/step - loss: 0.6269 - acc: 0.7760 - val_loss: 0.4588 - val_acc: 0.8380\n",
      "Epoch 158/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6255 - acc: 0.7755 - val_loss: 0.4575 - val_acc: 0.8380\n",
      "Epoch 159/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6197 - acc: 0.7786 - val_loss: 0.4579 - val_acc: 0.8340\n",
      "Epoch 160/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6210 - acc: 0.7778 - val_loss: 0.4561 - val_acc: 0.8360\n",
      "Epoch 161/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6248 - acc: 0.7759 - val_loss: 0.4582 - val_acc: 0.8350\n",
      "Epoch 162/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6231 - acc: 0.7788 - val_loss: 0.4569 - val_acc: 0.8390\n",
      "Epoch 163/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6215 - acc: 0.7783 - val_loss: 0.4566 - val_acc: 0.8340\n",
      "Epoch 164/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6207 - acc: 0.7789 - val_loss: 0.4570 - val_acc: 0.8310\n",
      "Epoch 165/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6155 - acc: 0.7783 - val_loss: 0.4588 - val_acc: 0.8350\n",
      "Epoch 166/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6188 - acc: 0.7793 - val_loss: 0.4541 - val_acc: 0.8390\n",
      "Epoch 167/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6229 - acc: 0.7765 - val_loss: 0.4588 - val_acc: 0.8360\n",
      "Epoch 168/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6142 - acc: 0.7792 - val_loss: 0.4553 - val_acc: 0.8340\n",
      "Epoch 169/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6204 - acc: 0.7789 - val_loss: 0.4572 - val_acc: 0.8360\n",
      "Epoch 170/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6184 - acc: 0.7796 - val_loss: 0.4578 - val_acc: 0.8400\n",
      "Epoch 171/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6186 - acc: 0.7794 - val_loss: 0.4590 - val_acc: 0.8390\n",
      "Epoch 172/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6174 - acc: 0.7773 - val_loss: 0.4582 - val_acc: 0.8410\n",
      "Epoch 173/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6200 - acc: 0.7779 - val_loss: 0.4565 - val_acc: 0.8460\n",
      "Epoch 174/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6159 - acc: 0.7806 - val_loss: 0.4559 - val_acc: 0.8350\n",
      "Epoch 175/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6153 - acc: 0.7795 - val_loss: 0.4557 - val_acc: 0.8410\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6143 - acc: 0.7818 - val_loss: 0.4588 - val_acc: 0.8400\n",
      "Epoch 177/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6140 - acc: 0.7801 - val_loss: 0.4577 - val_acc: 0.8440\n",
      "Epoch 178/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6133 - acc: 0.7806 - val_loss: 0.4600 - val_acc: 0.8340\n",
      "Epoch 179/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6135 - acc: 0.7818 - val_loss: 0.4586 - val_acc: 0.8370\n",
      "Epoch 180/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6130 - acc: 0.7818 - val_loss: 0.4560 - val_acc: 0.8400\n",
      "Epoch 181/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6138 - acc: 0.7807 - val_loss: 0.4577 - val_acc: 0.8460\n",
      "Epoch 182/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6091 - acc: 0.7827 - val_loss: 0.4575 - val_acc: 0.8410\n",
      "Epoch 183/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6138 - acc: 0.7817 - val_loss: 0.4589 - val_acc: 0.8400\n",
      "Epoch 184/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6143 - acc: 0.7810 - val_loss: 0.4551 - val_acc: 0.8450\n",
      "Epoch 185/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6120 - acc: 0.7816 - val_loss: 0.4568 - val_acc: 0.8410\n",
      "Epoch 186/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6116 - acc: 0.7821 - val_loss: 0.4573 - val_acc: 0.8420\n",
      "Epoch 187/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6168 - acc: 0.7801 - val_loss: 0.4558 - val_acc: 0.8440\n",
      "Epoch 188/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6071 - acc: 0.7819 - val_loss: 0.4556 - val_acc: 0.8420\n",
      "Epoch 189/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6131 - acc: 0.7813 - val_loss: 0.4584 - val_acc: 0.8390\n",
      "Epoch 190/200\n",
      "57500/57500 [==============================] - 3s 52us/step - loss: 0.6132 - acc: 0.7823 - val_loss: 0.4554 - val_acc: 0.8410\n",
      "Epoch 191/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6097 - acc: 0.7821 - val_loss: 0.4570 - val_acc: 0.8510\n",
      "Epoch 192/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6123 - acc: 0.7829 - val_loss: 0.4543 - val_acc: 0.8490\n",
      "Epoch 193/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6096 - acc: 0.7812 - val_loss: 0.4568 - val_acc: 0.8420\n",
      "Epoch 194/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.6104 - acc: 0.7806 - val_loss: 0.4533 - val_acc: 0.8440\n",
      "Epoch 195/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6050 - acc: 0.7857 - val_loss: 0.4546 - val_acc: 0.8440\n",
      "Epoch 196/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6050 - acc: 0.7841 - val_loss: 0.4540 - val_acc: 0.8420\n",
      "Epoch 197/200\n",
      "57500/57500 [==============================] - 3s 55us/step - loss: 0.6042 - acc: 0.7833 - val_loss: 0.4537 - val_acc: 0.8350\n",
      "Epoch 198/200\n",
      "57500/57500 [==============================] - 3s 53us/step - loss: 0.6046 - acc: 0.7853 - val_loss: 0.4563 - val_acc: 0.8380\n",
      "Epoch 199/200\n",
      "57500/57500 [==============================] - 3s 50us/step - loss: 0.6007 - acc: 0.7837 - val_loss: 0.4561 - val_acc: 0.8400\n",
      "Epoch 200/200\n",
      "57500/57500 [==============================] - 3s 51us/step - loss: 0.6040 - acc: 0.7864 - val_loss: 0.4524 - val_acc: 0.8450\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57500/57500 [==============================] - 3s 48us/step\n",
      "1500/1500 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38024211694468624, 0.8748000000041464]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4971508943239848, 0.8033333336512247]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 1.8931 - acc: 0.2074 - val_loss: 1.8488 - val_acc: 0.2337\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.7731 - acc: 0.2952 - val_loss: 1.7063 - val_acc: 0.3687\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.6002 - acc: 0.4384 - val_loss: 1.5107 - val_acc: 0.5063\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.3922 - acc: 0.5540 - val_loss: 1.3066 - val_acc: 0.5803\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.1979 - acc: 0.6264 - val_loss: 1.1376 - val_acc: 0.6437\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.0444 - acc: 0.6681 - val_loss: 1.0115 - val_acc: 0.6760\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.9314 - acc: 0.6953 - val_loss: 0.9210 - val_acc: 0.6937\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.8501 - acc: 0.7122 - val_loss: 0.8556 - val_acc: 0.7087\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.7915 - acc: 0.7250 - val_loss: 0.8076 - val_acc: 0.7183\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.7479 - acc: 0.7354 - val_loss: 0.7746 - val_acc: 0.7247\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.7144 - acc: 0.7431 - val_loss: 0.7475 - val_acc: 0.7303\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6879 - acc: 0.7507 - val_loss: 0.7268 - val_acc: 0.7373\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.6660 - acc: 0.7581 - val_loss: 0.7099 - val_acc: 0.7410\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.6471 - acc: 0.7626 - val_loss: 0.6962 - val_acc: 0.7477\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6312 - acc: 0.7682 - val_loss: 0.6847 - val_acc: 0.7490\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6177 - acc: 0.7735 - val_loss: 0.6748 - val_acc: 0.7523\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6049 - acc: 0.7787 - val_loss: 0.6657 - val_acc: 0.7557\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5934 - acc: 0.7808 - val_loss: 0.6616 - val_acc: 0.7567\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5835 - acc: 0.7848 - val_loss: 0.6511 - val_acc: 0.7617\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5737 - acc: 0.7890 - val_loss: 0.6442 - val_acc: 0.7640\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5652 - acc: 0.7933 - val_loss: 0.6398 - val_acc: 0.7657\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5568 - acc: 0.7957 - val_loss: 0.6350 - val_acc: 0.7650\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5492 - acc: 0.7991 - val_loss: 0.6301 - val_acc: 0.7710\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.5419 - acc: 0.8022 - val_loss: 0.6256 - val_acc: 0.7693\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5349 - acc: 0.8051 - val_loss: 0.6236 - val_acc: 0.7673\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5284 - acc: 0.8066 - val_loss: 0.6187 - val_acc: 0.7723\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5220 - acc: 0.8094 - val_loss: 0.6146 - val_acc: 0.7793\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5160 - acc: 0.8122 - val_loss: 0.6129 - val_acc: 0.7740\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5107 - acc: 0.8142 - val_loss: 0.6093 - val_acc: 0.7787\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5050 - acc: 0.8166 - val_loss: 0.6067 - val_acc: 0.7807\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.5001 - acc: 0.8184 - val_loss: 0.6049 - val_acc: 0.7790\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4949 - acc: 0.8206 - val_loss: 0.6042 - val_acc: 0.7833\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4900 - acc: 0.8227 - val_loss: 0.6031 - val_acc: 0.7817\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4855 - acc: 0.8241 - val_loss: 0.6004 - val_acc: 0.7813\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4809 - acc: 0.8258 - val_loss: 0.5991 - val_acc: 0.7843\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4765 - acc: 0.8273 - val_loss: 0.5969 - val_acc: 0.7853\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4726 - acc: 0.8291 - val_loss: 0.5969 - val_acc: 0.7850\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4685 - acc: 0.8310 - val_loss: 0.5937 - val_acc: 0.7870\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4641 - acc: 0.8324 - val_loss: 0.5918 - val_acc: 0.7880\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4604 - acc: 0.8339 - val_loss: 0.5927 - val_acc: 0.7887\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4568 - acc: 0.8347 - val_loss: 0.5917 - val_acc: 0.7867\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4532 - acc: 0.8367 - val_loss: 0.5896 - val_acc: 0.7893\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4496 - acc: 0.8390 - val_loss: 0.5896 - val_acc: 0.7910\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4463 - acc: 0.8396 - val_loss: 0.5878 - val_acc: 0.7907\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4430 - acc: 0.8407 - val_loss: 0.5889 - val_acc: 0.7920\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4397 - acc: 0.8412 - val_loss: 0.5882 - val_acc: 0.7920\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4365 - acc: 0.8436 - val_loss: 0.5878 - val_acc: 0.7953\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4335 - acc: 0.8452 - val_loss: 0.5918 - val_acc: 0.7927\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4305 - acc: 0.8456 - val_loss: 0.5864 - val_acc: 0.7940\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4276 - acc: 0.8464 - val_loss: 0.5873 - val_acc: 0.7963\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4248 - acc: 0.8488 - val_loss: 0.5902 - val_acc: 0.7963\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4222 - acc: 0.8497 - val_loss: 0.5883 - val_acc: 0.7943\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4194 - acc: 0.8507 - val_loss: 0.5877 - val_acc: 0.7980\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4167 - acc: 0.8508 - val_loss: 0.5872 - val_acc: 0.7933\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4140 - acc: 0.8518 - val_loss: 0.5872 - val_acc: 0.7913\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4114 - acc: 0.8531 - val_loss: 0.5859 - val_acc: 0.7907\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4088 - acc: 0.8548 - val_loss: 0.5850 - val_acc: 0.7917\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4069 - acc: 0.8554 - val_loss: 0.5876 - val_acc: 0.7907\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4042 - acc: 0.8569 - val_loss: 0.5882 - val_acc: 0.7987\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4021 - acc: 0.8560 - val_loss: 0.5874 - val_acc: 0.7953\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3996 - acc: 0.8585 - val_loss: 0.5878 - val_acc: 0.7977\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3972 - acc: 0.8588 - val_loss: 0.5868 - val_acc: 0.7933\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3954 - acc: 0.8593 - val_loss: 0.5885 - val_acc: 0.7963\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3930 - acc: 0.8617 - val_loss: 0.5908 - val_acc: 0.7997\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3908 - acc: 0.8606 - val_loss: 0.5876 - val_acc: 0.7973\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3889 - acc: 0.8616 - val_loss: 0.5888 - val_acc: 0.7950\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3869 - acc: 0.8630 - val_loss: 0.5892 - val_acc: 0.7967\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3851 - acc: 0.8638 - val_loss: 0.5894 - val_acc: 0.7953\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3828 - acc: 0.8650 - val_loss: 0.5935 - val_acc: 0.8040\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3807 - acc: 0.8653 - val_loss: 0.5967 - val_acc: 0.8023\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3789 - acc: 0.8658 - val_loss: 0.5962 - val_acc: 0.7930\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3772 - acc: 0.8665 - val_loss: 0.5968 - val_acc: 0.7997\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3751 - acc: 0.8670 - val_loss: 0.5944 - val_acc: 0.8020\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3735 - acc: 0.8676 - val_loss: 0.5980 - val_acc: 0.7977\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3715 - acc: 0.8687 - val_loss: 0.5945 - val_acc: 0.7980\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3696 - acc: 0.8694 - val_loss: 0.5947 - val_acc: 0.7963\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3682 - acc: 0.8700 - val_loss: 0.5960 - val_acc: 0.7940\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3663 - acc: 0.8710 - val_loss: 0.5987 - val_acc: 0.7997\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3646 - acc: 0.8709 - val_loss: 0.5997 - val_acc: 0.7993\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3630 - acc: 0.8719 - val_loss: 0.5994 - val_acc: 0.7970\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3615 - acc: 0.8726 - val_loss: 0.5990 - val_acc: 0.7923\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3597 - acc: 0.8733 - val_loss: 0.6006 - val_acc: 0.7933\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3583 - acc: 0.8742 - val_loss: 0.6031 - val_acc: 0.7953\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3565 - acc: 0.8741 - val_loss: 0.6043 - val_acc: 0.7977\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3552 - acc: 0.8746 - val_loss: 0.6033 - val_acc: 0.7927\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3534 - acc: 0.8757 - val_loss: 0.6060 - val_acc: 0.7960\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3520 - acc: 0.8758 - val_loss: 0.6079 - val_acc: 0.7940\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3505 - acc: 0.8768 - val_loss: 0.6052 - val_acc: 0.7917\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3485 - acc: 0.8779 - val_loss: 0.6057 - val_acc: 0.7923\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3472 - acc: 0.8776 - val_loss: 0.6074 - val_acc: 0.7923\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3458 - acc: 0.8786 - val_loss: 0.6115 - val_acc: 0.7917\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3446 - acc: 0.8792 - val_loss: 0.6106 - val_acc: 0.7913\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3428 - acc: 0.8792 - val_loss: 0.6100 - val_acc: 0.7920\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3418 - acc: 0.8812 - val_loss: 0.6129 - val_acc: 0.7943\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3400 - acc: 0.8808 - val_loss: 0.6129 - val_acc: 0.7913\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3389 - acc: 0.8811 - val_loss: 0.6137 - val_acc: 0.7917\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3376 - acc: 0.8821 - val_loss: 0.6142 - val_acc: 0.7913\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3363 - acc: 0.8818 - val_loss: 0.6154 - val_acc: 0.7937\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3347 - acc: 0.8825 - val_loss: 0.6203 - val_acc: 0.7957\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3333 - acc: 0.8842 - val_loss: 0.6209 - val_acc: 0.7930\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3319 - acc: 0.8835 - val_loss: 0.6233 - val_acc: 0.7933\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3307 - acc: 0.8847 - val_loss: 0.6217 - val_acc: 0.7940\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3294 - acc: 0.8851 - val_loss: 0.6258 - val_acc: 0.7893\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3283 - acc: 0.8852 - val_loss: 0.6260 - val_acc: 0.7947\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3267 - acc: 0.8859 - val_loss: 0.6270 - val_acc: 0.7953\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3254 - acc: 0.8865 - val_loss: 0.6234 - val_acc: 0.7903\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3246 - acc: 0.8868 - val_loss: 0.6275 - val_acc: 0.7907\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3231 - acc: 0.8872 - val_loss: 0.6332 - val_acc: 0.7913\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3221 - acc: 0.8874 - val_loss: 0.6295 - val_acc: 0.7923\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3205 - acc: 0.8875 - val_loss: 0.6287 - val_acc: 0.7890\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3195 - acc: 0.8886 - val_loss: 0.6305 - val_acc: 0.7903\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3181 - acc: 0.8890 - val_loss: 0.6338 - val_acc: 0.7917\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3170 - acc: 0.8899 - val_loss: 0.6339 - val_acc: 0.7910\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3158 - acc: 0.8908 - val_loss: 0.6369 - val_acc: 0.7903\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3149 - acc: 0.8901 - val_loss: 0.6355 - val_acc: 0.7887\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3135 - acc: 0.8909 - val_loss: 0.6393 - val_acc: 0.7903\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3122 - acc: 0.8919 - val_loss: 0.6408 - val_acc: 0.7883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3115 - acc: 0.8919 - val_loss: 0.6455 - val_acc: 0.7880\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3100 - acc: 0.8925 - val_loss: 0.6418 - val_acc: 0.7880\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3089 - acc: 0.8928 - val_loss: 0.6449 - val_acc: 0.7887\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 21us/step\n",
      "4000/4000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29492314792401864, 0.8997272727272727]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
